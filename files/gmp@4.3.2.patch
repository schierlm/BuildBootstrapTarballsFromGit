--- 1/acinclude.m4
+++ 2/acinclude.m4
@@ -44,10 +44,8 @@
 [[powerpc64-*-* | powerpc64le-*-* | powerpc620-*-* | powerpc630-*-* | powerpc970-*-* | power[3-9]-*-*]])
 
 define(X86_PATTERN,
-[[i?86*-*-* | k[5-8]*-*-* | pentium*-*-* | athlon-*-* | viac3*-*-* | geode*-*-* | atom-*-*]])
+[[i?86*-*-* | k[5-8]*-*-* | pentium*-*-* | athlon-*-* | viac3*-*-* | geode*-*-*]])
 
-define(X86_64_PATTERN,
-[[athlon64-*-* | pentium4-*-* | atom-*-* | core2-*-* | corei-*-* | x86_64-*-* | nano-*-*]])
 
 dnl  GMP_FAT_SUFFIX(DSTVAR, DIRECTORY)
 dnl  ---------------------------------
@@ -112,12 +110,13 @@
 dnl  instead of gmp.h, since that file isn't generated until the end of the
 dnl  configure.
 dnl
-dnl  Dummy value for GMP_LIMB_BITS is enough
+dnl  Dummy values for __GMP_BITS_PER_MP_LIMB and GMP_LIMB_BITS are enough
 dnl  for all current configure-time uses of gmp.h.
 
 define(GMP_INCLUDE_GMP_H,
 [[#define __GMP_WITHIN_CONFIGURE 1   /* ignore template stuff */
 #define GMP_NAIL_BITS $GMP_NAIL_BITS
+#define __GMP_BITS_PER_MP_LIMB 123 /* dummy for GMP_NUMB_BITS etc */
 #define GMP_LIMB_BITS 123
 $DEFN_LONG_LONG_LIMB
 #include "$srcdir/gmp-h.in"]
@@ -3801,8 +3800,6 @@
 dnl  --------------------
 dnl  Determine whether CC_FOR_BUILD is ANSI, and establish U_FOR_BUILD
 dnl  accordingly.
-dnl
-dnl  FIXME: Use AC_PROG_CC sets ac_cv_prog_cc_c89 which could be used instead
 
 AC_DEFUN([GMP_C_FOR_BUILD_ANSI],
 [AC_REQUIRE([GMP_PROG_CC_FOR_BUILD])
--- 1/AUTHORS
+++ 2/AUTHORS
@@ -1,48 +1,25 @@
 Authors of GNU MP (in chronological order of initial contribution)
 
 Torbjörn Granlund	Main author
-
 John Amanatides		Original version of mpz/pprime_p.c
-
 Paul Zimmermann		mpn/generic/mul_fft.c, dc_divrem_n.c, rootrem.c,
 			old mpz/powm.c, old toom3 code.
-
 Ken Weber		mpn/generic/bdivmod.c, old mpn/generic/gcd.c
-
 Bennet Yee		mpz/jacobi.c mpz/legendre.c
-
-Andreas Schwab		mpn/m68k/lshift.asm, mpn/m68k/rshift.asm
-
-Robert Harley		Old mpn/generic/mul_n.c, many files in mpn/arm
-
+Andreas Schwab		mpn/m68k/lshift.S, mpn/m68k/rshift.S
+Robert Harley		Old mpn/generic/mul_n.c, files in mpn/arm
 Linus Nordberg		Random number framework, original autoconfery
-
 Kent Boortz		MacOS 9 port
-
 Kevin Ryde		Most x86 assembly, new autoconfery, and countless other
 			things (please see the GMP manual for complete list)
-
 Gerardo Ballabio	gmpxx.h and C++ istream input
-
 Pedro Gimeno		Mersenne Twister random generator, other random number
 			revisions
-
-Jason Moxham		mpz/fac_ui.c and gen-fac_ui.c
-
+Jason Moxham		New mpz/fac_ui.c and gen-fac_ui.c
 Niels Möller		mpn/generic/hgcd2.c, gcd.c, gcdext.c, matrix22_mul.c,
 			hgcd.c, gcdext_1.c, gcd_subdiv_step.c, gcd_lehmer.c,
 			gcdext_subdiv_step.c, gcdext_lehmer.c,
-			toom_interpolate_7pts, mulmod_bnm1.c, dcpi1_bdiv_qr.c,
-			dcpi1_bdiv_q.c, sbpi1_bdiv_qr.c, sbpi1_bdiv_q.c,
-			toom_eval_dgr3_pm1.c, toom_eval_dgr3_pm2.c,
-			toom_eval_pm1.c, toom_eval_pm2.c, toom_eval_pm2exp.c,
-			divexact.c, mpz/nextprime.c.
-
+			toom_interpolate_7pts, mpz/nextprime.c.
 Marco Bodrato		mpn/generic/toom44_mul.c, toom4_sqr.c, toom53_mul.c,
-			toom62_mul.c, toom43_mul.c, toom52_mul.c,
-			toom_interpolate_6pts.c, mulmod_bnm1.c,
-			toom_eval_pm2.c, invert.c, invertappr.c.
-
-David Harvey		mpn/x86_64/mul_basecase.asm
-
-Martin Boij		mpn/generic/perfpow.c
+			toom62_mul.c
+David Harvey		mpn/x86_64/mul_basecase.asm.
--- 1/ChangeLog
+++ 2/ChangeLog
@@ -1,34 +1,22 @@
-2009-12-17  Torbjorn Granlund  <tege@gmplib.org>
-
-	* doc/gmp.texi: Update References section.  Update Contributors
-	section.  Misc updates.
+2010-01-07  Torbjorn Granlund  <tege@gmplib.org>
 
-	* gmp-impl.h: Renew default values for all THRESHOLDs.
+	* Version 4.3.2 released.
 
-2009-12-15  Marco Bodrato <bodrato@mail.dm.unipi.it>
+2009-12-27  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/generic/invert.c: Added some comment.
-	* mpn/generic/invertappr.c: Slightly better threshold handling.
-
-	* gmp-impl.h (INV_NEWTON_THRESHOLD): Default to 200.
-
-2009-12-17  Torbjorn Granlund  <tege@gmplib.org>
+	* (mpn_sqr): New name for mpn_sqr_n.  Make minimal changes,
+	leaving internal references still to old name.
 
-	* mpn/generic/bdiv_q.c (mpn_bdiv_q_itch): Rewrite.
+	* gmp-impl.h: Update mpn_mul_fft declaration.
 
-2009-12-16  Torbjorn Granlund  <tege@gmplib.org>
+2009-12-21  Torbjorn Granlund  <tege@gmplib.org>
 
-	* tests/mpn/t-bdiv.c (bdiv_q_valid_p, bdiv_qr_valid_p): Call refmpn_mul
-	instead of refmpn_mul_basecase.
-	* tests/mpn/toom-shared.h: Likewise.
-	* tests/refmpn.c (refmpn_mullo_n,refmpn_sqr,refmpn_mul_any): Likewise.
+	* gmp-h.in (__GNU_MP_VERSION_PATCHLEVEL): Bump.
 
-	* minithres/gmp-mparam.h: Add new thresholds, trim old values.
+2009-12-17  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/generic/powm.c: Use mp_bitcnt_t for bit counts.
-	Handle REDC_1_TO_REDC_N_THRESHOLD < MUL_TOOM22_THRESHOLD in
-	non-WANT_REDC_2 INNERLOOP expansion code.
-	* mpn/generic/powm_sec.c: Use mp_bitcnt_t for bit counts.
+	* doc/gmp.texi: Update References section.  Update Contributors
+	section.  Misc updates.
 
 2009-12-16  Niels Möller  <nisse@lysator.liu.se>
 
@@ -38,302 +26,16 @@
 	* mpn/generic/gcdext_lehmer.c (mpn_gcdext_lehmer_n): Get ASSERT
 	right.
 
-2009-12-16  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tests/mpz/t-mul.c: Misc cleanups.
-	(mul_basecase): Remove.
-	(ref_mpn_mul): Remove.
-	* tests/refmpn.c (refmpn_mul): New function, mainly from t-mul.c's
-	ref_mpn_mul.
-	(refmpn_mullo_n): Add a missing free.
-
-	* tune/speed.c (routine): Measure speed_mpn_{sb,dc}pi1_div_qr,
-	mpn_{sb,dc}pi1_divappr_q, mpn_{sb,dc}pi1_bdiv_qr, and
-	mpn_{sb,dc}pi1_bdiv_q.
-
-	* mpn/generic/invertappr.c: New file, meat from invert.c.
-	* mpn/generic/invert.c: Leave just mpn_invert.c.
-	* configure.in (gmp_mpn_functions): Add invertappr.
-	* tune/Makefile.am (TUNE_MPN_SRCS_BASIC): Add invertappr.c.
-	* gmp-impl.h (mpn_invert_itch, mpn_invertappr_itch): New macros.
-
 2009-12-15  Torbjorn Granlund  <tege@gmplib.org>
 
 	* mpn/generic/gcdext_subdiv_step.c: Get an ASSERT right.
 
-2009-12-15  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/sbpi1_div_qr.c (mpn_sbpi1_div_qr): A very small step
-	towards nail support.
-
-2009-12-15  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* gmp-impl.h (mpn_ni_invertappr): Added prototype and name-mangling.
-	* mpn/generic/mulmod_bnm1.c: Comment representation of class [0].
-
-2009-12-14  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/sbpi1_divappr_q.c (mpn_sbpi1_divappr_q): Use
-	udiv_qr_3by2.
-
-2009-12-14  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c (tune_binvert): Remove BINV_MULMOD_BNM1_THRESHOLD
-	tuning, it was always zero and caused BINV_NEWTON_THRESHOLD to be
-	wrong (as pointed out by Marco).
-	* (BINV_MULMOD_BNM1_THRESHOLD): Clean from other files too.
-
-2009-12-14  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/invert.c: Improved comments.
-	(mpn_bc_invertapp): Conditionally re-enable mpn_dcpi1_divappr_q.
-
-2009-12-14  Niels Möller  <nisse@lysator.liu.se>
-
-	* gmp-impl.h (udiv_qr_3by2): Fix typo in argument list.
-
-2009-12-13  Niels Möller  <nisse@lysator.liu.se>
-
-	* gmp-impl.h (udiv_qr_3by2): New macro.
-	* mpn/generic/sbpi1_div_qr.c (mpn_sbpi1_div_qr): Use udiv_qr_3by2.
-
 2009-12-13  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/generic/dcpi1_divappr_q.c (mpn_dcpi1_divappr_q): Avoid a buffer
-	overrun.
-
 	* mpn/generic/mul_fft.c (mpn_mul_fft_full): Handle carry-out from 2nd
 	mpn_mul_fft, add an ASSERT for the 1st mpn_mul_fft.  Replace some
 	comments on cc's range with ASSERTs.
 
-	* mpn/generic/gcdext.c (compute_v): Normalize tp[] after mpn_mul.
-
-	* mpz/powm.c: Rework buffer handling.
-
-2009-12-13  Niels Möller  <nisse@lysator.liu.se>
-
-	* tests/mpn/toom-shared.h (main): Use refmpn_mul_basecase to check
-	results (slow!). Iteration counts of all toom tests reduced
-	considerably.
-
-2009-12-13  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/invert.c (mpn_invertapp): Split in _bc and _ni.
-	(mpn_bc_invertapp): New function, the basecase.
-	(mpn_ni_invertapp): New function, Newton iteration.
-	(mpn_invert): Use mpn_ni_invertapp.
-	* tune/tuneup.c (tune_invert): Min for INV_APPR_THRESHOLD.
-	(tune_invertappr): Min for INV_NEWTON_THRESHOLD.
-
-	* tune/speed.h (SPEED_ROUTINE_MPN_NI_INVERTAPPR): New macro.
-	(speed_mpn_ni_invertappr): New function.
-	* tune/common.c (speed_mpn_ni_invertappr): New function.
-	* tune/speed.c (routine): Add speed_mpn_ni_invertappr.
-
-	* tune/tuneup.c (tune_invertappr): Use speed_mpn_ni_invertappr to
-	tune INV_MULMOD_BNM1_THRESHOLD.
-
-2009-12-12  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/mu_bdiv_qr.c (mpn_mu_bdiv_qr_itch): Rewrite.
-
-2009-12-12  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* tests/mpn/t-mulmod_bnm1.c (main): Disable B^n+1 stressing test
-	for odd sizes.
-
-	* mpn/generic/invert.c: Complete rewrite. Uses Newton iterations.
-	* gmp-impl.h (mpn_invertappr): Added prototype and name-mangling.
-	(mpn_invertappr_itch): Added prototype and name-mangling.
-	(INV_APPR_THRESHOLD): Support for a new tunable const.
-	* tune/speed.h (SPEED_ROUTINE_MPN_INVERTAPPR): New macro.
-	(speed_mpn_invertappr): New function.
-	* tune/common.c (speed_mpn_invertappr): New function.
-	* tune/speed.c (routine): Add speed_mpn_invertappr.
-	* tune/tuneup.c (tune_invertappr): New function: was tune_invert.
-	(tune_invert): Now tune only INV_APPR_THRESHOLD.
-	(all): Enable call to tune_invert and tune_invertappr.
-
-2009-12-11  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/binvert.c: Use mpn_mulmod_bnm1 instead of FFT wrapping.
-	Old, evidently broken wrapping code removed.
-	* tune/tuneup.c (tune_binvert): Tune BINV_MULMOD_BNM1_THRESHOLD.
-	* gmp-impl.h: Provide declarations for corresponding threshold var.
-
-	* tests/mpn/t-bdiv.c (COUNT): Decrease to keep run time reasonable.
-
-	* tune/tuneup.c (tune_invert): Tune INV_MULMOD_BNM1_THRESHOLD.
-	* gmp-impl.h: Provide declarations for corresponding threshold var.
-
-	* tests/mpn/t-mulmod_bnm1.c: Avoid a division by zero.
-
-	* configure.in: Set up different paths for different 64-bit sparc
-	processors.
-	* mpn/sparc64/ultrasparc34/gmp-mparam.h: New file.
-
-2009-12-10  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/*/gmp-mparam.h: Regenerate many of these files.
-
-2009-12-10  Niels Möller  <nisse@lysator.liu.se>
-
-	* gmp-impl.h (mpn_divexact): Removed scratch pointer from
-	prototype.
-	* mpn/generic/gcdext.c (divexact): Deleted, moved to...
-	* mpn/generic/divexact.c (mpn_divexact): New implementation (moved
-	from gcdext.c). The bidirectional divexact is kept but #if:ed out.
-	Interface change, since the new code doesn't take a scratch
-	argument.
-
-	* tests/mpn/t-mulmod_bnm1.c (main): Ensure that an >= bn. Lowered
-	MIN_N to 1. Various fixes to handle n == 1 properly.
-
-	* mpn/generic/mulmod_bnm1.c (mpn_mulmod_bnm1): Small interface
-	change, require an >= bn.
-
-	* mpn/generic/mulmod_bnm1.c (mpn_mulmod_bnm1): Fixed non-recursive
-	case to not write beyond end of result area.
-
-2009-12-09  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/speed.h (SPEED_ROUTINE_MPN_MULMOD_BNM1_CALL): New macro, made
-	from now deleted SPEED_ROUTINE_MPN_MULMOD_BNM1.
-	* tune/common.c (speed_mpn_bc_mulmod_bnm1): New function.
-	(speed_mpn_mulmod_bnm1): Use SPEED_ROUTINE_MPN_MULMOD_BNM1_CALL.
-	* tune/speed.c (routine): Add mpn_bc_mulmod_bnm1.
-
-	* mpn/generic/mulmod_bnm1.c (mpn_mulmod_bnm1_next_size): Rewrite.
-
-	* tune/tuneup.c (tune_mulmod_bnm1): Rewrite.
-
-2009-12-08  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/mulmod_bnm1.c (mpn_bc_mulmod_bnm1,
-	mpn_bc_mulmod_bnp1): Added a parameter for scratch area, possibly
-	same as result area (as suggested by Niels Möller).
-	(mpn_mulmod_bnm1): Calls changed accordingly.
-
-2009-12-08  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/gcdext_1.c (mpn_gcdext_1) [GCDEXT_1_USE_BINARY]: Use
-	table lookup for count_trailing_zeros. Binary algorithm still
-	disabled by default.
-
-	* mpn/generic/gcdext.c (divexact): Local definition of divexact,
-	using mpn_bdiv_q.
-	(compute_v): Use it.
-
-	* tests/mpn/Makefile.am (check_PROGRAMS): Added t-bdiv.
-
-	* tests/mpn/t-bdiv.c: New file.
-
-	* mpn/generic/bdiv_q.c (mpn_bdiv_q): Fixed bad quotient length,
-	should have qn == nn.
-
-	* mpn/generic/bdiv_qr.c (mpn_bdiv_qr): Pass correct nn length to
-	the lower-level functions.
-
-2009-12-08  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/speed.h (SPEED_ROUTINE_MPN_MULMOD_BNM1_ROUNDED): New define.
-	* tune/common.c (speed_mpn_mulmod_bnm1_rounded): New function.
-	* tune/speed.c (routine): Add mpn_mulmod_bnm1_rounded for measuring
-	mpn_mulmod_bnm1 at recommended sizes.
-
-	* mpn/generic/mulmod_bnm1.c (mpn_mulmod_bnm1_next_size): Rewrite.
-	(mpn_bc_mulmod_bnm1): Use mpn_add_n instead of mpn_add.
-
-	* tune/speed.c (routine): Add mpn_invert.
-
-	* tune/tuneup.c (tune_invert): New function.
-	* tune/speed.h (SPEED_ROUTINE_MPN_INVERT): New macro.
-	* tune/common.c (speed_mpn_invert): New function.
-	* gmp-impl.h: Provide declarations for corresponding threshold var.
-	* tune/Makefile.am (TUNE_MPN_SRCS_BASIC): Add invert.c.
-
-2009-12-08  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* tests/devel/try.c: Test mpn_addlsh2_n and mpn_{add,sub}lsh_n;
-	mpn_rsblsh_n now tests all shift values.
-	* tests/refmpn.c (refmpn_addlsh_n, refmpn_sublsh_n): New functions.
-	(refmpn_addlsh1_n): Use generic refmpn_addlsh_n.
-	(refmpn_sublsh1_n): Use generic refmpn_sublsh_n.
-	(refmpn_addlsh2_n): New function.
-	* tests/tests.h: Declare new functions.
-
-2009-12-06  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c (tune_mulmod_bnm1): Up min_size to 12.
-
-	* Globally: Rename *mullow* to *mullo*, *MULLOW* to *MULLO*.
-
-	* configure.in: Don't include ev5 directory for ev6* and ev7.  Misc
-	alpha path cleanups.
-	* mpn/alpha/add_n.asm: Replaced by mpn/alpha/ev5/add_n.asm.
-	* mpn/alpha/sub_n.asm: Replaced by mpn/alpha/ev5/sub_n.asm.
-	* mpn/alpha/lshift.asm: Replaced by mpn/alpha/ev5/lshift.asm.
-	* mpn/alpha/rshift.asm: Replaced by mpn/alpha/ev5/rshift.asm.
-	* mpn/alpha/com_n.asm: New, moved from mpn/alpha/ev5/rshift.asm.
-	* mpn/alpha/ev5/diveby3.asm: New, moved from mpn/alpha/diveby3.asm.
-
-	* mpn/powerpc64/mode64/diveby3.asm: Remove, it is slower than
-	mpn_bdiv_dbm1c on all hardware.
-
-	* mpn/generic/powm_sec.c: Rework logic for mpn_sqr_basecase size limit.
-
-	* gmp-impl.h (mpn_redc_1_sec): Declare.
-	* configure.in (gmp_mpn_functions): Add redc_1_sec.
-
-2009-12-06  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* tests/devel/try.c (try_one): DATA_SRC0_HIGHBIT sets the high bit.
-
-2009-12-05  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/toom_eval_dgr3_pm1.c: Change return value: 0 or ~0.
-	* mpn/generic/toom_eval_dgr3_pm2.c: Likewise.
-	* mpn/generic/toom_eval_pm1.c: Likewise.
-	* mpn/generic/toom_eval_pm2exp.c: Likewise.
-	* mpn/generic/toom_eval_pm2.c: Rewrite to use mpn_addlsh2_n.
-
-	* mpn/generic/toom_interpolate_5pts.c: Param sa is a flag, not a sign.
-
-	* mpn/generic/toom33_mul.c: Adapt to changes above.
-	* mpn/generic/toom3_sqr.c: Likewise.
-	* mpn/generic/toom42_mul.c: Likewise.
-	* mpn/generic/toom43_mul.c: Reduce branches.
-	* mpn/generic/toom44_mul.c: Likewise.
-	* mpn/generic/toom53_mul.c: Likewise.
-	* mpn/generic/toom62_mul.c: Likewise.
-
-	* mpn/generic/toom52_mul.c: Use toom_eval_ functions.
-
-	* mpn/generic/toom4_sqr.c: Avoid C99 construct.
-	* mpn/generic/toom_interpolate_7pts.c: Likewise.
-
-2009-12-05  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/redc_1_sec.c: New file.
-	* mpn/generic/powm_sec.c: Use redc_1_sec.  Use dummy full subtract
-	instead of mpn_cmp since the latter leaks to the side channel.
-	(mpn_local_sqr_n): New function, with associated macros.
-	(mpn_powm_sec): Use mpn_local_sqr_n.
-
-	* configure.in (HAVE_NATIVE): Add missing functions, then sort.
-
-2009-12-04  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c (tune_dc_div): Up min_size to 6.
-	(tune_mod_1): Set MOD_1_1_THRESHOLD min_size to 2.
-
-	* tune/speed.h: Negate "binvert"-type inverses, as required.
-
-	* mpn/generic/redc_1.c: Add ASSERTs.
-	* mpn/generic/redc_2.c: Likewise.
-
-	* mpn/generic/sbpi1_bdiv_q.c: Simplify loops, indexing.
-
 2009-12-03  Yann Droneaud  <yann@droneaud.fr>
 
 	* acinclude.m4 ([long long reliability test 1]): Add a "static" for C99
@@ -341,43 +43,15 @@
 
 2009-12-03  Torbjorn Granlund  <tege@gmplib.org>
 
-	* configure.in: Move intptr_t test into common AC_CHECK_TYPES.
-
 	* mpn/generic/gcdext.c: Add a TMP_FREE.
 
 2009-12-03  Niels Möller  <nisse@lysator.liu.se>
 
-	* mpn/generic/gcdext_1.c (mpn_gcdext_1) [GCDEXT_1_USE_BINARY]:
-	Added various masking tricks.
-
-	* mpn/generic/gcdext_1.c (mpn_gcdext_1) [GCDEXT_1_USE_BINARY]:
-	Reimplemented binary gcdext, with proper canonicalization.
-
 	* mpn/generic/gcdext_lehmer.c (mpn_gcdext_lehmer_n): Handle v == 0
 	from mpn_gcdext_1.
 	* mpn/generic/gcdext_1.c (mpn_gcdext_1): Allow inputs with a < b,
 	assertions fixed accordingly.
 
-2009-12-03  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c: Tune DC_DIVAPPR_Q_THRESHOLD.  Rewrite
-	DC_DIV_QR_THRESHOLD tuning code.
-	(tune_dc_div): Rewrite.
-	* tune/speed.h (SPEED_ROUTINE_MPN_PI1_DIV): New macro.
-	* tune/common.c (speed_mpn_sbpi1_div_qr, speed_mpn_dcpi1_div_qr,
-	speed_mpn_sbpi1_divappr_q, speed_mpn_sbpi1_bdiv_qr): New functions.
-	* gmp-impl.h: Provide declarations for corresponding threshold vars.
-	* tune/Makefile.am (TUNE_MPN_SRCS_BASIC): Add dcpi1_divappr_q.c.
-
-	* tune/tuneup.c (tune_binvert): Up max_size.
-
-2009-12-02  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* tests/devel/try.c: Test mpn_rsblsh2_n and mpn_rsblsh_n.
-	* tests/refmpn.c (refmpn_rsblsh_n, refmpn_rsblsh2_n): New functions.
-	(refmpn_rsblsh1_n): Use generic refmpn_rsblsh_n.
-	* tests/tests.h: Declare new functions.
-
 2009-12-03  Niels Möller  <nisse@lysator.liu.se>
 
 	* mpn/generic/gcdext_subdiv_step.c (mpn_gcdext_subdiv_step):
@@ -400,712 +74,30 @@
 
 	* doc/gmp.texi (Low-level Functions): Document mpn_sqr_n.
 
-	* tune/speed.c (routine): Add mpn_binvert.
-
-	* tune/tuneup.c: Tune BINV_NEWTON_THRESHOLD.
-	(tune_binvert): New function.
-	* tune/speed.h (SPEED_ROUTINE_MPN_BINVERT): New macro.
-	* tune/common.c (speed_mpn_binvert): New function.
-	* gmp-impl.h: Provide declarations for corresponding threshold var.
-	* tune/Makefile.am (TUNE_MPN_SRCS_BASIC): Add binvert.c.
-
-	* tune/tuneup.c: Tune DC_BDIV_QR_THRESHOLD and DC_BDIV_Q_THRESHOLD.
-	(tune_dc_bdiv): New function.
-	(tune_dc_div): New name for tune_dc.
-	* tune/speed.h (SPEED_ROUTINE_MPN_PI1_BDIV_QR,
-	SPEED_ROUTINE_MPN_PI1_BDIV_Q): New macros.
-	* tune/common.c (speed_mpn_sbpi1_bdiv_qr, speed_mpn_dcpi1_bdiv_qr,
-	speed_mpn_sbpi1_bdiv_q, speed_mpn_dcpi1_bdiv_q): New functions.
-	* gmp-impl.h: Provide declarations for corresponding threshold vars.
-	* tune/Makefile.am (TUNE_MPN_SRCS_BASIC): Add dcpi1_bdiv_qr.c and
-	dcpi1_bdiv_q.c.
-
-2009-12-01  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/toom53_mul.c: Removed double computation of vinf.
-
-	* mpn/x86_64/aorrlsh_n.asm: Correct return value for rsblsh_n.
-	* mpn/asm-defs.m4 (define_mpn): Add rsblsh_n.
-	* gmp-impl.h (mpn_rsblsh_n): Added prototype and name-mangling.
-
-	* mpn/generic/fib2_ui.c: Reduce the amount of temporary storage.
-	Use mpn_rsblsh_n.
-
-2009-12-01  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/redc_n.c: Rework temp allocation.
-
-	* mpn/generic/dcpi1_bdiv_qr.c (mpn_dcpi1_bdiv_qr_n_itch): Add pi1 also
-	to this function.
-
-	* mpn/generic/dcpi1_bdiv_q.c: Get the mpn_sbpi1_bdiv_q call right.
-	Misc cleanups.
-
-	* tune/speed.c (routine): Fix typo in last change.
-	Add mpn_redc_2.
-
-	* tune/speed.h (SPEED_ROUTINE_REDC_N): Set min size properly.
-
-2009-12-01  Niels Möller  <nisse@lysator.liu.se>
-
-	* tune/speed.c (routine): Added mpn_toom42_mul and mpn_redc_n.
-	* tune/speed.h (SPEED_ROUTINE_MPN_TOOM42_MUL): New macro.
-	(speed_mpn_toom42_mul): Declare function.
-	* tune/common.c (speed_mpn_toom42_mul): New function.
-	* gmp-impl.h (MPN_TOOM42_MUL_MINSIZE): New constant.
-
-2009-11-30  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/fib2_ui.c: Use mpn_rsblsh2_n.
-
-2009-11-29  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86_64/pentium4/gmp-mparam.h
-	(HAVE_NATIVE_mpn_addlsh1_n, HAVE_NATIVE_mpn_sublsh1_n): Don't undef.
-
-	* Makefile.am (EXTRA_DIST): Remove macos.
-
-2009-11-28  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c (tune_redc): Set min_size to 16 for redc_n tuning.
-
-	* mpn/x86_64/sqr_basecase.asm (SQR_TOOM2_THRESHOLD_MAX): Avoid quoting
-	to allow configure.in parse it more easily.  Trim from 120 to 80.
-
-2009-11-28  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/mulmod_bnm1.c: Basecases made simpler, this also corrects
-	a bug affecting previous version.
-
-2009-11-28  Torbjorn Granlund  <tege@gmplib.org>
-
-	* configure.in: Handle atom also in 32-bit mode.
-	* mpn/x86/atom/gmp-mparam.h: New file.
-
-	* gmp-impl.h (MULMOD_BNM1_THRESHOLD): Default.
-
-	* mpn/generic/redc_n.c: Use mpn_mulmod_bnm1 instead of mpn_mul_n.
-
-	* Use TMP_ALLOC_LIMBS consistently.
-	* Finish renaming BITS_PER_MP_LIMB to GMP_LIMB_BITS.
-
-	* macos: Remove entire directory.
-
-2009-11-27  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86_64/corei/gmp-mparam.h: New file.
-	* mpn/x86_64/core2/gmp-mparam.h: Now for just core2.
-	* mpn/powerpc64/mode64/p3/gmp-mparam.h: New file.
-	* mpn/powerpc64/mode64/p4/gmp-mparam.h: New file.
-	* mpn/powerpc64/mode64/p5/gmp-mparam.h: New file.
-
-	* config.guess: Return "corei" for core i7 and core i5.
-	* config.sub: Recognise "corei".
-	* acinclude.m4 (X86_64_PATTERN): Add corei.
-	* configure.in (powerpc): Set up more CPU-specific paths.
-	(x86): Handle corei.
-
-	* mpz/powm.c: Allow input operand overlap also when exponent = 1.
-	Misc cleanups.
-
-2009-11-26  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* tests/mpn/t-mulmod_bnm1.c: New test file.
-
-	* mpn/generic/mullow_n.c: Comments on Mulders' trick implementation.
-
-2009-11-26  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/powm.c: Make comments reflect current code state.
-
-	* tests/devel/try.c: Make mpn_mullow_n testing actually work.
-
-2009-11-25  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpz/powm.c: Clean up unused defs.
-
-2009-11-24  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c (tune_redc): Rewrite.
-	* mpn/generic/powm.c: Use REDC_1_TO_REDC_2_THRESHOLD,
-	REDC_1_TO_REDC_N_THRESHOLD, and REDC_2_TO_REDC_N_THRESHOLD.
-	Get rid of previous REDC params, including LOCAL_REDC_N_THRESHOLD.
-	(WANT_REDC_2): Define.
-	* gmp-impl.h: Corresponding changes.
-
-2009-11-23  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/powm.c: Fix typo.
-	Define LOCAL_REDC_N_THRESHOLD, use in REDC_2_THRESHOLD...
-	REDC_N_THRESHOLD chain.
-
-2009-11-22  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c (tune_mullow): Set min_size to 1.
-
-	* mpn/generic/powm_sec.c: Use just mpn_mul_basecase and
-	mpn_sqr_basecase for multiplication and squaring.
-
-	* tune/tuneup.c: Tune REDC_2_THRESHOLD and REDC_N_THRESHOLD.
-	(tune_redc): New function.
-	(tune_powm): Remove function.
-	* tune/speed.h (SPEED_ROUTINE_REDC_2, SPEED_ROUTINE_REDC_N): New.
-	* tune/common.c (speed_mpn_redc_2, speed_mpn_redc_n): New.
-
-	* mpz/powm.c: Complete rewrite.  Use mpn_powm and mpn_powlo.
-	* mpn/generic/powm.c: Rewrite.
-	* mpn/generic/redc_n.c: New file.
-	* configure.in (gmp_mpn_functions): Add redc_n.
-	* gmp-impl.h (REDC_2_THRESHOLD, REDC_N_THRESHOLD): Default, and define
-	for tuneup.
-
-2009-11-21  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/mullow_n.c: Disable Mulders' trick for small operands,
-	use fft for bigger ones.
-	* tests/mpn/t-mullo.c: New test file.
-
-2009-11-22  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/tuneup.c (tune_mullow): Rewrite.
-
-2009-11-21  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* gmp-impl.h: Removed unused macros (CACHED_ABOVE_THRESHOLD and
-	CACHED_BELOW_THRESHOLD).
-
-	* mpn/generic/mullow_n.c: Use Mulders' trick.
-	* tune/tuneup.c (tune_mullow): MULLOW_MUL_N_THRESHOLD range of
-	search depends on FFT tuning;
-	(all): Anticipate tune_fft_{mul,sqr}.
-
-	* tune/speed.c (routine): Add entry related to mpn_mulmod_bnm1.
-
-2009-11-19  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/toom_eval_dgr3_pm2.c (mpn_toom_eval_dgr3_pm2)
-	[HAVE_NATIVE_mpn_add_n_sub_n]: Fixed typo in mpn_add_n_sub_n call
-	(spotted by Marco Bodrato).
-	* mpn/generic/toom_eval_pm2.c (mpn_toom_eval_pm2): Likewise.
-	* mpn/generic/toom_eval_pm2exp.c (mpn_toom_eval_pm2exp): Likewise.
-
-	* mpn/generic/toom_eval_pm2.c (mpn_toom_eval_pm2) [HAVE_NATIVE_mpn_addlsh_n]:
-	Fixed missing declaration.
-
-	* mpn/asm-defs.m4 (define_mpn): Add addlsh_n.
-	* gmp-impl.h (mpn_addlsh_n): Added prototype and name-mangling.
-
-2009-11-19  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/toom_eval_pm2.c (mpn_toom_eval_pm2): New file.
-	* mpn/generic/toom53_mul.c (mpn_toom53_mul): Use mpn_toom_eval_pm2.
-	* mpn/generic/toom62_mul.c (mpn_toom62_mul): Likewise.
-	* configure.in (gmp_mpn_functions): Added toom_eval_dgr3_pm2.
-
-2009-11-18  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-impl.h (mpn_and_n, etc): Adapt to now-public logic functions.
-
-	* config.guess: Recognise VIA nano.
-	* config.sub: Likewise.
-	* configure.in: Generalise x86_64 support; recognise VIA nano.
-
-2009-11-16  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/speed.c (routine): Add measurement of mpn_addlsh2_n,
-	mpn_sublsh2_n, mpn_rsblsh2_n.
-	* tune/common.c: Add speed routines for lsh2 functions.
-
-	* mpn/generic/divis.c: Use MU_BDIV_QR_THRESHOLD.
-
-	* configure.in (gmp_mpn_functions_optional): Add *lsh_n functions.
-
-	* mpn/generic/toom_eval_pm2exp.c: Make HAVE_NATIVE_mpn_addlsh_n code
-	work.
-
-	* mpn/x86_64/aorrlsh2_n.asm: Optimise inner loop.
-
-	* configure.in (gmp_mpn_functions_optional): Remove copyi,copyd, they
-	are now in gmp_mpn_functions.  Analogously move logical functions.
-
-2009-11-16  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/toom53_mul.c: Use addlsh2 for evaluation (and fix typo).
-	* mpn/generic/toom_eval_dgr3_pm2.c: Likewise (affects toom44 and 43).
-
-	* mpn/asm-defs.m4: Fix comments for op_lsh2 new functions.
-	* gmp-impl.h: Likewise.
-	* tests/mpz/t-fac_ui.c: Fix a comment.
-
-2009-11-15  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86_64/aorrlsh2_n.asm: New file.
-	* configure.in: Add support for addlsh2_n, sublsh2_n, and rsblsh2_n,
-	including mulfuncs.
-	* gmp-impl.h (mpn_addlsh2_n, mpn_sublsh2_n, mpn_rsblsh2_n): Declare.
-	* mpn/asm-defs.m4: Likewise.
-
-	* mpn/generic/copyi.c: New file.
-	* mpn/generic/copyd.c: Likewise.
-	* mpn/generic/zero.c: Likewise.
-	* gmp-h.in: Declare new functions.
-	* configure.in (gmp_mpn_functions): Add new functions.
-
-2009-11-15  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/mulmod_bnm1.c (mpn_mulmod_bnm1_next_size): fix typo
-
-	* mpn/generic/toom33_mul.c: Use rsblsh1 for evaluation.
-	* mpn/generic/toom3_sqr.c: Likewise.
-
-2009-11-14  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/toom52_mul.c: Use mpn_addlsh1_n.
-
-	* mpn/generic/toom52_mul.c: Toggle the right flag bit in an
-	HAVE_NATIVE_mpn_add_n_sub_n arm.
-
-	* tests/mpz/t-remove.c: New file.
-
-	* mpn/generic/remove.c: Major overhaul.  Add parameter 'cap'.
-
-	* mpn/generic/binvert.c: Fix typo in last change.
-
-	* mpn/generic/bdiv_qr.c: Make it actually work.  Also use passed-in
-	scratch space.
-
-	* mpn/generic/mu_bdiv_qr.c: Reset FFT parameters for each call.
-
-2009-11-12  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86/k7/gcd_1.asm (MASK): Compute from MAXSHIFT.
-
-2009-11-11  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/binvert.c: Simplify, fix comments.
-
-	* tests/devel/try.c: Test mpn_invert and mpn_binvert.
-
-	* tests/refmpn.c (refmpn_invert, refmpn_binvert): New functions.
-	* tests/tests.h: Declare new functions.
-
-2009-11-10  Torbjorn Granlund  <tege@gmplib.org>
-
-	* configure.in: Supply compiler options for atom in 32-bit mode.
-
-	* acinclude.m4 (X86_64_PATTERN): New.
-	* configure.in: Setup and use X86_64_PATTERN.
-
-	* mpn/x86_64/fat/fat.c: New file.
-	* mpn/x86_64/fat/fat_entry.asm: New file.
-	* mpn/x86_64/fat: Copy C placeholder files from mpn/x86/fat.
-	* mpn/x86_64/x86_64-defs.m4 (CPUVEC_FUNCS_LIST): New, copied from
-	mpn/x86/x86-defs.m4.
-	* configure.in: Move down x86 fat setup code until after ABI has been
-	determined; generalise to handle x86_64.
-
 2009-11-09  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/x86/fat/mod_1.c: New file.
-
 	* acinclude.m4 (GMP_C_FOR_BUILD_ANSI): Avoid poor quoting.
 
-2009-11-08  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-impl.h (MPN_LOGOPS_N_INLINE): Rewrite, update interface.  Callers
-	updated.
-	* mpn/generic/logops_n.c: New file.
-	* doc/gmp.texi (Low-level Functions): Document logical mpn functions.
-
-2009-11-07  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/speed.h (SPEED_ROUTINE_MPN_MULMOD_BNM1): Adapt to new
-	mpn_mulmod_bnm1 interface.
-
-2009-11-07  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/mulmod_bnm1.c: New interface, with size
-	specified for all operands in mpn_mulmod_bnm1.
-	* gmp-impl.h: Changed mpn_mulmod_bnm1 prototype.
-
-2009-11-05  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86/k7/gcd_1.asm: Actually use div-reduced value.
-	Mnemonic cleanup.
-
-	* mpn/x86_64/gcd_1.asm: New file.
-
-2009-11-03  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/Makefile.am (TUNE_MPN_SRCS_BASIC): Add sqr_n.c.
-
-2009-11-03  Marco Bodrato <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/toom_interpolate_6pts.c: removed an addmul_1 and cleanup.
-
-2009-11-02  Torbjorn Granlund  <tege@gmplib.org>
-
-	* configure.in (gmp_mpn_functions): Remove obsolete functions
-	dc_divrem_n and sb_divrem_mn.
-	* gmp-impl.h: Misc cleanup.
-	(mpn_sb_divrem_mn, mpn_dc_divrem_n): Remove.
-	(DIV_DC_THRESHOLD): Remove.
-	* mpn/generic/dc_divrem_n.c: Remove.
-	* mpn/generic/sb_divrem_mn.c: Remove.
-	* mpn/generic/tdiv_qr.c: Use DC_DIV_QR_THRESHOLD, not DIV_DC_THRESHOLD.
-
-	* tests/devel/try.c: Replace mpn_sb_divrem_mn by mpn_sbpi1_div_qr.
-	* tests/refmpn.c (refmpn_sb_div_qr): New name for refmpn_sb_divrem_mn.
-
-	* tune/Makefile.am (libspeed_la_SOURCES): Remove sb_div.c and sb_inv.c.
-	(TUNE_MPN_SRCS_BASIC): Remove sb_divrem_mn.c.
-	* tune/common.c (speed_mpn_dcpi1_div_qr_n): New function.
-	 Remove mpn_sb_divrem_mn related functions.
-	* tune/speed.c (routine): Remove entries related to mpn_dc_divrem and
-	mpn_sb_divrem.
-	(routine): New entry for mpn_dc_div_qr_n.
-	* tune/speed.h (SPEED_ROUTINE_MPN_DC_DIVREM_CALL): Compute inverse
-	needed by pi1 calls.
-	(SPEED_ROUTINE_MPN_SB_DIVREM_M3): Remove.
-	* tune/tuneup.c (tune_sb_preinv): Remove.
-	(tune_dc): Update to measure DC_DIV_QR_THRESHOLD.
-
-	* mpn/generic/sb_divappr_q.c: Remove.
-
-2009-11-01  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-impl.h: Misc minor cleanups.
-
-2009-10-31  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-impl.h (toom itch functions): Simplify, make some into macros.
-	(MPN_KARA_MUL_N_TSIZE, MPN_KARA_SQR_N_TSIZE): Remove.
-	* mpn/generic/mul_n.c (mpn_toom3_mul_n, mpn_toom3_sqr_n): Remove.
-	* mpn/generic/mul_n.c (mpn_sqr_n): Move from here...
-	* mpn/generic/sqr_n.c: ...to this new file.
-	* configure.in (gmp_mpn_functions): Add sqr_n.
-
-	* Globally change
-	  MUL_TOOM3_THRESHOLD => MUL_TOOM33_THRESHOLD,
-	  MUL_KARATSUBA_THRESHOLD => MUL_TOOM22_THRESHOLD,
-	  SQR_KARATSUBA_THRESHOLD => SQR_TOOM2_THRESHOLD,
-	and associated names analogously.
-
-2009-10-31  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/toom_interpolate_7pts.c: Changed evaluation points,
-	replacing -1/2 by -2.
-	* mpn/generic/toom44_mul.c: Updated to use new evaluation points,
-	and use mpn_toom_eval_dgr3_pm2.
-	* mpn/generic/toom4_sqr.c (mpn_toom4_sqr): Likewise.
-	* mpn/generic/toom53_mul.c (mpn_toom53_mul): Updated to use new
-	evaluation points, and use mpn_toom_eval_pm1 and
-	mpn_toom_eval_pm2exp.
-	* mpn/generic/toom62_mul.c (mpn_toom62_mul): Likewise.
-
-	* mpn/generic/toom_eval_pm2exp.c: New file.
-	* mpn/generic/toom_eval_pm1.c: New file.
-
-	* mpn/generic/toom43_mul.c (mpn_toom43_mul): Use
-	mpn_toom_eval_dgr3_pm2.
-
-2009-10-30  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tune/Makefile.am (TUNE_MPN_SRCS_BASIC): Add toom2* and toom3* files.
-
-2009-10-30  Niels Möller  <nisse@lysator.liu.se>
-
-	* configure.in (gmp_mpn_functions): Added toom_eval_dgr3_pm2.
-	* gmp-impl.h: Added prototype for mpn_toom_eval_dgr3_pm2.
-	* mpn/generic/toom_eval_dgr3_pm2.c: New file.
-
-2009-10-29  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/toom43_mul.c (mpn_toom43_mul): Use
-	mpn_toom_eval_dgr3_pm1.
-	* mpn/generic/toom42_mul.c (mpn_toom42_mul): Likewise.
-
 2009-10-29  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/generic/mulmod_bnm1.c: Replace some add_1 by INCR.
-
-	* gmp-impl.h (mpn_mulmod_bnm1_itch): New macro.
-
-	* mpn/generic/mulmod_bnm1.c (mpn_mulmod_bnm1): Call mpn_mul_fft.
-	(mpn_mulmod_bnm1_next_size): Adopt to SS FFT.
-
 	* mpn/generic/mul_fft.c (mpn_mul_fft): Make it return high limb.
 	(mpn_mul_fft_internal): Likewise.
 
-	* mpn/generic/mulmod_bnm1.c: New file, by Niels Möller.
-	* configure.in (gmp_mpn_functions):  Add mulmod_bnm1.
-	* gmp-impl.h: Add related declarations.
-	* tune/tuneup.c: Tune MULMOD_BNM1_THRESHOLD.
-	* tune/speed.h (SPEED_ROUTINE_MPN_MULMOD_BNM1): New macro.
-	* tune/common.c (speed_mpn_mulmod_bnm1): New function.
-	* Makefile.am (TUNE_MPN_SRCS_BASIC): Add mulmod_bnm1.c.
-
-	* gmp-impl.h (mpn_kara_mul_n, mpn_kara_sqr_n): Remove declarations.
-	* tune/common.c: Remove/rename kara functions.
-	* tune/speed.h: Likewise.
-
-	* tests/devel/try.c: Clean up usage of %p printf arguments.
-
-	* gmp-impl.h: Update MUL/SQR MINSIZE macros to reflect new function
-	names and limitations
-	* tune/tuneup.c: Use updated macro names.
-	* tune/speed.h: Likewise.
-	* tests/devel/try.c: Test new mul/sqr functions, remove old tests.
-
-2009-10-29  Niels Möller  <nisse@lysator.liu.se>
-
-	* tune/speed.c: Added support for mpn_toom4_sqr,
-
-	* tune/speed.h (SPEED_ROUTINE_MPN_TOOM4_SQR): New macro.
-	(SPEED_ROUTINE_MPN_KARA_MUL_N): Deleted.
-	(SPEED_ROUTINE_MPN_TOOM3_MUL_N): Deleted.
-	(SPEED_ROUTINE_MPN_TOOM2_SQR): Use mpn_toom2_sqr_itch.
-
-	* gmp-impl.h (mpn_toom3_mul_n, mpn_toom3_sqr_n): Remove
-	declarations.
-	(mpn_toom2_sqr_itch): Add margin for recursive calls.
-
-2009-10-28  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/mul_n.c (mpn_kara_mul_n): Deleted old Karatsuba
-	implementation.
-	(mpn_kara_sqr_n): Likewise deleted.
-
-	* mpn/generic/mul_n.c (mpn_sqr_n): Use mpn_toom2_sqr and
-	mpn_toom3_sqr, not the old implementations.
-
-	* gmp-impl.h (MPN_TOOM3_MUL_N_TSIZE): Deleted, replaced by
-	mpn_toom33_mul_itch.
-	(MPN_TOOM3_SQR_N_TSIZE): Deleted, replaced by
-	mpn_toom3_sqr_itch.
-	(mpn_toom33_mul_itch): Needs more scratch.
-	(mpn_toom3_sqr_itch): Likewise.
-	* tune/speed.h (SPEED_ROUTINE_MPN_TOOM3_MUL_N): Use
-	mpn_toom33_mul_itch.
-	(SPEED_ROUTINE_MPN_TOOM3_SQR_N): Use mpn_toom3_sqr_itch.
-	* mpn/generic/mul_n.c (mpn_mul_n): Use mpn_toom33_mul_itch.
-	(mpn_sqr_n): Use mpn_toom3_sqr_itch.
-
-	* mpn/generic/toom33_mul.c (mpn_toom33_mul): Avoid TMP_ALLOC. Needs
-	some more supplied scratch instead.
-	* mpn/generic/toom3_sqr.c (mpn_toom3_sqr): Likewise.
-
-2009-10-26  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-impl.h (invert_pi1): Streamline, as suggested by Niels.
-
-2009-10-24  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/bdiv_q.c: Update to call new functions.
-	* mpn/generic/bdiv_qr.c: Likewise.
-	* mpn/generic/binvert.c: Likewise.
-	* mpn/generic/divexact.c: Likewise.
-	* mpn/generic/divis.c: Likewise.
-	* mpn/generic/perfpow.c: Likewise.
-	* mpn/generic/tdiv_qr.c: Likewise.
-	* mpn/generic/dcpi1_bdiv_q.c: New file.
-	* mpn/generic/dcpi1_bdiv_qr.c: New file.
-	* mpn/generic/dcpi1_div_q.c: New file.
-	* mpn/generic/dcpi1_div_qr.c: New file.
-	* mpn/generic/dcpi1_divappr_q.c: New file.
-	* mpn/generic/sbpi1_bdiv_q.c: New file.
-	* mpn/generic/sbpi1_bdiv_qr.c: New file.
-	* mpn/generic/sbpi1_div_q.c: New file.
-	* mpn/generic/sbpi1_div_qr.c: New file.
-	* mpn/generic/sbpi1_divappr_q.c: New file.
-	* mpn/generic/dc_bdiv_q.c: Removed.
-	* mpn/generic/dc_bdiv_qr.c: Removed.
-	* mpn/generic/dc_div_q.c: Removed.
-	* mpn/generic/dc_div_qr.c: Removed.
-	* mpn/generic/dc_divappr_q.c: Removed.
-	* mpn/generic/sb_bdiv_q.c: Removed.
-	* mpn/generic/sb_bdiv_qr.c: Removed.
-	* mpn/generic/sb_div_q.c: Removed.
-	* mpn/generic/sb_div_qr.c: Removed.
-
-	* configure.in (gmp_mpn_functions): Add new division functions, remove
-	obsolete division functions.
-
-	* gmp-impl.h: Add declarations of new division functions, remove
-	corresponding obsolete declarations.
-	(gmp_pi1_t, gmp_pi2_t): New types.
-	(invert_pi1): New macro for computing 2/1 and 3/2 inverses.
-
-2009-10-23  Niels Möller  <nisse@lysator.liu.se>
-
-	* gmp-impl.h (mpn_toom62_mul_itch): New function.
-
-	* tests/mpn/t-toom53.c: New test program.
-	* tests/mpn/t-toom62.c: New test program.
-
 2009-10-23  Torbjorn Granlund  <tege@gmplib.org>
 
 	* mpn/generic/get_d.c: Fix code handling denorms for 64-bit machines.
 	* tests/mpf/t-get_d.c (test_denorms): New function.
 
-2009-10-23  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/toom52_mul.c (mpn_toom52_mul): Use supplied scratch
-	space, not TMP_ALLOC. Interface change, now requires input sizes
-	such that s + t >= 5.
-
-	* gmp-impl.h (mpn_toom52_mul_itch): New function.
-
-	* tests/mpn/t-toom52.c: New test program.
-
-2009-10-22  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86_64/sqr_basecase.asm: Tune for speed and a 7% size decrease.
-
-2009-10-22  Niels Möller  <nisse@lysator.liu.se>
-
-	* tests/mpn/t-toom44.c: New test program.
-	* tests/mpn/t-toom33.c: New test program.
-
-	* tests/mpn/toom-shared.h (main): Reorganized input generation.
-	Users are now supposed to define macros MAX_AN, MIN_BN and MAX_BN.
-	Updated existing toom test programs.
-
-2009-10-22  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tests/devel/try.c: Fix typos in last change.
-
-2009-10-21  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/asm-defs.m4 (define_mpn): Add mullow_basecase.
-
-	* tests/devel/try.c: Test mpn_mullow_n.
-
-	* tests/refmpn.c (refmpn_mullow_n): New function.
-	* tests/tests.h: Declare it.
-
-2009-10-21  Niels Möller  <nisse@lysator.liu.se>
-
-	* tests/mpn/toom-shared.h (main): Check for writes outside of the
-	product or scratch area.
-
-	* gmp-impl.h (mpn_toom43_mul_itch): New function.
-
-	* mpn/generic/toom43_mul.c (mpn_toom43_mul): Use supplied scratch
-	space, not TMP_ALLOC. Interface change, now requires input sizes
-	such that s + t >= 5.
-
-2009-10-20  Niels Möller  <nisse@lysator.liu.se>
-
-	* tests/mpn/toom-shared.h (MIN_BLOCK): New constant, which can be
-	overridden by users. Needed by t-toom42 and t-toom43.
-
-	* tests/mpn/Makefile.am (check_PROGRAMS): Added t-toom32,
-	t-toom42 and t-toom43.
-	* tests/mpn/t-toom43.c: New test program.
-	* tests/mpn/t-toom42.c: New test program.
-	* tests/mpn/t-toom32.c: New test program.
-
-	* tests/mpn/Makefile.am (check_PROGRAMS): Added t-toom22.
-	* tests/mpn/t-toom22.c: New test file.
-	* tests/mpn/toom-shared.h: New file. Test framework for Toom
-	functions.
-
-2009-10-14  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/hgcd.c (mpn_hgcd_itch): Thanks to the new
-	mpn_matrix22_mul_strassen, the scratch need is reduced by 16%.
-
-2009-10-14  Marco Bodrato  <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/matrix22_mul.c (mpn_matrix22_mul_strassen): New
-	Strassen-like algorithm, to reduce the amount of temporary
-	storage.
-	(mpn_matrix22_mul_itch): Updated to reflect the reduced storage
-	need.
-
-2009-10-03  Torbjorn Granlund  <tege@gmplib.org>
-
-	* Rename mpn_addsub_n to mpn_add_n_sub_n.
-
-2009-10-01  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/tdiv_qr.c: Call mpn_divrem_1 and mpn_dc_div_qr instead of
-	old functions.
-
-	* mpn/generic/mul_n.c: Call toom22 and toom33 instead of old functions.
-
-	* mpn/generic/toom42_mul.c (TOOM42_MUL_N_REC): Renamed from
-	TOOM22_MUL_N_REC.  Unconditionally call the generic mpn_mul_n.
-	* mpn/generic/toom32_mul.c: Analogous changes.
-
-2009-09-28  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/x86_64/invert_limb.asm: Rewrite. Exploit cancellation in the
-	Newton iteration.
-
-2009-09-27  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/x86/invert_limb.asm: Reduce register usage. Eliminated $1
-	arguments to add, sub and shift.
-
-2009-09-25  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/x86/invert_limb.asm: New file.
-
-2009-09-24  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/toom33_mul.c: Use new toom functions for all recursive
-	products.
-	* mpn/generic/toom3_sqr.c: Likewise.
-	* mpn/generic/toom44_mul.c: Likewise.
-	* mpn/generic/toom4_sqr.c: Likewise.
-
-	* mpn/generic/add_n.c: Relax operand overlap ASSERTs.
-	* mpn/generic/sub_n.c: Likewise.
-
 2009-09-15  Torbjorn Granlund  <tege@gmplib.org>
 
 	Suggested by Uwe Mueller:
 	* printf/doprnt.c: Use "%ld" for exponent printing.
 	* printf/doprntf.c (__gmp_doprnt_mpf): Make expval "long".
 
-2009-09-14  Torbjorn Granlund  <tege@gmplib.org>
-
-	* configure.in: Handle mingw64.
-	* gmp-impl.h (gmp_intptr_t): Declare.
-	* tests/amd64check.c (calling_conventions_values): Use CNST_LIMB.
-	* tests/memory.c: Use gmp_intptr_t; print pointers using C90 "%p".
-	* tests/misc.c: Use gmp_intptr_t.
-	* tests/mpq/t-get_str.c: Print pointers using C90 "%p".
-
-2009-08-12  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/mod_1_1.c (mpn_mod_1_1p_cps): Remove silly ASSERT code.
-
-	* mpn/asm-defs.m4 (define_mpn): Remove mod_1s_1p, add mod_1_1p.
-
-	* mpn/arm/invert_limb.asm: Complete rewrite.
-
-	* longlong.h: Document LONGLONG_STANDALONE and NO_ASM.
-
 2009-08-05  Torbjorn Granlund  <tege@gmplib.org>
 
 	* tests/mpz/dive_ui.c (check_random): Avoid zero divisors.
 
-2009-07-31  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/mod_1_1.c: Tweak to handle any modulus (possibility
-	pointed out by Per Austrin).
-	(mpn_mod_1_1p): Renamed from mpn_mod_1s_1p.
-	(mpn_mod_1_1p_cps): Renamed from mpn_mod_1s_1p_cps.
-	*mpn/generic/mod_1.c (mpn_mod_1): Reorganise to call mpn_mod_1_1p for
-	any modulus.
-
-2009-07-28  Torbjorn Granlund  <tege@gmplib.org>
-
-	* configure.in: Pass arch for x86 also in 64-bit mode.
-
 2009-07-26  Torbjorn Granlund  <tege@swox.com>
 
 	* config.guess (_cpuid): Recognize more Intel "Core" processors.
@@ -1114,11 +106,7 @@
 
 	* mpf/eq.c: Rewrite.
 
-	* tests/mpf/t-eq.c: New test.
-
-2009-07-06  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-impl.h (__mp_bases): Remove this alias.
+2009-07-05  Torbjorn Granlund  <tege@gmplib.org>
 
 	* mpf/get_str.c: Use less overflow prone expression for computing limb
 	allocation.
@@ -1126,176 +114,13 @@
 	* mpf/set_str.c: Likewise.
 	* mpz/set_str.c: Likewise.
 
-2009-07-03  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/gcd_1.c (mpn_gcd_1): Use masking tricks to reduce
-	the number of branches in the loop.
-
-2009-06-28  Torbjorn Granlund  <tege@gmplib.org>
-
-	* demos/factorize.c (factor_using_pollard_rho): Rewrite.
-
-	* mpz/clears.c: New file.
-	* mpq/clears.c: New file.
-	* mpf/clears.c: New file.
-	* gmp-h.in (mpz_clears, mpq_clears, mpf_clears): Declare.
-	* mpz/Makefile.am: Add clears.c.
-	* mpq/Makefile.am: Add clears.c.
-	* mpf/Makefile.am: Add clears.c.
-	* Makefile.am: Add these also to respective OBJECTS variables.
-	* doc/gmp.texi: Document inits function and clears functions.
-
-2009-06-20  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mp-h.in (mp_bitcnt_t): Declare here too.
-
-2009-06-19  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpq/inits.c: New file.
-	* mpf/inits.c: New file.
-	* gmp-h.in (mpz_inits, mpq_inits, mpf_inits): Declare .
-
-	* mpn/generic/remove.c: New file.
-	* configure.in (gmp_mpn_functions): Add remove.
-	* gmp-impl.h (mpn_remove): Declare.
-
-	* gmp-h.in (mp_bitcnt_t): New basic type.
-	* mpn/generic/perfpow.c (mp_bitcnt_t): Remove private definition.
-
-	* mpn/generic/bdiv_qr.c: Make it actually work.
-
-	* mpn/x86_64/core2/aorsmul_1.asm: Rewrite to use shorter pipeline and
-	to need fewer registers.
-
 2009-06-17  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/x86_64/rsh1aors_n.asm: New file.
-	* mpn/x86_64/rsh1add_n.asm: Remove.
-	* mpn/x86_64/rsh1sub_n.asm: Remove.
-
-	* mpz/inits.c: New file.
-
-	* gen-trialdivtab.c: Wrap limb constants into CNST_LIMB.
-
-	With Martin Boij:
-	* mpn/generic/perfpow.c (binv_root, binv_sqroot): Change from being
-	recursive to being iterative.
-	(mpn_perfect_power_p): Reorganize temp memory usage to avoid a buffer
-	overun.  Trim allocation of next and prev.  Never create oversize
-	products in the multiplicity binary search.
-
-	* mpn/generic/dc_div_q.c: Add missing TMP_FREE.
-
-2009-06-16  Torbjorn Granlund  <tege@gmplib.org>
-
-	Revert:
-	* mpn/generic/perfpow.c (perfpow): Test exponents up to ub, inclusive.
-
-2009-06-16  Martin Boij  <mboij@kth.se>
-
-	* mpn/generic/perfpow.c (logs): Use more conservative table.
-
-2009-06-15  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/pa64/aors_n.asm: New file.
-	* mpn/pa64/add_n.asm: Remove.
-	* mpn/pa64/sub_n.asm: Remove.
-
-	* mpn/generic/perfpow.c (perfpow): Test exponents up to ub, inclusive.
-
-2009-06-14  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86_64/bdiv_q_1.asm: Optimize away a mov insn.
-	* mpn/x86_64/dive_1.asm: Likewise.
-
-	* mpn/generic/perfpow.c (binv_root): Use mpn_bdiv_q_1, not
-	mpn_divexact_itch for 2-adic division.
-	(all functions): Micro optimize.
-
-	* Makefile.am (libmp_la_SOURCES): Add nextprime.c.
-
-2009-06-13  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-h.in (mpn_perfect_power_p): Declare.
-	* configure.in (gmp_mpn_functions): Add perfpow.
-	* mpz/perfpow.c: Now trivial, simply calls mpn_perfect_power_p.
-
-2009-06-13  Martin Boij  <mboij@kth.se>
-
-	* mpn/generic/perfpow.c: New file.
-	* tests/mpz/t-perfpow.c: Rewrite.
+	* mpn/generic/toom44_mul.c: Use TMP_ALLOC for unbounded allocations.
 
 2009-06-12  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/generic/bdiv_qr.c: New file.
-	* mpn/generic/bdiv_q.c: New file.
-	* configure.in (gmp_mpn_functions): Add bdiv_qr and bdiv_q.
-	* gmp-impl.h: Declare new functions.
-
-	* nextprime.c: New file.
-	* gmp-impl.h (gmp_primesieve_t, gmp_init_primesieve, gmp_nextprime):
-	Declare
-	* Makefile.am (libgmp_la_SOURCES): Add nextprime.c.
-
-2009-06-11  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/trialdiv.c: New file.
-	* gen-trialdivtab.c: New file.
-	* configure.in (gmp_mpn_functions): Add trialdiv.
-	* gmp-impl.h (mpn_trialdiv): Declare
-	* Makefile.am: Add rules for gen-trialdivtab and trialdiv.
-
-	* longlong.h (arm count_leading_zeros): Define for armv5.
-
-	* gmp-impl.h: Move down toom itch functions to after we've #defined
-	all THRESHOLDs.
-
-	* dumbmp.c (isprime): Replace with slightly less inefficient code.
-	(mpz_tdiv_r): New function.
-
-2009-06-11  Niels Möller  <nisse@lysator.liu.se>
-
-	Support for mpn_toom32_mul in speed:
-	* tune/speed.c (routine): Added mpn_toom32_mul.
-	* tune/speed.h (SPEED_ROUTINE_MPN_TOOM32_MUL): New macro.
-	* tune/common.c (speed_mpn_toom32_mul): New function.
-
-	* gmp-impl.h (mpn_toom32_mul_itch): Count scratch space needed
-	for the calls to mpn_toom22_mul.
-	(ABOVE_THRESHOLD): Moved this and related macros so it can be used
-	by mpn_toom32_mul_itch.
-	(mpn_toom22_mul_itch): Count scratch space for recursive calls.
-
-2009-06-11  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86/k7/mod_1_4.asm: New file, mainly for k7, but perhaps useful
-	also for k6 and non-sse p6.
-
-2009-06-10  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86_64/mod_1_4.asm: Minor size reducing tweaks.
-
-	* mpn/x86/mod_1.asm: Remove obsolete file.
-	* mpn/x86/k7/mmx/mod_1.asm: Likewise.
-	* mpn/x86/pentium4/sse2/mod_1.asm: Likewise.
-	* mpn/x86/p6/mod_1.asm: Likewise.
-	* mpn/x86/pentium/mod_1.asm: Likewise.
-
-2009-06-08  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/toom4_sqr.c (mpn_toom4_sqr): Reorganized, to reduce
-	the need for scratch space, and get rid of TMP_ALLOC. Also use
-	mpn_toom_eval_dgr3_pm1.
-
-	* mpn/generic/toom_interpolate_6pts.c (mpn_toom_interpolate_6pts):
-	Stricter ASSERTs based on maximum size of polynomial coefficients.
-	Improved comments on the signedness of intermediate values.
-
-2009-06-07  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/toom2_sqr.c: Make it actually work.
-
-	* mpn/generic/toom3_sqr.c: Reduce local scratch space.
+	* mpn/generic/mul.c: Use TMP_ALLOC_LIMBS for unbounded allocations.
 
 2009-06-05  Torbjorn Granlund  <tege@gmplib.org>
 
@@ -1304,187 +129,16 @@
 	FFT_TABLE2_SIZE if they are defined.
 	(struct nk): Use bit field.
 
-2009-06-05  Niels Möller  <nisse@lysator.liu.se>
-
-	* mpn/generic/toom44_mul.c (mpn_toom44_mult): Use
-	mpn_toom_eval_dgr3_pm1.
-
-	* mpn/generic/toom_eval_dgr3_pm1.c: New file.
-
-	* mpn/generic/toom_interpolate_7pts.c (mpn_toom_interpolate_7pts):
-	Minor cleanup, use mpn_add rather than mpn_add_n + MPN_INCR_U.
-
-	* mpn/generic/toom44_mul.c (mpn_toom44_mul): Reorganized, to
-	reduce the need for scratch space, and get rid of TMP_ALLOC.
-
-2009-06-05  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/toom_interpolate_7pts.c: Fall back mpn_divexact_byN to
-	mpn_bdiv_q_1_pi1, if the latter is NATIVE.
-
-2009-06-04  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86_64/bdiv_q_1.asm: New file.
-
-	* configure.in (HAVE_NATIVE): Add recently added functions.
-	(GMP_MULFUNC_CHOICES): Handle addlsh_n, sublsh_n, rsblsh_n.
-
-	* tune/common.c (speed_mpn_bdiv_q_1, speed_mpn_bdiv_q_1_pi1):
-	New functions.
-	* tune/speed.c (routine): Add mpn_bdiv_q_1 and mpn_bdiv_q_1_pi1.
-	* tune/speed.h (SPEED_ROUTINE_MPN_BDIV_Q_1_PI1): New #define.
-	(SPEED_ROUTINE_MPN_BDIV_Q_1): Mew #define.
-
-	* configure.in (gmp_mpn_functions): Add bdiv_q_1.
-	* mpn/generic/bdiv_q_1.c: New file.
-	* mpn/asm-defs.m4 (define_mpn): Add mpn_bdiv_q_1 and mpn_bdiv_q_1_pi1.
-	* gmp-impl.h (mpn_bdiv_q_1, mpn_bdiv_q_1_pi1): Declare.
-
-	* mpn/x86_64/lshift.asm: Cleanup.
-	* mpn/x86_64/rshift.asm: Cleanup.
-
-	* mpn/x86_64/addlsh1_n.asm: Removed.
-	* mpn/x86_64/aorrlsh1_n.asm: Generalized addlsh1_n.asm to handle
-	addlsh1_n and rsblsh1_n functionality.
-
-	* tests/refmpn.c (refmpn_rsblsh1_n): New function.
-	* tests/devel/try.c: Test mpn_rsblsh1_n.
-	* tests/tests.h: Declare refmpn_rsblsh1_n.
-	* tune/common.c (speed_mpn_rsblsh1_n): New function.
-	* tune/speed.c (routine): Add mpn_rsblsh1_n.
-	* tune/speed.h (mpn_rsblsh1_n): Declare.
-
-	* configure.in (gmp_mpn_functions_optional): Add rsblsh1_n.
-	(GMP_MULFUNC_CHOICES): Handle rsblsh1_n defined with a mulfunc.
-	* mpn/asm-defs.m4 (define_mpn): Add rsblsh1_n.
-	* gmp-impl.h (mpn_rsblsh1_n): Declare.
-
-	* mpn/generic/toom32_mul.c: Consistently use TOOM22_MUL_N_REC.
-
-2009-06-03  Marco Bodrato  <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/toom43_mul.c: New file.
-	* mpn/generic/toom52_mul.c: New file.
-	* mpn/generic/toom_interpolate_6pts.c: New file.
-
-2009-06-03  Torbjorn Granlund  <tege@gmplib.org>
-
-	* configure.in (gmp_mpn_functions): Add toom43_mul, toom52_mul, and
-	toom_interpolate_6pts, but also some previously forgotten functions.
-	* mpn/Makefile.am (nodist_EXTRA_libmpn_la_SOURCES): Likewise.
-	* gmp-impl.h: Declare new functions. Sort toom function declarations.
-
-	* gmp-impl.h: Rename  toom4_* flags enum to toom7_*.  Relevant C files
-	updated.
-
-	* mpn/generic/toom_interpolate_7pts (divexact_2exp): Remove.
-
-2009-06-02  Torbjorn Granlund  <tege@gmplib.org>
-
-	* demos/factorize.c: Add -q command line option.
-
-2009-06-02  Marco Bodrato  <bodrato@mail.dm.unipi.it>
-
-	* mpn/generic/toom_interpolate_7pts.c: Streamline, resulting in speed
-	improvements.
-
-	* mpn/generic/toom_interpolate_5pts.c: Likewise, but also completely
-	do away with explicit scratch space.
-	* gmp-impl.h (mpn_toom_interpolate_5pts): Update prototype.
-
-	* mpn/generic/mul_n.c (mpn_toom3_sqr_n, mpn_toom3_mul_n):
-	Update toom_interpolate_5pts call without scratch space parameter.
-	* mpn/generic/toom3_sqr.c: Likewise.
-	* mpn/generic/toom42_mul.c: Likewise.
-	* mpn/generic/toom33_mul.c: Likewise.
-
-	* mpn/generic/toom33_mul.c: Reduce local scratch space.
-	* mpn/generic/toom32_mul.c: Rewrite to not use local scratch space.
-
-2009-06-02  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/toom22_mul.c (TOOM22_MUL_MN_REC): New macro, use it for
-	oo point.
-
 2009-06-01  Torbjorn Granlund  <tege@gmplib.org>
 
-	* mpn/generic/mul.c: Loop to avoid excessive recursion in toom33 and
-	toom44 slicing code.
-
 	* mpz/remove.c: Correctly handle multiplicity that does not fit an int.
 
 	* Makefile.am (dist-hook): Check library version consistency.
 
-	* mpn/generic/mul.c: Rewrite.
-
-2009-05-29  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tests/mpz/t-divis.c (check_random): Create huge test operands.
-
-	* mpn/generic/toom44_mul.c: Allocate temp space using one TMP_ALLOC
-	call, not multiple TMP_SALLOC.
-	* mpn/generic/toom4_sqr.c: Likewise.
-
-	* gmp-impl.h (mpn_toom22_mul_itch): Replace totally wrong code.
-
-	* mpn/generic/mullow_n.c: Relax overlap requirement implied by ASSERT.
-
-	* mpn/generic/divis.c: Rewrite.
-
-	* gmp-impl.h (mpn_mu_bdiv_qr): Now returns mp_limb_t.
-	(mpn_toom2_sqr_itch): Simplify.
-
-	* mpn/generic/mu_bdiv_qr.c: Implement properly.
-
-2009-05-27  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/generic/mod_1_1.c: Add proper ASSERT functionality cps function.
-	* mpn/generic/mod_1_2.c: Likewise.
-	* mpn/generic/mod_1_3.c: Likewise.
-	* mpn/generic/mod_1_4.c: Likewise.
-
-	* tune: Add speed measuring of toom22, toom33, and toom44.
-
-	* mpn/generic/toom22_mul.c: Handle potentially unbalanced coefficient
-	product better.
-
-2009-05-26  Torbjorn Granlund  <tege@gmplib.org>
-
-	* tests/mpz/t-mul.c (ref_mpn_mul): Use mpn_toom44_mul in FFT range for
-	better huge-operands performance.
-
 2009-05-24  Torbjorn Granlund  <tege@gmplib.org>
 
 	* acinclude.m4 (GMP_ASM_LSYM_PREFIX): Try "$L" too, before "$".
 
-2009-05-23  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-impl.h (mpn_mod_1s_1p,mpn_mod_1s_2p,mpn_mod_1s_3p,mpn_mod_1s_4p):
-	Declare using __GMP_ATTRIBUTE_PURE.
-
-	* tune/tuneup.c (tune_mod_1): Specify check_size for measuring mod_1_N
-	functions.
-	(one): Remove redundant size loop exit condition.
-
-2009-05-20  Torbjorn Granlund  <tege@gmplib.org>
-
-	* mpn/x86/pentium4/sse2/mod_1_4.asm: New file.
-	* mpn/x86/p6/sse2/mod_1_4.asm: New file (grabbing pentium4 code).
-
-2009-05-18  Torbjorn Granlund  <tege@gmplib.org>
-
-	* gmp-h.in (__GNU_MP_VERSION_MINOR): Bump to 4.
-	(__GNU_MP_VERSION_PATCHLEVEL): Set to -1.
-
-	* mpn/x86_64/mod_1_4.asm: New file.
-
-	* mpn/asm-defs.m4: Correct names for mod_1_N functions.
-	Add defines for corresponding cps functions.
-
-	* mpn/generic/mod_1_2.c: Suppport any sizes > 1.
-	* mpn/generic/mod_1_3.c: Likewise.
-	* mpn/generic/mod_1_4.c: Likewise.
-
 2009-05-12  Torbjorn Granlund  <tege@gmplib.org>
 
 	* Version 4.3.1 released.
@@ -1577,9 +231,6 @@
 	* mpn/x86_64/darwin.m4: Likewise.
 	* mpn/x86_64/sqr_basecase.asm: Rework switch code using JUMPTABSECT.
 
-	* tune/common.c (speed_mpn_hgcd, speed_mpn_hgcd_lehmer):
-	Remove an unused variable.
-
 	* mpn/x86/x86-defs.m4 (LEA): Get SIZE arguments right.
 
 2009-04-14  Torbjorn Granlund  <tege@gmplib.org>
@@ -2807,7 +1458,7 @@
 
 	* mpn/x86/p6/aors_n.asm: Table cycle counts.
 
-	* mpn/x86/k7/mod_34lsub1.asm: Fix over-optimistic cycle count claims.
+	* mpn/x86/k7/mod_34lsub1.asm: Fix over-optimisitc cycle count claims.
 
 	* mpn/x86/x86-defs.m4 (DEF_OBJECT, END_OBJECT): New define's.
 
@@ -4704,7 +3355,7 @@
 	(what_objects): Only demand 9 object files, as for instance occurs in
 	the scanf directory.
 	(asm files): Transform labels L(foo) -> Lfoo.  Take func name from
-	PROLOGUE to support empty "EPILOGUE()".  Recognise and substitute
+	PROLOGUE to support empty "EPILOGUE()".  Recognise and subsitute
 	register name "define()"s.
 	* macos/Makefile.in (CmnObjs): Add tal-notreent.o.
 
@@ -5127,7 +3778,7 @@
 	* tests/mpn/t-hgcd.c: Remove unused variables.
 
 	* mpn/ia64/hamdist.asm: Remove bundling incompatible with HP-UX
-	assembler.  Misc HP-UX changes.
+	assember.  Misc HP-UX changes.
 	* mpn/ia64/gcd_1.asm: Add some syntax to placid the HP-UX assembler.
 
 2004-02-11  Kevin Ryde  <kevin@swox.se>
@@ -6345,7 +4996,7 @@
 
 2003-10-21  Torbjorn Granlund  <tege@swox.com>
 
-	* mpn/ia64/submul_1.asm: Slightly reschedule loop to accommodate
+	* mpn/ia64/submul_1.asm: Slightly reschedule loop to accomodate
 	Itanium 2 getf.sig latency.
 
 2003-10-21  Kevin Ryde  <kevin@swox.se>
@@ -8966,7 +7617,7 @@
 	* tune/tuneup.c (sqr_karatsuba_threshold): Initialize to
 	TUNE_SQR_KARATSUBA_MAX so mpn_sqr_n works for randmt initialization.
 
-	* gmp.texi (Integer Comparisons): Remove mention of non-existent
+	* gmp.texi (Integer Comparisons): Remove mention of non-existant
 	mpz_cmpabs_si, reported by Conrad Curry.
 
 	* tune/speed.c, tune/speed.h, tune/common.c: Add gmp_randseed,
@@ -16818,7 +15469,7 @@
 
 	* mp.h (mp_set_memory_functions): Add missing #define.
 	* mpbsd/tests/allfuns.c (mp_set_memory_functions): Verify its
-	existence.
+	existance.
 
 	* mpf/tests/t-misc.c (check_mpf_getset_prec): New test, verifying
 	reverted behaviour of mpf_get_prec.
@@ -18125,7 +16776,7 @@
 	* mpn/powerpc32: Use dnl/C instead of `#' for comments.
 
 	* config.guess: Get "model" limit between pentium 2 and pentium3 right.
-	Get rid of code determining `_' prefix; use double labels instead.
+	Get rid of code determining `_' prefix;	use double labels instead.
 	* config.guess: Partially merge with FSF version of April 22.
 	(Don't bring over NetBSD changes for now.)
 
@@ -18931,7 +17582,7 @@
 
 	* randraw.c (gmp_rand_getraw): Handle the case where (1) the LC
 	scheme doesn't generate even limbs and (2) more than one LC
-	invocation is necessary to produce the requested number of bits.
+	invokation is necessary to produce the requested number of bits.
 
 2000-04-05  Torbjorn Granlund  <tege@swox.com>
 
@@ -20983,7 +19634,7 @@
 	* configure.in (i[3456]86* etc): Check if using gcc before
 	choosing mt-x86.
 
-	* configure.in (m68*-*-*): New alternative.
+	* configure.in (m68*-*-*): New alterantive.
 	* config/mt-m68k: New file.
 
 	* mpn/alpha/invert-limb.s: Put tables in text segment,
@@ -21307,7 +19958,7 @@
 	* mpn/x86/pentium/[lr]shift.S: Likewise.
 	* mpn/config/t-oldgas (SFLAGS): Pass -DOLD_GAS.
 
-	* gmp-impl.h: In code for determining endianness, test also
+	* gmp-impl.h: In code for determining endianess, test also
 	__BIG_ENDIAN__ and __hppa__.  Remove test of __NeXT__.
 
 Wed Oct 16 03:50:34 1996  Torbjorn Granlund  <tege@quiet.matematik.su.se>
@@ -24583,7 +23234,7 @@
 	* dist-Makefile: Go via tmp- files for cre* redirection.
 	* dist-Makefile: Add tmp* to "clean" target.
 
-	* dist-Makefile: Use LOCAL_CC for cre*, to simplify cross
+	* dist-Makefile: Use LOCAL_CC for cre*, to simplyfy cross
 	  compilation.
 
 	* gmp.h, mp.h: Don't define NULL here.
@@ -24821,7 +23472,7 @@
 
 Sat Apr 27 21:03:11 1991  Torbjorn Granlund  (tege@zevs.sics.se)
 
-	* Install multiplication using Karatsuba's algorithm as default.
+	* Install multplication using Karatsuba's algorithm as default.
 
 Fri Apr 26 01:03:57 1991  Torbjorn Granlund  (tege@zevs.sics.se)
 
@@ -24839,7 +23490,7 @@
 
 Mon Apr 22 01:31:57 1991  Torbjorn Granlund  (tege@zevs.sics.se)
 
-	* karatsuba.c: New file for Karatsuba's multiplication algorithm.
+	* karatsuba.c: New file for Karatsuba's multplication algorithm.
 
 	* mpz_random, mpz_init, mpz_mod_2exp: New files and functions.
 
@@ -24934,7 +23585,7 @@
 
 Thu Mar 14 18:45:28 1991  Torbjorn Granlund  (tege@musta.nada.kth.se)
 
-	* mpq_mul.c: New file for rational multiplication.
+	* mpq_mul.c: New file for rational multplication.
 
 	* gmp.h: Add definitions for rational arithmetics.
 
--- 1/config.guess
+++ 2/config.guess
@@ -735,7 +735,7 @@
 	  else if (model <= 12)	modelstr = "pentium3";
 	  else if (model <= 14)	modelstr = "pentiumm";
 	  else if (model <= 25)	modelstr = "core2";
-	  else if (model <= 27)	modelstr = "corei"; /* core i5, i7 */
+	  else if (model <= 27)	modelstr = "core2"; /* core i5, i7 */
 	  else if (model == 28)	modelstr = "atom";
 	  else if (model == 29)	modelstr = "core2";
 	  break;
@@ -775,8 +775,7 @@
 	{
 	case 6:
 	  if (model < 9)	modelstr = "viac3";
-	  else if (model < 15)	modelstr = "viac32";
-	  else			modelstr = "nano";
+	  else			modelstr = "viac32";
 	  break;
 	}
     }
--- 1/config.sub
+++ 2/config.sub
@@ -89,9 +89,9 @@
 case "$given_cpu" in
 itanium | itanium2)
   test_cpu=ia64 ;;
-pentium | pentiummmx | pentiumpro | pentium[234m] | k[56] | k6[23] | geode | athlon | viac3* | nano)
+pentium | pentiummmx | pentiumpro | pentium[234m] | k[56] | k6[23] | geode | athlon | viac3*)
   test_cpu=i386 ;;
-athlon64 | atom | core2 | corei | opteron)
+athlon64 | atom | core2 | opteron)
   test_cpu=x86_64 ;;
 power[2-9] | power2sc)
   test_cpu=power ;;
--- 1/configure.in
+++ 2/configure.in
@@ -4,7 +4,7 @@
 define(GMP_COPYRIGHT,[[
 
 Copyright 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006,
-2007, 2008, 2009 Free Software Foundation, Inc.
+2007 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -250,7 +250,6 @@
 #undef HAVE_HOST_CPU_FAMILY_power
 #undef HAVE_HOST_CPU_FAMILY_powerpc
 #undef HAVE_HOST_CPU_FAMILY_x86
-#undef HAVE_HOST_CPU_FAMILY_x86_64
 
 /* Define one of the following to 1 for the host CPU, as per the output of
    ./config.guess.  If your CPU is not listed here, leave all undefined.  */
@@ -407,14 +406,11 @@
   alpha*-*-*)
     AC_DEFINE(HAVE_HOST_CPU_FAMILY_alpha)
     case $host_cpu in
-      alphaev5* | alphapca5*)
-      	path="alpha/ev5 alpha" ;;
+      alphaev5* | alphapca5*) path="alpha/ev5 alpha" ;;
       alphaev67 | alphaev68 | alphaev7*)
-        path="alpha/ev67 alpha/ev6 alpha" ;;
-      alphaev6)
-	path="alpha/ev6 alpha" ;;
-      *)
-        path="alpha" ;;
+        path="alpha/ev67 alpha/ev6 alpha/ev5 alpha" ;;
+      alphaev6* | alphaev7*)  path="alpha/ev6 alpha/ev5 alpha" ;;
+      *)                      path="alpha" ;;
     esac
     extra_functions="cntlz"
     gcc_cflags_optlist="asm cpu oldas" # need asm ahead of cpu, see below
@@ -572,6 +568,7 @@
     # checking sizeof(long), either 4 or 8 bytes respectively.  Do this in
     # ABI=1.0 too, in case someone tries to build that with a 2.0w gcc.
     #
+    gcc_cflags="-O2"
     gcc_cflags_optlist="arch"
     gcc_testlist="sizeof-long-4"
     SPEED_CYCLECOUNTER_OBJ=hppa.lo
@@ -854,7 +851,6 @@
     gcc_cflags_subtype="-force_cpusubtype_ALL"	# for vmx on darwin
     gcc_cflags_asm=""
     gcc_cflags_cpu=""
-    vmx_path=""
 
     # grab this object, though it's not a true cycle counter routine
     SPEED_CYCLECOUNTER_OBJ=powerpc.lo
@@ -896,8 +892,7 @@
       powerpc604)   gcc_cflags_cpu="-mcpu=604" ;;
       powerpc604e)  gcc_cflags_cpu="-mcpu=604e -mcpu=604" ;;
       powerpc620)   gcc_cflags_cpu="-mcpu=620" ;;
-      powerpc630)   gcc_cflags_cpu="-mcpu=630"
-		    cpu_path="p3" ;;
+      powerpc630)   gcc_cflags_cpu="-mcpu=630" ;;
       powerpc740)   gcc_cflags_cpu="-mcpu=740" ;;
       powerpc7400 | powerpc7410)
 		    gcc_cflags_asm="-Wa,-maltivec"
@@ -910,15 +905,7 @@
       powerpc821)   gcc_cflags_cpu="-mcpu=821" ;;
       powerpc823)   gcc_cflags_cpu="-mcpu=823" ;;
       powerpc860)   gcc_cflags_cpu="-mcpu=860" ;;
-      powerpc970)   gcc_cflags_cpu="-mtune=970"
-		    vmx_path="powerpc64/vmx"
-		    cpu_path="p4" ;;
-      power4)	    gcc_cflags_cpu="-mtune=power4"
-		    cpu_path="p4" ;;
-      power5)	    gcc_cflags_cpu="-mtune=power5 -mtune=power4"
-		    cpu_path="p5 p4" ;;
-      power6)	    gcc_cflags_cpu="-mtune=power6"
-		    cpu_path="p6" ;;
+      powerpc970)   gcc_cflags_cpu="-mcpu=970" ;;
     esac
 
     case $host in
@@ -947,6 +934,10 @@
 
     case $host in
       POWERPC64_PATTERN)
+        case $host_cpu in
+	  powerpc970)		vmx_path="powerpc64/vmx" ;;
+	  *)			vmx_path="" ;;
+	esac
         case $host in
           *-*-aix*)
             # On AIX a true 64-bit ABI is available.
@@ -961,9 +952,7 @@
             # Must indicate object type to ar and nm
 	    ar_aix64_flags="-X64"
 	    nm_aix64_flags="-X64"
-	    path_aix64=""
-	    for i in $cpu_path; do path_aix64="${path_aix64}powerpc64/mode64/$i "; done
-            path_aix64="${path_aix64}powerpc64/mode64 $vmx_path powerpc64"
+            path_aix64="powerpc64/mode64 $vmx_path powerpc64"
             # grab this object, though it's not a true cycle counter routine
             SPEED_CYCLECOUNTER_OBJ_aix64=powerpc64.lo
             cyclecounter_size_aix64=0
@@ -978,7 +967,7 @@
             # longlong.h macros expect limb operands in a single 64-bit
             # register, not two 32-bit registers as would be given for a
             # long long without -mpowerpc64.  In theory we could detect and
-            # accommodate both styles, but the proper 64-bit registers will
+            # accomodate both styles, but the proper 64-bit registers will
             # be fastest and are what we really want to use.
             #
 	    # One would think -mpowerpc64 would set the assembler in the right
@@ -1001,9 +990,7 @@
 	    gcc_mode64_cflags="-m64"
 	    gcc_mode64_cflags_optlist="cpu opt"
 	    gcc_mode64_cflags_opt="-O3 -O2 -O1"
-	    path_mode64=""
-	    for i in $cpu_path; do path_mode64="${path_mode64}powerpc64/mode64/$i "; done
-	    path_mode64="${path_mode64}powerpc64/mode64 $vmx_path powerpc64"
+	    path_mode64="powerpc64/mode64 $vmx_path powerpc64"
             SPEED_CYCLECOUNTER_OBJ_mode64=powerpc64.lo
             cyclecounter_size_mode64=0
 	    any_mode64_testlist="sizeof-long-8"
@@ -1034,9 +1021,7 @@
 	    gcc_mode64_cflags_maybe="-m64"
 	    gcc_mode64_cflags_optlist="cpu opt"
 	    gcc_mode64_cflags_opt="-O3 -O2 -O1"
-	    path_mode64=""
-	    for i in $cpu_path; do path_mode64="${path_mode64}powerpc64/mode64/$i "; done
-	    path_mode64="${path_mode64}powerpc64/mode64 $vmx_path powerpc64"
+	    path_mode64="powerpc64/mode64 $vmx_path powerpc64"
             SPEED_CYCLECOUNTER_OBJ_mode64=powerpc64.lo
             cyclecounter_size_mode64=0
 	    any_mode64_testlist="sizeof-long-8"
@@ -1226,15 +1211,7 @@
           *) abilist="64 32" ;;
         esac
 
-	case $host_cpu in
-	  ultrasparc | ultrasparc2 | ultrasparc2i)
-	    path_64="sparc64/ultrasparc12 sparc64" ;;
-	  ultrasparc3)
-	    path_64="sparc64/ultrasparc34 sparc64/ultrasparc12 sparc64" ;;
-	  *)
-	    path_64="sparc64"
-	esac
-
+        path_64="sparc64"
         cclist_64="gcc"
         any_64_testlist="sizeof-long-8"
 
@@ -1304,7 +1281,7 @@
   # mode, in case -m32 has failed not because it's an old gcc, but because
   # it's a dual 32/64-bit gcc without a 32-bit libc, or whatever.
   #
-  X86_PATTERN | X86_64_PATTERN)
+  X86_PATTERN | athlon64-*-* | atom-*-* | core2-*-* | x86_64-*-*)
     abilist="32"
     cclist="gcc icc cc"
     gcc_cflags="$gcc_cflags $fomit_frame_pointer"
@@ -1428,14 +1405,10 @@
         gcc_cflags_cpu="-mtune=k8 -mcpu=athlon -mcpu=pentiumpro -mcpu=i486 -m486"
         gcc_cflags_arch="-march=k8 -march=k8~-mno-sse2 -march=athlon -march=pentiumpro -march=pentium"
         ;;
-      core2 | corei)
+      core2)
         gcc_cflags_cpu="-mtune=core2 -mtune=k8"
         gcc_cflags_arch="-march=core2 -march=core2~-mno-sse2 -march=k8 -march=k8~-mno-sse2"
         ;;
-      atom)
-        gcc_cflags_cpu="-mtune=atom -mtune=pentium3"
-        gcc_cflags_arch="-march=atom -march=pentium3"
-        ;;
       *)
         gcc_cflags_cpu="-mtune=i486 -mcpu=i486 -m486"
         gcc_cflags_arch="-march=i486"
@@ -1450,8 +1423,7 @@
       i686 | pentiumpro)    path="x86/p6 x86" ;;
       pentium2)             path="x86/p6/mmx x86/p6 x86" ;;
       pentium3)             path="x86/p6/p3mmx x86/p6/mmx x86/p6 x86";;
-      pentiumm | core2 | corei)
-                            path="x86/p6/sse2 x86/p6/p3mmx x86/p6/mmx x86/p6 x86";;
+      pentiumm | core2)     path="x86/p6/sse2 x86/p6/p3mmx x86/p6/mmx x86/p6 x86";;
       [k6[23]])             path="x86/k6/k62mmx x86/k6/mmx x86/k6 x86" ;;
       k6)                   path="x86/k6/mmx x86/k6 x86" ;;
       geode)                path="x86/k6/k62mmx x86/k6/mmx x86/k6 x86" ;;
@@ -1462,55 +1434,64 @@
       # VIA/Centaur processors, sold as CyrixIII and C3.
       viac32)               path="x86/p6/p3mmx x86/p6/mmx x86/p6 x86";;
       viac3*)               path="x86/pentium/mmx x86/pentium x86";;
-      atom)                 path="x86/atom x86" ;;
       *)                    path="x86" ;;
     esac
 
-    case $host in
-      X86_64_PATTERN)
-	cclist_64="gcc"
-	gcc_64_cflags="$gcc_64_cflags -m64"
-	gcc_64_cflags_optlist="cpu arch"
-	CALLING_CONVENTIONS_OBJS_64='amd64call.lo amd64check$U.lo'
-	SPEED_CYCLECOUNTER_OBJ_64=x86_64.lo
-	cyclecounter_size_64=2
-	abilist="64 32"
-	path_64="x86_64"
+    # If the user asked for a fat build, override the path and flags set above
+    if test $enable_fat = yes; then
+      gcc_cflags_cpu=""
+      gcc_cflags_arch=""
+      extra_functions="$extra_functions fat fat_entry"
+      path="x86/fat x86"
+      fat_path="x86 x86/fat x86/i486
+		x86/k6 x86/k6/mmx x86/k6/k62mmx
+		x86/k7 x86/k7/mmx
+		x86/pentium x86/pentium/mmx
+		x86/p6 x86/p6/mmx x86/p6/p3mmx x86/p6/sse2
+		x86/pentium4 x86/pentium4/mmx x86/pentium4/sse2"
+      fat_functions="add_n addmul_1 copyd copyi
+		     dive_1 diveby3 divrem_1 gcd_1 lshift
+		     mod_1 mod_34lsub1 mode1o mul_1 mul_basecase
+		     pre_divrem_1 pre_mod_1 rshift
+		     sqr_basecase sub_n submul_1"
+      fat_thresholds="MUL_KARATSUBA_THRESHOLD MUL_TOOM3_THRESHOLD
+		      SQR_KARATSUBA_THRESHOLD SQR_TOOM3_THRESHOLD"
+    fi
+
+    case $host_cpu in
+      athlon64 | atom | core2 | pentium4 | x86_64)
+        cclist_64="gcc"
+        gcc_64_cflags="$gcc_64_cflags -m64"
+        gcc_64_cflags_optlist="cpu"
+        CALLING_CONVENTIONS_OBJS_64='amd64call.lo amd64check$U.lo'
+        SPEED_CYCLECOUNTER_OBJ_64=x86_64.lo
+        cyclecounter_size_64=2
+
+        case $host in
+          *-*-solaris*)
+            # Sun cc.
+            cclist_64="$cclist_64 cc"
+            cc_64_cflags="-xO3 -m64"
+            ;;
+        esac
 
 	case $host_cpu in
-	  x86_64)
-	    ;;
-	  athlon64)
-	    path_64="x86_64/k8 $path_64"
+	  athlon64 | x86_64)
+	    abilist="64 32"
+	    path_64="x86_64"
 	    ;;
 	  pentium4)
-	    path_64="x86_64/pentium4 $path_64"
+	    abilist="64 32"
+	    path_64="x86_64/pentium4 x86_64"
 	    ;;
 	  core2)
-	    path_64="x86_64/core2 $path_64"
-	    ;;
-	  corei)
-	    path_64="x86_64/corei x86_64/core2 $path_64"
+	    abilist="64 32"
+	    path_64="x86_64/core2 x86_64"
 	    ;;
 	  atom)
-	    path_64="x86_64/atom $path_64"
-	    ;;
-	  nano)
-	    path_64="x86_64/nano $path_64"
-	    ;;
-	esac
-
-	case $host in
-	  *-*-solaris*)
-	    # Sun cc.
-	    cclist_64="$cclist_64 cc"
-	    cc_64_cflags="-xO3 -m64"
-	    ;;
-	  *-*-mingw*)
-	    limb_64=longlong
-	    path_64=""	# Windows amd64 calling conventions are *different*
-	    # Silence many pedantic warnings for w64.  FIXME.
-	    gcc_64_cflags="$gcc_64_cflags -std=gnu99"
+	    # The AMD K8/K9/K10 code seems best for Intel Atom
+	    abilist="64 32"
+	    path_64="x86_64/atom x86_64"
 	    ;;
 	esac
 	;;
@@ -1711,10 +1692,10 @@
           cflags="$cflags_maybe $cflags"
         fi
 
-        # Any user CFLAGS, even an empty string, takes precedence
+        # Any user CFLAGS, even an empty string, takes precendence
         if test "$test_CFLAGS" = set; then cflags=$CFLAGS; fi
 
-        # Any user CPPFLAGS, even an empty string, takes precedence
+        # Any user CPPFLAGS, even an empty string, takes precendence
                                eval cppflags=\"\$${ccbase}${abi1}_cppflags\"
         test -n "$cppflags" || eval cppflags=\"\$${ccbase}${abi2}_cppflags\"
         if test "$test_CPPFLAGS" = set; then cppflags=$CPPFLAGS; fi
@@ -1789,42 +1770,6 @@
   AC_MSG_ERROR([could not find a working compiler, see config.log for details])
 fi
 
-case $host in
-  X86_PATTERN | X86_64_PATTERN)
-    # If the user asked for a fat build, override the path and flags set above
-    if test $enable_fat = yes; then
-      gcc_cflags_cpu=""
-      gcc_cflags_arch=""
-
-      if test "$abi" = 32; then
-	extra_functions="$extra_functions fat fat_entry"
-	path="x86/fat x86"
-	fat_path="x86 x86/fat x86/i486
-		  x86/k6 x86/k6/mmx x86/k6/k62mmx
-		  x86/k7 x86/k7/mmx
-		  x86/pentium x86/pentium/mmx
-		  x86/p6 x86/p6/mmx x86/p6/p3mmx x86/p6/sse2
-		  x86/pentium4 x86/pentium4/mmx x86/pentium4/sse2"
-      fi
-
-      if test "$abi" = 64; then
-	gcc_64_cflags=""
-	extra_functions_64="$extra_functions_64 fat fat_entry"
-	path_64="x86_64/fat x86_64"
-	fat_path="x86_64 x86_64/fat x86_64/pentium4 x86_64/core2 x86_64/corei x86_64/atom x86_64/nano"
-      fi
-
-      fat_functions="add_n addmul_1 copyd copyi
-		     dive_1 diveby3 divrem_1 gcd_1 lshift
-		     mod_1 mod_34lsub1 mode1o mul_1 mul_basecase
-		     pre_divrem_1 pre_mod_1 rshift
-		     sqr_basecase sub_n submul_1"
-      fat_thresholds="MUL_TOOM22_THRESHOLD MUL_TOOM33_THRESHOLD
-		      SQR_TOOM2_THRESHOLD SQR_TOOM3_THRESHOLD"
-    fi
-    ;;
-esac
-
 
 if test $found_compiler = yes; then
 
@@ -2092,7 +2037,7 @@
 # enough assembler.
 #
 case $host in
-  X86_PATTERN | X86_64_PATTERN)
+  X86_PATTERN | athlon64-*-* | atom-*-* | core2-*-* | x86_64-*-*)
     if test "$ABI" = 32; then
       case "$path $fat_path" in
         *mmx*)   GMP_ASM_X86_MMX( , [GMP_STRIP_PATH(*mmx*)]) ;;
@@ -2347,8 +2292,7 @@
 #
 # the default includes are sufficient for all these types
 #
-AC_CHECK_TYPES([intmax_t, long double, long long, ptrdiff_t, quad_t,
-		uint_least32_t, intptr_t])
+AC_CHECK_TYPES([intmax_t, long double, long long, ptrdiff_t, quad_t, uint_least32_t])
 
 AC_C_STRINGIZE
 
@@ -2463,51 +2407,43 @@
 # can provide some functions too.  (mpn/Makefile.am passes
 # -DOPERATION_<func> to get them to generate the right code.)
 
+# Note: The following lines defining $gmp_mpn_functions_optional
+#       and $gmp_mpn_functions are parsed by the "macos/configure"
+#       Perl script. So if you change the lines in a major way
+#       make sure to run and examine the output from
+#
+#           % (cd macos; perl configure)
+#
 # Note: $gmp_mpn_functions must have mod_1 before pre_mod_1 so the former
 #       can optionally provide the latter as an extra entrypoint.  Likewise
 #       divrem_1 and pre_divrem_1.
 
-gmp_mpn_functions_optional="umul udiv com_n				\
+gmp_mpn_functions_optional="umul udiv copyi copyd com_n
+  and_n andn_n nand_n ior_n iorn_n nior_n xor_n xnor_n			\
   invert_limb sqr_diagonal						\
   mul_2 mul_3 mul_4							\
   addmul_2 addmul_3 addmul_4 addmul_5 addmul_6 addmul_7 addmul_8	\
-  addlsh1_n sublsh1_n rsblsh1_n rsh1add_n rsh1sub_n			\
-  addlsh2_n sublsh2_n rsblsh2_n						\
-  addlsh_n sublsh_n rsblsh_n						\
-  add_n_sub_n addaddmul_1msb0 lshiftc"
+  addlsh1_n sublsh1_n rsh1add_n rsh1sub_n addsub_n addaddmul_1msb0 lshiftc"
 
 gmp_mpn_functions="$extra_functions					   \
   add add_1 add_n sub sub_1 sub_n neg_n mul_1 addmul_1			   \
   submul_1 lshift rshift dive_1 diveby3 divis divrem divrem_1 divrem_2     \
   fib2_ui mod_1 mod_34lsub1 mode1o pre_divrem_1 pre_mod_1 dump		   \
   mod_1_1 mod_1_2 mod_1_3 mod_1_4					   \
-  mul mul_fft mul_n sqr_n mul_basecase sqr_basecase random random2 pow_1   \
-  rootrem sqrtrem get_str set_str scan0 scan1 popcount hamdist cmp	   \
-  perfsqr perfpow							   \
+  mul mul_fft mul_n mul_basecase sqr_basecase random random2 pow_1	   \
+  rootrem sqrtrem get_str set_str scan0 scan1 popcount hamdist cmp perfsqr \
   bdivmod gcd_1 gcd gcdext_1 gcdext gcd_lehmer gcd_subdiv_step		   \
   gcdext_lehmer gcdext_subdiv_step					   \
-  tdiv_qr jacbase get_d							   \
-  matrix22_mul hgcd2 hgcd mullo_n mullo_basecase			   \
-  toom22_mul toom32_mul toom42_mul toom52_mul toom62_mul		   \
-  toom33_mul toom43_mul toom53_mul					   \
-  toom44_mul								   \
+  tdiv_qr dc_divrem_n sb_divrem_mn jacbase get_d			   \
+  matrix22_mul hgcd2 hgcd mullow_n mullow_basecase			   \
+  toom22_mul toom33_mul toom44_mul					   \
+  toom32_mul toom42_mul toom62_mul toom53_mul				   \
   toom2_sqr toom3_sqr toom4_sqr						   \
-  toom_eval_dgr3_pm1 toom_eval_dgr3_pm2 				   \
-  toom_eval_pm1 toom_eval_pm2 toom_eval_pm2exp	   			   \
-  toom_interpolate_5pts toom_interpolate_6pts toom_interpolate_7pts	   \
-  invertappr invert binvert mulmod_bnm1					   \
-  sbpi1_div_q sbpi1_div_qr sbpi1_divappr_q				   \
-  dcpi1_div_q dcpi1_div_qr dcpi1_divappr_q				   \
+  toom_interpolate_5pts toom_interpolate_7pts invert binvert		   \
+  sb_div_qr sb_divappr_q sb_div_q dc_div_qr dc_divappr_q dc_div_q	   \
   mu_div_qr mu_divappr_q mu_div_q					   \
-  bdiv_q_1								   \
-  sbpi1_bdiv_q sbpi1_bdiv_qr						   \
-  dcpi1_bdiv_q dcpi1_bdiv_qr						   \
-  mu_bdiv_q mu_bdiv_qr							   \
-  bdiv_q bdiv_qr							   \
-  divexact bdiv_dbm1c redc_1 redc_2 redc_n powm powlo powm_sec subcnd_n	   \
-  redc_1_sec trialdiv remove						   \
-  and_n andn_n nand_n ior_n iorn_n nior_n xor_n xnor_n			   \
-  copyi copyd zero							   \
+  sb_bdiv_q sb_bdiv_qr dc_bdiv_q dc_bdiv_qr mu_bdiv_q mu_bdiv_qr	   \
+  divexact bdiv_dbm1c redc_1 redc_2 powm powlo powm_sec subcnd_n	   \
   $gmp_mpn_functions_optional"
 
 define(GMP_MULFUNC_CHOICES,
@@ -2520,24 +2456,8 @@
   and_n|andn_n|nand_n | ior_n|iorn_n|nior_n | xor_n|xnor_n)
                      tmp_mulfunc="logops_n"  ;;
   lshift|rshift)     tmp_mulfunc="lorrshift";;
-  addlsh1_n)
-		     tmp_mulfunc="aorslsh1_n aorrlsh1_n";;
-  sublsh1_n)
-		     tmp_mulfunc="aorslsh1_n sorrlsh1_n";;
-  rsblsh1_n)
-		     tmp_mulfunc="aorrlsh1_n sorrlsh1_n";;
-  addlsh2_n)
-		     tmp_mulfunc="aorslsh2_n aorrlsh2_n";;
-  sublsh2_n)
-		     tmp_mulfunc="aorslsh2_n sorrlsh2_n";;
-  rsblsh2_n)
-		     tmp_mulfunc="aorrlsh2_n sorrlsh2_n";;
-  addlsh_n)
-		     tmp_mulfunc="aorslsh_n aorrlsh_n";;
-  sublsh_n)
-		     tmp_mulfunc="aorslsh_n sorrlsh_n";;
-  rsblsh_n)
-		     tmp_mulfunc="aorrlsh_n sorrlsh_n";;
+  addlsh1_n|sublsh1_n)
+		     tmp_mulfunc="aorslsh1_n";;
   rsh1add_n|rsh1sub_n)
 		     tmp_mulfunc="rsh1aors_n";;
 esac
@@ -2594,10 +2514,10 @@
 # setup that structure, on a per-directory basis ready for
 # mpn/<cpu>/fat/fat.c.
 #
-# fat.h includes thresholds listed in $fat_thresholds, extracted from
+# fat.h includes thesholds listed in $fat_thresholds, extracted from
 # gmp-mparam.h in each directory.  An overall maximum for each threshold is
 # established, for use in making fixed size arrays of temporary space.
-# (Eg. MUL_TOOM33_THRESHOLD_LIMIT used by mpn/generic/mul.c.)
+# (Eg. MUL_TOOM3_THRESHOLD_LIMIT used by mpn/generic/mul.c.)
 #
 # It'd be possible to do some of this manually, but when there's more than a
 # few functions and a few directories it becomes very tedious, and very
@@ -2645,7 +2565,7 @@
   echo "
 /* Copy all fields into __gmpn_cpuvec.
    memcpy is not used because it might operate byte-wise (depending on its
-   implementation), and we need the function pointer writes to be atomic.
+   implemenation), and we need the function pointer writes to be atomic.
    "volatile" discourages the compiler from trying to optimize this.  */
 #define CPUVEC_INSTALL(vec) \\
   do { \\
@@ -2707,7 +2627,7 @@
     esac
 
     # Extract desired thresholds from gmp-mparam.h file in this directory,
-    # if present.
+    # if prsent.
     tmp_mparam=$srcdir/mpn/$tmp_dir/gmp-mparam.h
     if test -f $tmp_mparam; then
       for tmp_tn in $fat_thresholds; do
@@ -2960,12 +2880,8 @@
 [/* Define to 1 each of the following for which a native (ie. CPU specific)
     implementation of the corresponding routine exists.  */
 #undef HAVE_NATIVE_mpn_add_n
-#undef HAVE_NATIVE_mpn_add_n_sub_n
 #undef HAVE_NATIVE_mpn_add_nc
-#undef HAVE_NATIVE_mpn_addaddmul_1msb0
 #undef HAVE_NATIVE_mpn_addlsh1_n
-#undef HAVE_NATIVE_mpn_addlsh2_n
-#undef HAVE_NATIVE_mpn_addlsh_n
 #undef HAVE_NATIVE_mpn_addmul_1c
 #undef HAVE_NATIVE_mpn_addmul_2
 #undef HAVE_NATIVE_mpn_addmul_3
@@ -2974,11 +2890,11 @@
 #undef HAVE_NATIVE_mpn_addmul_6
 #undef HAVE_NATIVE_mpn_addmul_7
 #undef HAVE_NATIVE_mpn_addmul_8
+#undef HAVE_NATIVE_mpn_addsub_n
+#undef HAVE_NATIVE_mpn_addaddmul_1msb0
 #undef HAVE_NATIVE_mpn_and_n
 #undef HAVE_NATIVE_mpn_andn_n
 #undef HAVE_NATIVE_mpn_bdiv_dbm1c
-#undef HAVE_NATIVE_mpn_bdiv_q_1
-#undef HAVE_NATIVE_mpn_bdiv_q_1_pi1
 #undef HAVE_NATIVE_mpn_com_n
 #undef HAVE_NATIVE_mpn_copyd
 #undef HAVE_NATIVE_mpn_copyi
@@ -2988,52 +2904,36 @@
 #undef HAVE_NATIVE_mpn_divrem_1c
 #undef HAVE_NATIVE_mpn_divrem_2
 #undef HAVE_NATIVE_mpn_gcd_1
-#undef HAVE_NATIVE_mpn_hamdist
 #undef HAVE_NATIVE_mpn_invert_limb
 #undef HAVE_NATIVE_mpn_ior_n
 #undef HAVE_NATIVE_mpn_iorn_n
-#undef HAVE_NATIVE_mpn_lshift
 #undef HAVE_NATIVE_mpn_lshiftc
-#undef HAVE_NATIVE_mpn_lshsub_n
 #undef HAVE_NATIVE_mpn_mod_1
-#undef HAVE_NATIVE_mpn_mod_1_1p
 #undef HAVE_NATIVE_mpn_mod_1c
-#undef HAVE_NATIVE_mpn_mod_1s_2p
-#undef HAVE_NATIVE_mpn_mod_1s_4p
-#undef HAVE_NATIVE_mpn_mod_34lsub1
 #undef HAVE_NATIVE_mpn_modexact_1_odd
 #undef HAVE_NATIVE_mpn_modexact_1c_odd
-#undef HAVE_NATIVE_mpn_mul_1
 #undef HAVE_NATIVE_mpn_mul_1c
 #undef HAVE_NATIVE_mpn_mul_2
 #undef HAVE_NATIVE_mpn_mul_3
 #undef HAVE_NATIVE_mpn_mul_4
-#undef HAVE_NATIVE_mpn_mul_basecase
 #undef HAVE_NATIVE_mpn_nand_n
 #undef HAVE_NATIVE_mpn_nior_n
-#undef HAVE_NATIVE_mpn_popcount
 #undef HAVE_NATIVE_mpn_preinv_divrem_1
 #undef HAVE_NATIVE_mpn_preinv_mod_1
 #undef HAVE_NATIVE_mpn_redc_1
 #undef HAVE_NATIVE_mpn_redc_2
-#undef HAVE_NATIVE_mpn_rsblsh1_n
-#undef HAVE_NATIVE_mpn_rsblsh2_n
-#undef HAVE_NATIVE_mpn_rsblsh_n
 #undef HAVE_NATIVE_mpn_rsh1add_n
 #undef HAVE_NATIVE_mpn_rsh1sub_n
-#undef HAVE_NATIVE_mpn_rshift
 #undef HAVE_NATIVE_mpn_sqr_basecase
 #undef HAVE_NATIVE_mpn_sqr_diagonal
 #undef HAVE_NATIVE_mpn_sub_n
 #undef HAVE_NATIVE_mpn_sub_nc
 #undef HAVE_NATIVE_mpn_sublsh1_n
-#undef HAVE_NATIVE_mpn_sublsh2_n
-#undef HAVE_NATIVE_mpn_sublsh_n
 #undef HAVE_NATIVE_mpn_submul_1c
-#undef HAVE_NATIVE_mpn_udiv_qrnnd
-#undef HAVE_NATIVE_mpn_udiv_qrnnd_r
 #undef HAVE_NATIVE_mpn_umul_ppmm
 #undef HAVE_NATIVE_mpn_umul_ppmm_r
+#undef HAVE_NATIVE_mpn_udiv_qrnnd
+#undef HAVE_NATIVE_mpn_udiv_qrnnd_r
 #undef HAVE_NATIVE_mpn_xor_n
 #undef HAVE_NATIVE_mpn_xnor_n])
 
@@ -3117,7 +3017,7 @@
           ;;
       esac
       ;;
-    X86_PATTERN | X86_64_PATTERN)
+    X86_PATTERN | athlon64-*-* | atom-*-* | core2-*-* | x86_64-*-*)
       GMP_ASM_ALIGN_FILL_0x90
       case $ABI in
         32)
@@ -3136,7 +3036,6 @@
           ;;
         64)
           GMP_INCLUDE_MPN(x86_64/x86_64-defs.m4)
-          AC_DEFINE(HAVE_HOST_CPU_FAMILY_x86_64)
 	  case $host in
 	    *-*-darwin*)
 	      GMP_INCLUDE_MPN(x86_64/darwin.m4) ;;
@@ -3181,26 +3080,25 @@
 [The gmp-mparam.h file (a string) the tune program should suggest updating.])
 
 
-# Copy any SQR_TOOM2_THRESHOLD from gmp-mparam.h to config.m4.
+# Copy any SQR_KARATSUBA_THRESHOLD from gmp-mparam.h to config.m4.
 # Some versions of sqr_basecase.asm use this.
 # Fat binaries do this on a per-file basis, so skip in that case.
 #
 if test -z "$fat_path"; then
-  tmp_gmp_karatsuba_sqr_threshold=`sed -n 's/^#define SQR_TOOM2_THRESHOLD[ 	]*\([0-9][0-9]*\).*$/\1/p' $gmp_mparam_source`
+  tmp_gmp_karatsuba_sqr_threshold=`sed -n 's/^#define SQR_KARATSUBA_THRESHOLD[ 	]*\([0-9][0-9]*\).*$/\1/p' $gmp_mparam_source`
   if test -n "$tmp_gmp_karatsuba_sqr_threshold"; then
-    GMP_DEFINE_RAW(["define(<SQR_TOOM2_THRESHOLD>,<$tmp_gmp_karatsuba_sqr_threshold>)"])
+    GMP_DEFINE_RAW(["define(<SQR_KARATSUBA_THRESHOLD>,<$tmp_gmp_karatsuba_sqr_threshold>)"])
   fi
 fi
 
 
 # Sizes of some types, needed at preprocessing time.
 #
-# FIXME: The assumption that GMP_LIMB_BITS is 8*sizeof(mp_limb_t) might
-# be slightly rash, but it's true everywhere we know of and ought to be true
+# FIXME: The assumption that BITS_PER_MP_LIMB is 8*sizeof(mp_limb_t) might
+# be slightly rash, but it's true everwhere we know of and ought to be true
 # of any sensible system.  In a generic C build, grepping LONG_BIT out of
 # <limits.h> might be an alternative, for maximum portability.
 #
-AC_CHECK_SIZEOF(void *)
 AC_CHECK_SIZEOF(unsigned short)
 AC_CHECK_SIZEOF(unsigned)
 AC_CHECK_SIZEOF(unsigned long)
@@ -3208,7 +3106,7 @@
 if test "$ac_cv_sizeof_mp_limb_t" = 0; then
   AC_MSG_ERROR([Oops, mp_limb_t doesn't seem to work])
 fi
-AC_SUBST(GMP_LIMB_BITS, `expr 8 \* $ac_cv_sizeof_mp_limb_t`)
+AC_SUBST(BITS_PER_MP_LIMB, `expr 8 \* $ac_cv_sizeof_mp_limb_t`)
 GMP_DEFINE_RAW(["define(<SIZEOF_UNSIGNED>,<$ac_cv_sizeof_unsigned>)"])
 
 # Check compiler limb size matches gmp-mparam.h
@@ -3218,20 +3116,20 @@
 # probably wouldn't want to be fatal, none of the libgmp assembler code
 # depends on ulong.
 #
-mparam_bits=[`sed -n 's/^#define GMP_LIMB_BITS[ 	][ 	]*\([0-9]*\).*$/\1/p' $gmp_mparam_source`]
-if test -n "$mparam_bits" && test "$mparam_bits" -ne $GMP_LIMB_BITS; then
+mparam_bits=[`sed -n 's/^#define BITS_PER_MP_LIMB[ 	][ 	]*\([0-9]*\).*$/\1/p' $gmp_mparam_source`]
+if test -n "$mparam_bits" && test "$mparam_bits" -ne $BITS_PER_MP_LIMB; then
   if test "$test_CFLAGS" = set; then
-    AC_MSG_ERROR([Oops, mp_limb_t is $GMP_LIMB_BITS bits, but the assembler code
+    AC_MSG_ERROR([Oops, mp_limb_t is $BITS_PER_MP_LIMB bits, but the assembler code
 in this configuration expects $mparam_bits bits.
 You appear to have set \$CFLAGS, perhaps you also need to tell GMP the
 intended ABI, see "ABI and ISA" in the manual.])
   else
-    AC_MSG_ERROR([Oops, mp_limb_t is $GMP_LIMB_BITS bits, but the assembler code
+    AC_MSG_ERROR([Oops, mp_limb_t is $BITS_PER_MP_LIMB bits, but the assembler code
 in this configuration expects $mparam_bits bits.])
   fi
 fi
 
-GMP_DEFINE_RAW(["define(<GMP_LIMB_BITS>,$GMP_LIMB_BITS)"])
+GMP_DEFINE_RAW(["define(<GMP_LIMB_BITS>,$BITS_PER_MP_LIMB)"])
 GMP_DEFINE_RAW(["define(<GMP_NAIL_BITS>,$GMP_NAIL_BITS)"])
 GMP_DEFINE_RAW(["define(<GMP_NUMB_BITS>,eval(GMP_LIMB_BITS-GMP_NAIL_BITS))"])
 
@@ -3260,11 +3158,11 @@
 test -d tune || mkdir tune
 case $sqr_basecase_source in
   *.asm)
-    sqr_max=[`sed -n 's/^def...(SQR_TOOM2_THRESHOLD_MAX, *\([0-9]*\))/\1/p' $sqr_basecase_source`]
+    sqr_max=[`sed -n 's/^def...(SQR_KARATSUBA_THRESHOLD_MAX, *\([0-9]*\))/\1/p' $sqr_basecase_source`]
     if test -n "$sqr_max"; then
       TUNE_SQR_OBJ=sqr_asm.o
-      AC_DEFINE_UNQUOTED(TUNE_SQR_TOOM2_MAX,$sqr_max,
-      [Maximum size the tune program can test for SQR_TOOM2_THRESHOLD])
+      AC_DEFINE_UNQUOTED(TUNE_SQR_KARATSUBA_MAX,$sqr_max,
+      [Maximum size the tune program can test for SQR_KARATSUBA_THRESHOLD])
     fi
     cat >tune/sqr_basecase.c <<EOF
 /* not sure that an empty file can compile, so put in a dummy */
@@ -3273,7 +3171,7 @@
     ;;
   *.c)
     TUNE_SQR_OBJ=
-    AC_DEFINE(TUNE_SQR_TOOM2_MAX,SQR_TOOM2_MAX_GENERIC)
+    AC_DEFINE(TUNE_SQR_KARATSUBA_MAX,SQR_KARATSUBA_MAX_GENERIC)
     cat >tune/sqr_basecase.c <<EOF
 #define TUNE_PROGRAM_BUILD 1
 #define TUNE_PROGRAM_BUILD_SQR 1
--- 1/demos/expr/README
+++ 2/demos/expr/README
@@ -474,7 +474,7 @@
 There would want to be a standard table without assignments available
 though, so user input could be safely parsed.
 
-The closing parenthesis table entry could specify the type of open paren it
+The closing parethesis table entry could specify the type of open paren it
 expects, so that "(" and ")" could match and "[" and "]" match but not a
 mixture of the two.  Currently "[" and "]" can be added, but there's no
 error on writing a mixed expression like "2*(3+4]".  Maybe also there could
--- 1/demos/factorize.c
+++ 2/demos/factorize.c
@@ -1,7 +1,7 @@
 /* Factoring with Pollard's rho method.
 
-Copyright 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2005, 2009
-Free Software Foundation, Inc.
+Copyright 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2005 Free Software
+Foundation, Inc.
 
 This program is free software; you can redistribute it and/or modify it under
 the terms of the GNU General Public License as published by the Free Software
@@ -35,7 +35,7 @@
   unsigned *addv = add;
   unsigned int failures;
 
-  if (flag_verbose > 0)
+  if (flag_verbose)
     {
       printf ("[trial division (%u)] ", limit);
       fflush (stdout);
@@ -98,7 +98,8 @@
 	}
     }
 
-  mpz_clears (q, r, NULL);
+  mpz_clear (q);
+  mpz_clear (r);
 }
 
 void
@@ -108,7 +109,7 @@
   mpz_t f;
   unsigned int k;
 
-  if (flag_verbose > 0)
+  if (flag_verbose)
     {
       printf ("[trial division (%u)] ", limit);
       fflush (stdout);
@@ -131,133 +132,129 @@
       mpz_add_ui (f, f, 2 * p);
     }
 
-  mpz_clears (f, r, NULL);
+  mpz_clear (f);
+  mpz_clear (r);
 }
 
 void
-factor_using_pollard_rho (mpz_t n, unsigned long a, unsigned long p)
+factor_using_pollard_rho (mpz_t n, int a_int, unsigned long p)
 {
   mpz_t x, x1, y, P;
+  mpz_t a;
+  mpz_t g;
   mpz_t t1, t2;
-  unsigned long long k, l, i;
+  int k, l, c, i;
 
-  if (flag_verbose > 0)
+  if (flag_verbose)
     {
-      printf ("[pollard-rho (%lu)] ", a);
+      printf ("[pollard-rho (%d)] ", a_int);
       fflush (stdout);
     }
 
-  mpz_inits (t1, t2, NULL);
+  mpz_init (g);
+  mpz_init (t1);
+  mpz_init (t2);
+
+  mpz_init_set_si (a, a_int);
   mpz_init_set_si (y, 2);
   mpz_init_set_si (x, 2);
   mpz_init_set_si (x1, 2);
-  mpz_init_set_ui (P, 1);
   k = 1;
   l = 1;
+  mpz_init_set_ui (P, 1);
+  c = 0;
 
   while (mpz_cmp_ui (n, 1) != 0)
     {
-      for (;;)
+S2:
+      if (p != 0)
 	{
-	  do
+	  mpz_powm_ui (x, x, p, n); mpz_add (x, x, a);
+	}
+      else
+	{
+	  mpz_mul (x, x, x); mpz_add (x, x, a); mpz_mod (x, x, n);
+	}
+      mpz_sub (t1, x1, x); mpz_mul (t2, P, t1); mpz_mod (P, t2, n);
+      c++;
+      if (c == 20)
+	{
+	  c = 0;
+	  mpz_gcd (g, P, n);
+	  if (mpz_cmp_ui (g, 1) != 0)
+	    goto S4;
+	  mpz_set (y, x);
+	}
+S3:
+      k--;
+      if (k > 0)
+	goto S2;
+
+      mpz_gcd (g, P, n);
+      if (mpz_cmp_ui (g, 1) != 0)
+	goto S4;
+
+      mpz_set (x1, x);
+      k = l;
+      l = 2 * l;
+      for (i = 0; i < k; i++)
+	{
+	  if (p != 0)
 	    {
-	      if (p != 0)
-		{
-		  mpz_powm_ui (x, x, p, n);
-		  mpz_add_ui (x, x, a);
-		}
-	      else
-		{
-		  mpz_mul (t1, x, x);
-		  mpz_mod (x, t1, n);
-		  mpz_add_ui (x, x, a);
-		}
-
-	      mpz_sub (t1, x1, x);
-	      mpz_mul (t2, P, t1);
-	      mpz_mod (P, t2, n);
-
-	      if (k % 32 == 1)
-		{
-		  mpz_gcd (t1, P, n);
-		  if (mpz_cmp_ui (t1, 1) != 0)
-		    goto factor_found;
-		  mpz_set (y, x);
-		}
+	      mpz_powm_ui (x, x, p, n); mpz_add (x, x, a);
 	    }
-	  while (--k != 0);
-
-	  mpz_gcd (t1, P, n);
-	  if (mpz_cmp_ui (t1, 1) != 0)
-	    goto factor_found;
-
-	  mpz_set (x1, x);
-	  k = l;
-	  l = 2 * l;
-	  for (i = 0; i < k; i++)
+	  else
 	    {
-	      if (p != 0)
-		{
-		  mpz_powm_ui (x, x, p, n);
-		  mpz_add_ui (x, x, a);
-		}
-	      else
-		{
-		  mpz_mul (t1, x, x);
-		  mpz_mod (x, t1, n);
-		  mpz_add_ui (x, x, a);
-		}
+	      mpz_mul (x, x, x); mpz_add (x, x, a); mpz_mod (x, x, n);
 	    }
-	  mpz_set (y, x);
 	}
-
-    factor_found:
+      mpz_set (y, x);
+      c = 0;
+      goto S2;
+S4:
       do
 	{
 	  if (p != 0)
 	    {
-	      mpz_powm_ui (y, y, p, n); mpz_add_ui (y, y, a);
+	      mpz_powm_ui (y, y, p, n); mpz_add (y, y, a);
 	    }
 	  else
 	    {
-	      mpz_mul (t1, y, y);
-	      mpz_mod (y, t1, n);
-	      mpz_add_ui (y, y, a);
+	      mpz_mul (y, y, y); mpz_add (y, y, a); mpz_mod (y, y, n);
 	    }
-	  mpz_sub (t1, x1, y);
-	  mpz_gcd (t1, t1, n);
+	  mpz_sub (t1, x1, y); mpz_gcd (g, t1, n);
 	}
-      while (mpz_cmp_ui (t1, 1) == 0);
+      while (mpz_cmp_ui (g, 1) == 0);
 
-      mpz_divexact (n, n, t1);	/* divide by t1, before t1 is overwritten */
+      mpz_div (n, n, g);	/* divide by g, before g is overwritten */
 
-      if (!mpz_probab_prime_p (t1, 10))
+      if (!mpz_probab_prime_p (g, 3))
 	{
 	  do
 	    {
 	      mp_limb_t a_limb;
 	      mpn_random (&a_limb, (mp_size_t) 1);
-	      a = a_limb;
+	      a_int = (int) a_limb;
 	    }
-	  while (a == 0);
+	  while (a_int == -2 || a_int == 0);
 
-	  if (flag_verbose > 0)
+	  if (flag_verbose)
 	    {
 	      printf ("[composite factor--restarting pollard-rho] ");
 	      fflush (stdout);
 	    }
-	  factor_using_pollard_rho (t1, a, p);
+	  factor_using_pollard_rho (g, a_int, p);
 	}
       else
 	{
-	  mpz_out_str (stdout, 10, t1);
+	  mpz_out_str (stdout, 10, g);
 	  fflush (stdout);
 	  fputc (' ', stdout);
 	}
       mpz_mod (x, x, n);
       mpz_mod (x1, x1, n);
       mpz_mod (y, y, n);
-      if (mpz_probab_prime_p (n, 10))
+      if (mpz_probab_prime_p (n, 3))
 	{
 	  mpz_out_str (stdout, 10, n);
 	  fflush (stdout);
@@ -266,7 +263,14 @@
 	}
     }
 
-  mpz_clears (P, t2, t1, x1, x, y, NULL);
+  mpz_clear (g);
+  mpz_clear (P);
+  mpz_clear (t2);
+  mpz_clear (t1);
+  mpz_clear (a);
+  mpz_clear (x1);
+  mpz_clear (x);
+  mpz_clear (y);
 }
 
 void
@@ -291,15 +295,15 @@
 
   if (mpz_cmp_ui (t, 1) != 0)
     {
-      if (flag_verbose > 0)
+      if (flag_verbose)
 	{
 	  printf ("[is number prime?] ");
 	  fflush (stdout);
 	}
-      if (mpz_probab_prime_p (t, 10))
+      if (mpz_probab_prime_p (t, 3))
 	mpz_out_str (stdout, 10, t);
       else
-	factor_using_pollard_rho (t, 1L, p);
+	factor_using_pollard_rho (t, 1, p);
     }
 }
 
@@ -316,12 +320,6 @@
       argv++;
       argc--;
     }
-  if (argc > 1 && !strcmp (argv[1], "-q"))
-    {
-      flag_verbose = -1;
-      argv++;
-      argc--;
-    }
 
   mpz_init (t);
   if (argc > 1)
@@ -362,10 +360,7 @@
 	  mpz_inp_str (t, stdin, 0);
 	  if (feof (stdin))
 	    break;
-	  if (flag_verbose >= 0)
-	    {
-	      mpz_out_str (stdout, 10, t); printf (" = ");
-	    }
+	  mpz_out_str (stdout, 10, t); printf (" = ");
 	  factor (t, 0);
 	  puts ("");
 	}
--- 1/demos/perl/GMP.pm
+++ 2/demos/perl/GMP.pm
@@ -209,7 +209,7 @@
 string of byte data, which will be a multiple of $size bytes.
 
 C<invert> returns the inverse, or undef if it doesn't exist.  C<remove>
-returns a remainder/multiplicity pair.  C<root> returns the nth root, and
+returns a remainder/multiplicty pair.  C<root> returns the nth root, and
 C<roote> returns a root/bool pair, the bool indicating whether the root is
 exact.  C<sqrtrem> and C<rootrem> return a root/remainder pair.
 
@@ -477,7 +477,7 @@
 Arguments to operators and functions are converted as necessary to the
 appropriate type.  For instance C<**> requires an unsigned integer exponent,
 and an mpq argument will be converted, so long as it's an integer in the
-appropriate range.
+apropriate range.
 
     use GMP::Mpz (mpz);
     use GMP::Mpq (mpq);
--- 1/demos/perl/typemap
+++ 2/demos/perl/typemap
@@ -73,7 +73,7 @@
 ORDER_NOSWAP
 	assert ($arg != &PL_sv_yes);
 DUMMY
-	/* dummy $var */
+	/* dummy $var */	
 CONST_STRING_ASSUME
         /* No need to check for SvPOKp and use SvPV, this mapping is
            only used for overload_constant, which always gets literal
--- 1/demos/primes.c
+++ 2/demos/primes.c
@@ -283,7 +283,7 @@
 	    {
 	      start = (prime - mpz_tdiv_ui (fr, prime)) % prime;
 	      if (start % 2 != 0)
-		start += prime;		/* adjust if even divisible */
+		start += prime;		/* adjust if even divisable */
 	    }
 	  start2 = start / 2;
 	}
--- 1/doc/configuration
+++ 2/doc/configuration
@@ -160,6 +160,18 @@
 in EXTRA_PROGRAMS to get Makefile rules created, but they're never
 built or run unless an explicit "make someprog" is used.
 
+** Macos directory
+
+The macos/configure script will automatically pick up additions to
+gmp_mpn_functions, MPZ_OBJECTS, etc, but currently test programs need
+to be added to Makefile.in manually.
+
+When modifying the top-level configure.in ensure that it doesn't upset
+the macos/configure script heuristics.
+
+Unfortunately there's no way to test this stuff without getting an
+actual MacOS 9 system.
+
 
 * Adding a new CPU
 
--- 1/doc/gmp.texi
+++ 2/doc/gmp.texi
@@ -92,7 +92,7 @@
 @subtitle @value{UPDATED}
 
 @author by the GMP developers
-@c @email{tg@@gmplib.org}
+@c @email{tege@@gmplib.org}
 
 @c Include the Distribution inside the titlepage so
 @c that headings are turned off.
@@ -1528,6 +1528,12 @@
 precision when the hardware is in single precision mode.  Of course this
 affects all code, including application code, not just GMP.
 
+@item MacOS 9
+@cindex MacOS 9
+The @file{macos} directory contains an unsupported port to MacOS 9 on Power
+Macintosh, see @file{macos/README}.  Note that MacOS X ``Darwin'' should use
+the normal Unix-style @samp{./configure}.
+
 @item MS-DOS and MS Windows
 @cindex MS-DOS
 @cindex MS Windows
@@ -2992,11 +2998,6 @@
 Initialize @var{x}, and set its value to 0.
 @end deftypefun
 
-@deftypefun void mpz_inits (mpz_t @var{x}, ...)
-Initialize a NULL-terminated list of @code{mpz_t} variables, and set their
-values to 0.
-@end deftypefun
-
 @deftypefun void mpz_init2 (mpz_t @var{x}, unsigned long @var{n})
 Initialize @var{x}, with space for @var{n}-bit numbers, and set its value to 0.
 Calling this function instead of @code{mpz_init} or @code{mpz_inits} is never
@@ -3013,10 +3014,6 @@
 variables when you are done with them.
 @end deftypefun
 
-@deftypefun void mpz_clears (mpz_t @var{x}, ...)
-Free the space occupied by a NULL-terminated list of @code{mpz_t} variables.
-@end deftypefun
-
 @deftypefun void mpz_realloc2 (mpz_t @var{x}, unsigned long @var{n})
 Change the space allocated for @var{x} to @var{n} bits.  The value in @var{x}
 is preserved if it fits, or is set to 0 if not.
@@ -4184,20 +4181,11 @@
 between each initialization.
 @end deftypefun
 
-@deftypefun void mpq_inits (mpq_t @var{x}, ...)
-Initialize a NULL-terminated list of @code{mpq_t} variables, and set their
-values to 0/1.
-@end deftypefun
-
 @deftypefun void mpq_clear (mpq_t @var{x})
 Free the space occupied by @var{x}.  Make sure to call this function for all
 @code{mpq_t} variables when you are done with them.
 @end deftypefun
 
-@deftypefun void mpq_clears (mpq_t @var{x}, ...)
-Free the space occupied by a NULL-terminated list of @code{mpq_t} variables.
-@end deftypefun
-
 @deftypefun void mpq_set (mpq_t @var{rop}, mpq_t @var{op})
 @deftypefunx void mpq_set_z (mpq_t @var{rop}, mpz_t @var{op})
 Assign @var{rop} from @var{op}.
@@ -4551,22 +4539,11 @@
 least be cleared, using @code{mpf_clear}, between initializations.
 @end deftypefun
 
-@deftypefun void mpf_inits (mpf_t @var{x}, ...)
-Initialize a NULL-terminated list of @code{mpf_t} variables, and set their
-values to 0.  The precision of the initialized variables is undefined unless a
-default precision has already been established by a call to
-@code{mpf_set_default_prec}.
-@end deftypefun
-
 @deftypefun void mpf_clear (mpf_t @var{x})
 Free the space occupied by @var{x}.  Make sure to call this function for all
 @code{mpf_t} variables when you are done with them.
 @end deftypefun
 
-@deftypefun void mpf_clears (mpf_t @var{x}, ...)
-Free the space occupied by a NULL-terminated list of @code{mpf_t} variables.
-@end deftypefun
-
 @need 2000
 Here is an example on how to initialize floating-point variables:
 @example
@@ -5357,40 +5334,47 @@
 negative value if @math{@var{s1} < @var{s2}}.
 @end deftypefun
 
-@deftypefun mp_size_t mpn_gcd (mp_limb_t *@var{rp}, mp_limb_t *@var{s1p}, mp_size_t @var{s1n}, mp_limb_t *@var{s2p}, mp_size_t @var{s2n})
-Set @{@var{rp}, @var{retval}@} to the greatest common divisor of @{@var{s1p},
-@var{s1n}@} and @{@var{s2p}, @var{s2n}@}.  The result can be up to @var{s2n}
-limbs, the return value is the actual number produced.  Both source operands
-are destroyed.
-
-@{@var{s1p}, @var{s1n}@} must have at least as many bits as @{@var{s2p},
-@var{s2n}@}.  @{@var{s2p}, @var{s2n}@} must be odd.  Both operands must have
-non-zero most significant limbs.  No overlap is permitted between @{@var{s1p},
-@var{s1n}@} and @{@var{s2p}, @var{s2n}@}.
-@end deftypefun
-
-@deftypefun mp_limb_t mpn_gcd_1 (const mp_limb_t *@var{s1p}, mp_size_t @var{s1n}, mp_limb_t @var{s2limb})
-Return the greatest common divisor of @{@var{s1p}, @var{s1n}@} and
-@var{s2limb}.  Both operands must be non-zero.
-@end deftypefun
-
-@deftypefun mp_size_t mpn_gcdext (mp_limb_t *@var{r1p}, mp_limb_t *@var{r2p}, mp_size_t *@var{r2n}, mp_limb_t *@var{s1p}, mp_size_t @var{s1n}, mp_limb_t *@var{s2p}, mp_size_t @var{s2n})
-Calculate the greatest common divisor of @{@var{s1p}, @var{s1n}@} and
-@{@var{s2p}, @var{s2n}@}.  Store the gcd at @{@var{r1p}, @var{retval}@} and
-the first cofactor at @{@var{r2p}, *@var{r2n}@}, with *@var{r2n} negative if
-the cofactor is negative.  @var{r1p} and @var{r2p} should each have room for
-@math{@var{s1n}+1} limbs, but the return value and value stored through
-@var{r2n} indicate the actual number produced.
-
-@math{@{@var{s1p}, @var{s1n}@} @ge{} @{@var{s2p}, @var{s2n}@}} is required,
-and both must be non-zero.  The regions @{@var{s1p}, @math{@var{s1n}+1}@} and
-@{@var{s2p}, @math{@var{s2n}+1}@} are destroyed (i.e.@: the operands plus an
-extra limb past the end of each).
-
-The cofactor @var{r2} will satisfy @m{r_2 s_1 + k s_2 = r_1, @var{r2}*@var{s1}
-+ @var{k}*@var{s2} = @var{r1}}.  The second cofactor @var{k} is not calculated
-but can easily be obtained from @m{(r_1 - r_2 s_1) / s_2, (@var{r1} -
-@var{r2}*@var{s1}) / @var{s2}} (this division will be exact).
+@deftypefun mp_size_t mpn_gcd (mp_limb_t *@var{rp}, mp_limb_t *@var{xp}, mp_size_t @var{xn}, mp_limb_t *@var{yp}, mp_size_t @var{yn})
+Set @{@var{rp}, @var{retval}@} to the greatest common divisor of @{@var{xp},
+@var{xn}@} and @{@var{yp}, @var{yn}@}.  The result can be up to @var{yn} limbs,
+the return value is the actual number produced.  Both source operands are
+destroyed.
+
+@{@var{xp}, @var{xn}@} must have at least as many bits as @{@var{yp},
+@var{yn}@}.  @{@var{yp}, @var{yn}@} must be odd.  Both operands must have
+non-zero most significant limbs.  No overlap is permitted between @{@var{xp},
+@var{xn}@} and @{@var{yp}, @var{yn}@}.
+@end deftypefun
+
+@deftypefun mp_limb_t mpn_gcd_1 (const mp_limb_t *@var{xp}, mp_size_t @var{xn}, mp_limb_t @var{ylimb})
+Return the greatest common divisor of @{@var{xp}, @var{xn}@} and @var{ylimb}.
+Both operands must be non-zero.
+@end deftypefun
+
+@deftypefun mp_size_t mpn_gcdext (mp_limb_t *@var{gp}, mp_limb_t *@var{sp}, mp_size_t *@var{sn}, mp_limb_t *@var{xp}, mp_size_t @var{xn}, mp_limb_t *@var{yp}, mp_size_t @var{yn})
+Let @m{U,@var{U}} be defined by @{@var{xp}, @var{xn}@} and let @m{V,@var{V}} be
+defined by @{@var{yp}, @var{yn}@}.
+
+Compute the greatest common divisor @math{G} of @math{U} and @math{V}.  Compute
+a cofactor @math{S} such that @math{G = US + VT}.  The second cofactor @var{T}
+is not computed but can easily be obtained from @m{(G - US) / V, (@var{G} -
+@var{U}*@var{S}) / @var{V}} (the division will be exact).  It is required that
+@math{U @ge V > 0}.
+
+@math{S} satisfies @math{S = 1} or @math{@GMPabs{S} < V / (2 G)}. @math{S =
+0} if and only if @math{V} divides @math{U} (i.e., @math{G = V}).
+
+Store @math{G} at @var{gp} and let the return value define its limb count.
+Store @math{S} at @var{sp} and let |*@var{sn}| define its limb count.  @math{S}
+can be negative; when this happens *@var{sn} will be negative.  The areas at
+@var{gp} and @var{sp} should each have room for @math{@var{xn}+1} limbs.
+
+The areas @{@var{xp}, @math{@var{xn}+1}@} and @{@var{yp}, @math{@var{yn}+1}@}
+are destroyed (i.e.@: the input operands plus an extra limb past the end of
+each).
+
+Compatibility note: GMP 4.3.0 and 4.3.1 defined @math{S} less strictly.
+Earlier as well as later GMP releases define @math{S} as described here.
 @end deftypefun
 
 @deftypefun mp_size_t mpn_sqrtrem (mp_limb_t *@var{r1p}, mp_limb_t *@var{r2p}, const mp_limb_t *@var{sp}, mp_size_t @var{n})
@@ -5486,59 +5470,6 @@
 Return non-zero iff @{@var{s1p}, @var{n}@} is a perfect square.
 @end deftypefun
 
-@deftypefun void mpn_and_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical and of @{@var{s1p}, @var{n}@} and @{@var{s2p},
-@var{n}@}, and write the result to @{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_ior_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical inclusive or of @{@var{s1p}, @var{n}@} and
-@{@var{s2p}, @var{n}@}, and write the result to @{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_xor_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical exclusive or of @{@var{s1p}, @var{n}@} and
-@{@var{s2p}, @var{n}@}, and write the result to @{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_andn_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical and of @{@var{s1p}, @var{n}@} and the bitwise
-complement of @{@var{s2p}, @var{n}@}, and write the result to @{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_iorn_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical inclusive or of @{@var{s1p}, @var{n}@} and the bitwise
-complement of @{@var{s2p}, @var{n}@}, and write the result to @{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_nand_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical and of @{@var{s1p}, @var{n}@} and @{@var{s2p},
-@var{n}@}, and write the bitwise complement of the result to @{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_nior_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical inclusive or of @{@var{s1p}, @var{n}@} and
-@{@var{s2p}, @var{n}@}, and write the bitwise complement of the result to
-@{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_xnor_n (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, const mp_limb_t *@var{s2p}, mp_size_t @var{n})
-Perform the bitwise logical exclusive or of @{@var{s1p}, @var{n}@} and
-@{@var{s2p}, @var{n}@}, and write the bitwise complement of the result to
-@{@var{rp}, @var{n}@}.
-@end deftypefun
-
-@deftypefun void mpn_copyi (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, mp_size_t @var{n})
-Copy from @{@var{s1p}, @var{n}@} to @{@var{rp}, @var{n}@}, increasingly.
-@end deftypefun
-
-@deftypefun void mpn_copyd (mp_limb_t *@var{rp}, const mp_limb_t *@var{s1p}, mp_size_t @var{n})
-Copy from @{@var{s1p}, @var{n}@} to @{@var{rp}, @var{n}@}, decreasingly.
-@end deftypefun
-
-@deftypefun void mpn_zero (mp_limb_t *@var{rp}, mp_size_t @var{n})
-Zero @{@var{rp}, @var{n}@}.
-@end deftypefun
 
 @sp 1
 @section Nails
@@ -7394,7 +7325,7 @@
 @multitable {KaratsubaMMM} {MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM}
 @item Algorithm @tab Threshold
 @item Basecase  @tab (none)
-@item Karatsuba @tab @code{MUL_TOOM22_THRESHOLD}
+@item Karatsuba @tab @code{MUL_KARATSUBA_THRESHOLD}
 @item Toom-3    @tab @code{MUL_TOOM33_THRESHOLD}
 @item Toom-4    @tab @code{MUL_TOOM44_THRESHOLD}
 @item FFT       @tab @code{MUL_FFT_THRESHOLD}
@@ -7404,7 +7335,7 @@
 Similarly for squaring, with the @code{SQR} thresholds.
 
 N@cross{}M multiplications of operands with different sizes above
-@code{MUL_TOOM22_THRESHOLD} are currently done by special Toom-inspired
+@code{MUL_KARATSUBA_THRESHOLD} are currently done by special Toom-inspired
 algorithms or directly with FFT, depending on operand size (@pxref{Unbalanced
 Multiplication}).
 
@@ -7653,7 +7584,7 @@
 the exponent being @m{\log3/\log2,log(3)/log(2)}, representing 3 multiplies
 each @math{1/2} the size of the inputs.  This is a big improvement over the
 basecase multiply at @math{O(N^2)} and the advantage soon overcomes the extra
-additions Karatsuba performs.  @code{MUL_TOOM22_THRESHOLD} can be as little
+additions Karatsuba performs.  @code{MUL_KARATSUBA_THRESHOLD} can be as little
 as 10 limbs.  The @code{SQR} threshold is usually about twice the @code{MUL}.
 
 The basecase algorithm will take a time of the form @m{M(N) = aN^2 + bN + c,
@@ -8115,7 +8046,7 @@
 @cindex Unbalanced multiplication
 
 Multiplication of operands with different sizes, both below
-@code{MUL_TOOM22_THRESHOLD} are done with plain schoolbook multiplication
+@code{MUL_KARATSUBA_THRESHOLD} are done with plain schoolbook multiplication
 (@pxref{Basecase Multiplication}).
 
 For really large operands, we invoke FFT directly.
@@ -8239,10 +8170,10 @@
 then the work is about the same as a basecase division, but with more function
 call overheads and with some subtractions separated from the multiplies.
 These overheads mean that it's only when N/2 is above
-@code{MUL_TOOM22_THRESHOLD} that divide and conquer is of use.
+@code{MUL_KARATSUBA_THRESHOLD} that divide and conquer is of use.
 
 @code{DIV_DC_THRESHOLD} is based on the divisor size N, so it will be somewhere
-above twice @code{MUL_TOOM22_THRESHOLD}, but how much above depends on the
+above twice @code{MUL_KARATSUBA_THRESHOLD}, but how much above depends on the
 CPU@.  An optimized @code{mpn_mul_basecase} can lower @code{DIV_DC_THRESHOLD} a
 little by offering a ready-made advantage over repeated @code{mpn_submul_1}
 calls.
@@ -10348,22 +10279,17 @@
 Niels M@"oller wrote the sub-quadratic GCD and extended GCD code, the
 quadratic Hensel division code, and (with Torbj@"orn) the new divide and
 conquer division code for GMP 4.3.  Niels also helped implement the new Toom
-multiply code for GMP 4.3.  He wrote the original version of mpn_mulmod_bnm1.
+multiply code for GMP 4.3.
 
 Alberto Zanoni and Marco Bodrato suggested the unbalanced multiply strategy,
 and found the optimal strategies for evaluation and interpolation in Toom
-multiplication.
-
-Marco Bodrato helped implement the new Toom multiply code for GMP 4.3 and 4.4.
-He is the main author of the current mpn_mulmod_bnm1.  Marco also wrote
-the functions mpn_invert and mpn_invertappr.
+multiplication.  Marco also helped implement the new Toom multiply code for
+GMP 4.3.
 
 David Harvey suggested the internal function @code{mpn_bdiv_dbm1}, implementing
 division relevant to Toom multiplication.  He also worked on fast assembly
 sequences, in particular on a fast AMD64 @code{mpn_mul_basecase}.
 
-Martin Boij wrote @code{mpn_perfect_power_p}.
-
 (This list is chronological, not ordered after significance.  If you have
 contributed to GMP but are not listed above, please tell
 @email{gmp-devel@@gmplib.org} about the omission!)
--- 1/doc/projects.html
+++ 2/doc/projects.html
@@ -37,7 +37,7 @@
 
 <hr>
 <!-- NB. timestamp updated automatically by emacs -->
-  This file current as of 15 Nov 2009.  An up-to-date version is available at
+  This file current as of 1 May 2009.  An up-to-date version is available at
   <a href="http://gmplib.org/projects.html">http://gmplib.org/projects.html</a>.
   Please send comments about this page to gmp-devel<font>@</font>gmplib.org.
 
@@ -132,17 +132,17 @@
 
   <p> Standard techniques for these routines are unrolling, software
       pipelining, and specialization for common operand values.  For machines
-      with poor integer multiplication, it is sometimes possible to remedy the
-      situation using floating-point operations or SIMD operations such as MMX
-      (x86) (x86), SSE (x86), VMX (PowerPC), VIS (Sparc).
+      with poor integer multiplication, it is often possible to improve the
+      performance using floating-point operations, or SIMD operations such as
+      MMX or Sun's VIS.
 
   <p> Using floating-point operations is interesting but somewhat tricky.
       Since IEEE double has 53 bit of mantissa, one has to split the operands
-      in small pieces, so that no intermediates are greater than 2^53.  For
-      32-bit computers, splitting one operand into 16-bit pieces works.  For
-      64-bit machines, one operand can be split into 21-bit pieces and the
-      other into 32-bit pieces.  (A 64-bit operand can be split into just three
-      21-bit pieces if one allows the split operands to be negative!)
+      in small pieces, so that no result is greater than 2^53.  For 32-bit
+      computers, splitting one operand into 16-bit pieces works.  For 64-bit
+      machines, one operand can be split into 21-bit pieces and the other into
+      32-bit pieces.  (A 64-bit operand can be split into just three 21-bit
+      pieces if one allows the split operands to be negative!)
 
 
 <li> <strong>Math functions for the mpf layer</strong>
@@ -152,16 +152,12 @@
       functions are desirable: acos, acosh, asin, asinh, atan, atanh, atan2,
       cos, cosh, exp, log, log10, pow, sin, sinh, tan, tanh.
 
-  <p> Note that the <a href="http://mpfr.org">mpfr</a> functions already
-  provide these functions, and that we usually recommend new programs to use
-  mpfr instead of mpf.
-
 
 <li> <strong>Faster sqrt</strong>
 
   <p> The current code uses divisions, which are reasonably fast, but it'd be
       possible to use only multiplications by computing 1/sqrt(A) using this
-      iteration:
+      formula:
       <pre>
 				    2
 		   x   = x  (3 &minus; A x )/2
@@ -175,12 +171,12 @@
       overall.
 
   <p> We should probably allow a special exponent-like parameter, to speed
-      computations of a precise square root of a small number in mpf and mpfr.
+      computations of a precise square root of a small number in mpf.
 
 
 <li> <strong>Nth root</strong>
 
-  <p> Improve mpn_rootrem.  The current code is not too bad, but its average
+  <p> Improve mpn_rootrem.  The current code is not to bad, but its average
       time complexity is a function of the input, while it is possible to
       make it a function of the output.
 
--- 1/doc/tasks.html
+++ 2/doc/tasks.html
@@ -37,7 +37,7 @@
 
 <hr>
 <!-- NB. timestamp updated automatically by emacs -->
-  This file current as of 28 Nov 2009.  An up-to-date version is available at
+  This file current as of 1 May 2009.  An up-to-date version is available at
   <a href="http://gmplib.org/tasks.html">http://gmplib.org/tasks.html</a>.
   Please send comments about this page to gmp-devel<font>@</font>gmplib.org.
 
@@ -154,7 +154,7 @@
      <code>count_leading_zeros</code>, though it's possible an already
      normalized operand might not be encountered very often.
 <li> Rewrite <code>umul_ppmm</code> to use floating-point for generating the
-     most significant limb (if <code>GMP_LIMB_BITS</code> &lt= 52 bits).
+     most significant limb (if <code>BITS_PER_MP_LIMB</code> &lt= 52 bits).
      (Peter Montgomery has some ideas on this subject.)
 <li> Improve the default <code>umul_ppmm</code> code in longlong.h: Add partial
      products with fewer operations.
@@ -217,7 +217,7 @@
      an in-place there.
 <li> <code>mpf_div_ui</code>: Whether the high quotient limb is zero can be
      determined by testing the dividend for high&lt;divisor.  When non-zero, the
-     division can be done on prec dividend limbs instead of prec+1.  The result
+     divison can be done on prec dividend limbs instead of prec+1.  The result
      size is also known before the division, so that can be a tail call (once
      the <code>TMP_ALLOC</code> is eliminated).
 <li> <code>mpn_divrem_2</code> could usefully accept unnormalized divisors and
@@ -323,7 +323,7 @@
      <code>__builtin_clzl</code> and <code>__builtin_popcountl</code> using
      the corresponding CIX <code>ct</code> instructions, and
      <code>__builtin_alpha_cmpbge</code>.  These should give GCC more
-     information about scheduling etc than the <code>asm</code> blocks
+     information about sheduling etc than the <code>asm</code> blocks
      currently used in longlong.h and gmp-impl.h.
 <li> Alpha Unicos: Apparently there's no <code>alloca</code> on this system,
      making <code>configure</code> choose the slower
--- 1/doc/texinfo.tex
+++ 2/doc/texinfo.tex
@@ -3,7 +3,7 @@
 % Load plain if necessary, i.e., if running under initex.
 \expandafter\ifx\csname fmtname\endcsname\relax\input plain\fi
 %
-\def\texinfoversion{2009-11-15.11}
+\def\texinfoversion{2008-04-18.10}
 %
 % Copyright (C) 1985, 1986, 1988, 1990, 1991, 1992, 1993, 1994, 1995,
 % 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006,
@@ -4814,7 +4814,7 @@
 \chardef\maxseclevel = 3
 %
 % A numbered section within an unnumbered changes to unnumbered too.
-% To achieve this, remember the "biggest" unnum. sec. we are currently in:
+% To achive this, remember the "biggest" unnum. sec. we are currently in:
 \chardef\unmlevel = \maxseclevel
 %
 % Trace whether the current chapter is an appendix or not:
--- 1/dumbmp.c
+++ 2/dumbmp.c
@@ -101,41 +101,16 @@
     dst[i] = src[i];
 }
 
-static int
-isprime (unsigned long int t)
+int
+isprime (int n)
 {
-  unsigned long int q, r, d;
-
-  if (t < 32)
-    return (0xa08a28acUL >> t) & 1;
-  if ((t & 1) == 0)
-    return 0;
-
-  if (t % 3 == 0)
-    return 0;
-  if (t % 5 == 0)
-    return 0;
-  if (t % 7 == 0)
+  int  i;
+  if (n < 2)
     return 0;
-
-  for (d = 11;;)
-    {
-      q = t / d;
-      r = t - q * d;
-      if (q < d)
-	return 1;
-      if (r == 0)
-	break;
-      d += 2;
-      q = t / d;
-      r = t - q * d;
-      if (q < d)
-	return 1;
-      if (r == 0)
-	break;
-      d += 4;
-    }
-  return 0;
+  for (i = 2; i < n; i++)
+    if ((n % i) == 0)
+      return 0;
+  return 1;
 }
 
 int
@@ -704,16 +679,6 @@
 }
 
 void
-mpz_tdiv_r (mpz_t r, mpz_t a, mpz_t b)
-{
-  mpz_t  q;
-
-  mpz_init (q);
-  mpz_tdiv_qr (q, r, a, b);
-  mpz_clear (q);
-}
-
-void
 mpz_tdiv_q_ui (mpz_t q, mpz_t n, unsigned long d)
 {
   mpz_t  dz;
--- 1/gmp-h.in
+++ 2/gmp-h.in
@@ -28,9 +28,10 @@
 
 /* Instantiated by configure. */
 #if ! defined (__GMP_WITHIN_CONFIGURE)
+#define __GMP_BITS_PER_MP_LIMB             @BITS_PER_MP_LIMB@
 #define __GMP_HAVE_HOST_CPU_FAMILY_power   @HAVE_HOST_CPU_FAMILY_power@
 #define __GMP_HAVE_HOST_CPU_FAMILY_powerpc @HAVE_HOST_CPU_FAMILY_powerpc@
-#define GMP_LIMB_BITS                      @GMP_LIMB_BITS@
+#define GMP_LIMB_BITS                      @BITS_PER_MP_LIMB@
 #define GMP_NAIL_BITS                      @GMP_NAIL_BITS@
 #endif
 #define GMP_NUMB_BITS     (GMP_LIMB_BITS - GMP_NAIL_BITS)
@@ -194,7 +195,6 @@
 typedef long int		mp_limb_signed_t;
 #endif
 #endif
-typedef unsigned long int	mp_bitcnt_t;
 
 /* For reference, note that the name __mpz_struct gets into C++ mangled
    function names, which means although the "__" suggests an internal, we
@@ -746,9 +746,6 @@
 #define mpz_clear __gmpz_clear
 __GMP_DECLSPEC void mpz_clear __GMP_PROTO ((mpz_ptr));
 
-#define mpz_clears __gmpz_clears
-__GMP_DECLSPEC void mpz_clears __GMP_PROTO ((mpz_ptr, ...));
-
 #define mpz_clrbit __gmpz_clrbit
 __GMP_DECLSPEC void mpz_clrbit __GMP_PROTO ((mpz_ptr, unsigned long int));
 
@@ -912,9 +909,6 @@
 #define mpz_init2 __gmpz_init2
 __GMP_DECLSPEC void mpz_init2 __GMP_PROTO ((mpz_ptr, unsigned long));
 
-#define mpz_inits __gmpz_inits
-__GMP_DECLSPEC void mpz_inits __GMP_PROTO ((mpz_ptr, ...));
-
 #define mpz_init_set __gmpz_init_set
 __GMP_DECLSPEC void mpz_init_set __GMP_PROTO ((mpz_ptr, mpz_srcptr));
 
@@ -1185,9 +1179,6 @@
 #define mpq_clear __gmpq_clear
 __GMP_DECLSPEC void mpq_clear __GMP_PROTO ((mpq_ptr));
 
-#define mpq_clears __gmpq_clears
-__GMP_DECLSPEC void mpq_clears __GMP_PROTO ((mpq_ptr, ...));
-
 #define mpq_cmp __gmpq_cmp
 __GMP_DECLSPEC int mpq_cmp __GMP_PROTO ((mpq_srcptr, mpq_srcptr)) __GMP_ATTRIBUTE_PURE;
 
@@ -1221,9 +1212,6 @@
 #define mpq_init __gmpq_init
 __GMP_DECLSPEC void mpq_init __GMP_PROTO ((mpq_ptr));
 
-#define mpq_inits __gmpq_inits
-__GMP_DECLSPEC void mpq_inits __GMP_PROTO ((mpq_ptr, ...));
-
 #define mpq_inp_str __gmpq_inp_str
 #ifdef _GMP_H_HAVE_FILE
 __GMP_DECLSPEC size_t mpq_inp_str __GMP_PROTO ((mpq_ptr, FILE *, int));
@@ -1298,9 +1286,6 @@
 #define mpf_clear __gmpf_clear
 __GMP_DECLSPEC void mpf_clear __GMP_PROTO ((mpf_ptr));
 
-#define mpf_clears __gmpf_clears
-__GMP_DECLSPEC void mpf_clears __GMP_PROTO ((mpf_ptr, ...));
-
 #define mpf_cmp __gmpf_cmp
 __GMP_DECLSPEC int mpf_cmp __GMP_PROTO ((mpf_srcptr, mpf_srcptr)) __GMP_NOTHROW __GMP_ATTRIBUTE_PURE;
 
@@ -1376,9 +1361,6 @@
 #define mpf_init2 __gmpf_init2
 __GMP_DECLSPEC void mpf_init2 __GMP_PROTO ((mpf_ptr, unsigned long int));
 
-#define mpf_inits __gmpf_inits
-__GMP_DECLSPEC void mpf_inits __GMP_PROTO ((mpf_ptr, ...));
-
 #define mpf_init_set __gmpf_init_set
 __GMP_DECLSPEC void mpf_init_set __GMP_PROTO ((mpf_ptr, mpf_srcptr));
 
@@ -1568,6 +1550,9 @@
 #define mpn_mul_n __MPN(mul_n)
 __GMP_DECLSPEC void mpn_mul_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
 
+#define mpn_sqr __MPN(sqr)
+__GMP_DECLSPEC void mpn_sqr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
+
 #define mpn_neg_n __MPN(neg_n)
 #if __GMP_INLINE_PROTOTYPES || defined (__GMP_FORCE_mpn_neg_n)
 __GMP_DECLSPEC mp_limb_t mpn_neg_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
@@ -1576,9 +1561,6 @@
 #define mpn_perfect_square_p __MPN(perfect_square_p)
 __GMP_DECLSPEC int mpn_perfect_square_p __GMP_PROTO ((mp_srcptr, mp_size_t)) __GMP_ATTRIBUTE_PURE;
 
-#define mpn_perfect_power_p __MPN(perfect_power_p)
-__GMP_DECLSPEC int mpn_perfect_power_p __GMP_PROTO ((mp_srcptr, mp_size_t)) __GMP_ATTRIBUTE_PURE;
-
 #define mpn_popcount __MPN(popcount)
 __GMP_DECLSPEC unsigned long int mpn_popcount __GMP_PROTO ((mp_srcptr, mp_size_t)) __GMP_NOTHROW __GMP_ATTRIBUTE_PURE;
 
@@ -1629,29 +1611,6 @@
 #define mpn_tdiv_qr __MPN(tdiv_qr)
 __GMP_DECLSPEC void mpn_tdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
 
-#define mpn_and_n __MPN(and_n)
-__GMP_DECLSPEC void mpn_and_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-#define mpn_andn_n __MPN(andn_n)
-__GMP_DECLSPEC void mpn_andn_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-#define mpn_nand_n __MPN(nand_n)
-__GMP_DECLSPEC void mpn_nand_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-#define mpn_ior_n __MPN(ior_n)
-__GMP_DECLSPEC void mpn_ior_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-#define mpn_iorn_n __MPN(iorn_n)
-__GMP_DECLSPEC void mpn_iorn_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-#define mpn_nior_n __MPN(nior_n)
-__GMP_DECLSPEC void mpn_nior_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-#define mpn_xor_n __MPN(xor_n)
-__GMP_DECLSPEC void mpn_xor_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-#define mpn_xnor_n __MPN(xnor_n)
-__GMP_DECLSPEC void mpn_xnor_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-
-#define mpn_copyi __MPN(copyi)
-__GMP_DECLSPEC void mpn_copyi __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
-#define mpn_copyd __MPN(copyd)
-__GMP_DECLSPEC void mpn_copyd __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
-#define mpn_zero __MPN(zero)
-__GMP_DECLSPEC void mpn_zero __GMP_PROTO ((mp_ptr, mp_size_t));
 
 /**************** mpz inlines ****************/
 
@@ -2264,8 +2223,8 @@
 
 /* Major version number is the value of __GNU_MP__ too, above and in mp.h. */
 #define __GNU_MP_VERSION 4
-#define __GNU_MP_VERSION_MINOR 4
-#define __GNU_MP_VERSION_PATCHLEVEL -1
+#define __GNU_MP_VERSION_MINOR 3
+#define __GNU_MP_VERSION_PATCHLEVEL 2
 
 #define __GMP_H__
 #endif /* __GMP_H__ */
--- 1/gmp-impl.h
+++ 2/gmp-impl.h
@@ -4,7 +4,7 @@
    BE SUBJECT TO INCOMPATIBLE CHANGES IN FUTURE GNU MP RELEASES.
 
 Copyright 1991, 1993, 1994, 1995, 1996, 1997, 1999, 2000, 2001, 2002, 2003,
-2004, 2005, 2006, 2007, 2008, 2009 Free Software Foundation, Inc.
+2004, 2005, 2006, 2007, 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -56,41 +56,41 @@
    declared this way are only used to set function pointers in __gmp_cpuvec,
    they're not called directly.  */
 #define DECL_add_n(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t))
+  mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t))
 #define DECL_addmul_1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t))
 #define DECL_copyd(name) \
-  __GMP_DECLSPEC void name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t))
+  void name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t))
 #define DECL_copyi(name) \
   DECL_copyd (name)
 #define DECL_divexact_1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t))
 #define DECL_divexact_by3c(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t))
 #define DECL_divrem_1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t))
 #define DECL_gcd_1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t))
 #define DECL_lshift(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, unsigned))
+  mp_limb_t name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, unsigned))
 #define DECL_mod_1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t))
 #define DECL_mod_34lsub1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t))
+  mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t))
 #define DECL_modexact_1c_odd(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t))
 #define DECL_mul_1(name) \
   DECL_addmul_1 (name)
 #define DECL_mul_basecase(name) \
-  __GMP_DECLSPEC void name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t))
+  void name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t))
 #define DECL_preinv_divrem_1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t, int))
+  mp_limb_t name __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t, int))
 #define DECL_preinv_mod_1(name) \
-  __GMP_DECLSPEC mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t))
+  mp_limb_t name __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t))
 #define DECL_rshift(name) \
   DECL_lshift (name)
 #define DECL_sqr_basecase(name) \
-  __GMP_DECLSPEC void name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t))
+  void name __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t))
 #define DECL_sub_n(name) \
   DECL_add_n (name)
 #define DECL_submul_1(name) \
@@ -175,8 +175,8 @@
 #ifndef BYTES_PER_MP_LIMB
 #define BYTES_PER_MP_LIMB  SIZEOF_MP_LIMB_T
 #endif
-#ifndef GMP_LIMB_BITS
-#define GMP_LIMB_BITS  (8 * SIZEOF_MP_LIMB_T)
+#ifndef BITS_PER_MP_LIMB
+#define BITS_PER_MP_LIMB  (8 * SIZEOF_MP_LIMB_T)
 #endif
 
 #define BITS_PER_ULONG  (8 * SIZEOF_UNSIGNED_LONG)
@@ -198,19 +198,6 @@
 #endif
 
 
-/* gmp_intptr_t, for pointer to integer casts */
-#if HAVE_INTPTR_T
-typedef intptr_t            gmp_intptr_t;
-#else /* fallback */
-typedef size_t              gmp_intptr_t;
-#endif
-
-
-/* pre-inverse types for truncating division and modulo */
-typedef struct {mp_limb_t inv21, inv32;} gmp_pi1_t;
-typedef struct {mp_limb_t inv21, inv32, inv53;} gmp_pi2_t;
-
-
 /* const and signed must match __gmp_const and __gmp_signed, so follow the
    decision made for those in gmp.h.    */
 #if ! __GMP_HAVE_CONST
@@ -325,8 +312,8 @@
   struct tmp_reentrant_t  *next;
   size_t		  size;	  /* bytes, including header */
 };
-__GMP_DECLSPEC void *__gmp_tmp_reentrant_alloc __GMP_PROTO ((struct tmp_reentrant_t **, size_t)) ATTRIBUTE_MALLOC;
-__GMP_DECLSPEC void  __gmp_tmp_reentrant_free __GMP_PROTO ((struct tmp_reentrant_t *));
+void *__gmp_tmp_reentrant_alloc __GMP_PROTO ((struct tmp_reentrant_t **, size_t)) ATTRIBUTE_MALLOC;
+void  __gmp_tmp_reentrant_free __GMP_PROTO ((struct tmp_reentrant_t *));
 #endif
 
 #if WANT_TMP_ALLOCA
@@ -363,9 +350,9 @@
   struct tmp_stack *which_chunk;
   void *alloc_point;
 };
-__GMP_DECLSPEC void *__gmp_tmp_alloc __GMP_PROTO ((unsigned long)) ATTRIBUTE_MALLOC;
-__GMP_DECLSPEC void __gmp_tmp_mark __GMP_PROTO ((struct tmp_marker *));
-__GMP_DECLSPEC void __gmp_tmp_free __GMP_PROTO ((struct tmp_marker *));
+void *__gmp_tmp_alloc __GMP_PROTO ((unsigned long)) ATTRIBUTE_MALLOC;
+void __gmp_tmp_mark __GMP_PROTO ((struct tmp_marker *));
+void __gmp_tmp_free __GMP_PROTO ((struct tmp_marker *));
 #define TMP_SDECL		TMP_DECL
 #define TMP_DECL		struct tmp_marker __tmp_marker
 #define TMP_SMARK		TMP_MARK
@@ -390,15 +377,15 @@
   char                      *block;
   size_t                    size;
 };
-__GMP_DECLSPEC void  __gmp_tmp_debug_mark  __GMP_PROTO ((const char *, int, struct tmp_debug_t **,
-							 struct tmp_debug_t *,
-							 const char *, const char *));
-__GMP_DECLSPEC void *__gmp_tmp_debug_alloc __GMP_PROTO ((const char *, int, int,
-							 struct tmp_debug_t **, const char *,
-							 size_t)) ATTRIBUTE_MALLOC;
-__GMP_DECLSPEC void  __gmp_tmp_debug_free  __GMP_PROTO ((const char *, int, int,
-							 struct tmp_debug_t **,
-							 const char *, const char *));
+void  __gmp_tmp_debug_mark  __GMP_PROTO ((const char *, int, struct tmp_debug_t **,
+                                     struct tmp_debug_t *,
+                                     const char *, const char *));
+void *__gmp_tmp_debug_alloc __GMP_PROTO ((const char *, int, int,
+                                     struct tmp_debug_t **, const char *,
+                                     size_t)) ATTRIBUTE_MALLOC;
+void  __gmp_tmp_debug_free  __GMP_PROTO ((const char *, int, int,
+                                     struct tmp_debug_t **,
+                                     const char *, const char *));
 #define TMP_SDECL TMP_DECL_NAME(__tmp_xmarker, "__tmp_marker")
 #define TMP_DECL TMP_DECL_NAME(__tmp_xmarker, "__tmp_marker")
 #define TMP_SMARK TMP_MARK_NAME(__tmp_xmarker, "__tmp_marker")
@@ -575,10 +562,10 @@
    code that has not yet been qualified.  */
 
 #undef DIV_SB_PREINV_THRESHOLD
-#undef DC_DIV_QR_THRESHOLD
+#undef DIV_DC_THRESHOLD
 #undef POWM_THRESHOLD
 #define DIV_SB_PREINV_THRESHOLD           MP_SIZE_T_MAX
-#define DC_DIV_QR_THRESHOLD              50
+#define DIV_DC_THRESHOLD                 50
 #define POWM_THRESHOLD                    0
 
 #undef GCD_ACCEL_THRESHOLD
@@ -667,9 +654,9 @@
 __GMP_DECLSPEC extern void * (*__gmp_reallocate_func) __GMP_PROTO ((void *, size_t, size_t));
 __GMP_DECLSPEC extern void   (*__gmp_free_func) __GMP_PROTO ((void *, size_t));
 
-__GMP_DECLSPEC void *__gmp_default_allocate __GMP_PROTO ((size_t));
-__GMP_DECLSPEC void *__gmp_default_reallocate __GMP_PROTO ((void *, size_t, size_t));
-__GMP_DECLSPEC void __gmp_default_free __GMP_PROTO ((void *, size_t));
+void *__gmp_default_allocate __GMP_PROTO ((size_t));
+void *__gmp_default_reallocate __GMP_PROTO ((void *, size_t, size_t));
+void __gmp_default_free __GMP_PROTO ((void *, size_t));
 
 #define __GMP_ALLOCATE_FUNC_TYPE(n,type) \
   ((type *) (*__gmp_allocate_func) ((n) * sizeof (type)))
@@ -770,11 +757,11 @@
 #endif
 
 
-__GMP_DECLSPEC void __gmpz_aorsmul_1 __GMP_PROTO ((REGPARM_3_1 (mpz_ptr, mpz_srcptr, mp_limb_t, mp_size_t))) REGPARM_ATTR(1);
+void __gmpz_aorsmul_1 __GMP_PROTO ((REGPARM_3_1 (mpz_ptr, mpz_srcptr, mp_limb_t, mp_size_t))) REGPARM_ATTR(1);
 #define mpz_aorsmul_1(w,u,v,sub)  __gmpz_aorsmul_1 (REGPARM_3_1 (w, u, v, sub))
 
 #define mpz_n_pow_ui __gmpz_n_pow_ui
-__GMP_DECLSPEC void    mpz_n_pow_ui __GMP_PROTO ((mpz_ptr, mp_srcptr, mp_size_t, unsigned long));
+void    mpz_n_pow_ui __GMP_PROTO ((mpz_ptr, mp_srcptr, mp_size_t, unsigned long));
 
 
 #define mpn_addmul_1c __MPN(addmul_1c)
@@ -806,41 +793,11 @@
 #define mpn_addlsh1_n __MPN(addlsh1_n)
 __GMP_DECLSPEC mp_limb_t mpn_addlsh1_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
 
-/* mpn_addlsh2_n(c,a,b,n), when it exists, sets {c,n} to {a,n}+4*{b,n}, and
-   returns the carry out (0, ..., 4).  */
-#define mpn_addlsh2_n __MPN(addlsh2_n)
-__GMP_DECLSPEC mp_limb_t mpn_addlsh2_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-
-/* mpn_addlsh_n(c,a,b,n,k), when it exists, sets {c,n} to {a,n}+2^k*{b,n}, and
-   returns the carry out (0, ..., 2^k).  */
-#define mpn_addlsh_n __MPN(addlsh_n)
-  __GMP_DECLSPEC mp_limb_t mpn_addlsh_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, unsigned int));
-
 /* mpn_sublsh1_n(c,a,b,n), when it exists, sets {c,n} to {a,n}-2*{b,n}, and
    returns the borrow out (0, 1 or 2).  */
 #define mpn_sublsh1_n __MPN(sublsh1_n)
 __GMP_DECLSPEC mp_limb_t mpn_sublsh1_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
 
-/* mpn_rsblsh1_n(c,a,b,n), when it exists, sets {c,n} to 2*{b,n}-{a,n}, and
-   returns the carry out (-1, 0, 1).  */
-#define mpn_rsblsh1_n __MPN(rsblsh1_n)
-__GMP_DECLSPEC mp_limb_signed_t mpn_rsblsh1_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-
-/* mpn_sublsh2_n(c,a,b,n), when it exists, sets {c,n} to {a,n}-4*{b,n}, and
-   returns the borrow out (FIXME 0, 1, 2 or 3).  */
-#define mpn_sublsh2_n __MPN(sublsh2_n)
-__GMP_DECLSPEC mp_limb_t mpn_sublsh2_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-
-/* mpn_rsblsh2_n(c,a,b,n), when it exists, sets {c,n} to 4*{b,n}-{a,n}, and
-   returns the carry out (-1, ..., 3).  */
-#define mpn_rsblsh2_n __MPN(rsblsh2_n)
-__GMP_DECLSPEC mp_limb_signed_t mpn_rsblsh2_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-
-/* mpn_rsblsh_n(c,a,b,n,k), when it exists, sets {c,n} to 2^k*{b,n}-{a,n}, and
-   returns the carry out (-1, 0, ..., 2^k-1).  */
-#define mpn_rsblsh_n __MPN(rsblsh_n)
-__GMP_DECLSPEC mp_limb_signed_t mpn_rsblsh_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, unsigned int));
-
 /* mpn_rsh1add_n(c,a,b,n), when it exists, sets {c,n} to ({a,n} + {b,n}) >> 1,
    and returns the bit rshifted out (0 or 1).  */
 #define mpn_rsh1add_n __MPN(rsh1add_n)
@@ -856,11 +813,11 @@
 #define mpn_lshiftc __MPN(lshiftc)
 __GMP_DECLSPEC mp_limb_t mpn_lshiftc __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, unsigned int));
 
-#define mpn_add_n_sub_n __MPN(add_n_sub_n)
-__GMP_DECLSPEC mp_limb_t mpn_add_n_sub_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#define mpn_addsub_n __MPN(addsub_n)
+__GMP_DECLSPEC mp_limb_t mpn_addsub_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
 
-#define mpn_add_n_sub_nc __MPN(add_n_sub_nc)
-__GMP_DECLSPEC mp_limb_t mpn_add_n_sub_nc __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_limb_t));
+#define mpn_addsub_nc __MPN(addsub_nc)
+__GMP_DECLSPEC mp_limb_t mpn_addsub_nc __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_limb_t));
 
 #define mpn_addaddmul_1msb0 __MPN(addaddmul_1msb0)
 __GMP_DECLSPEC mp_limb_t mpn_addaddmul_1msb0 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t));
@@ -872,14 +829,14 @@
 __GMP_DECLSPEC void mpn_dump __GMP_PROTO ((mp_srcptr, mp_size_t));
 
 #define mpn_fib2_ui __MPN(fib2_ui)
-__GMP_DECLSPEC mp_size_t mpn_fib2_ui __GMP_PROTO ((mp_ptr, mp_ptr, unsigned long));
+mp_size_t mpn_fib2_ui __GMP_PROTO ((mp_ptr, mp_ptr, unsigned long));
 
 /* Remap names of internal mpn functions.  */
 #define __clz_tab               __MPN(clz_tab)
 #define mpn_udiv_w_sdiv		__MPN(udiv_w_sdiv)
 
 #define mpn_jacobi_base __MPN(jacobi_base)
-__GMP_DECLSPEC int mpn_jacobi_base __GMP_PROTO ((mp_limb_t, mp_limb_t, int)) ATTRIBUTE_CONST;
+int mpn_jacobi_base __GMP_PROTO ((mp_limb_t, mp_limb_t, int)) ATTRIBUTE_CONST;
 
 #define mpn_mod_1c __MPN(mod_1c)
 __GMP_DECLSPEC mp_limb_t mpn_mod_1c __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t)) __GMP_ATTRIBUTE_PURE;
@@ -888,7 +845,7 @@
 __GMP_DECLSPEC mp_limb_t mpn_mul_1c __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t));
 
 #define mpn_mul_2 __MPN(mul_2)
-__GMP_DECLSPEC mp_limb_t mpn_mul_2 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr));
+mp_limb_t mpn_mul_2 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr));
 
 #define mpn_mul_3 __MPN(mul_3)
 __GMP_DECLSPEC mp_limb_t mpn_mul_3 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr));
@@ -901,14 +858,13 @@
 __GMP_DECLSPEC void mpn_mul_basecase __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
 #endif
 
-#define mpn_mullo_n __MPN(mullo_n)
-__GMP_DECLSPEC void mpn_mullo_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#define mpn_mullow_n __MPN(mullow_n)
+__GMP_DECLSPEC void mpn_mullow_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
 
-#define mpn_mullo_basecase __MPN(mullo_basecase)
-__GMP_DECLSPEC void mpn_mullo_basecase __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#define mpn_mullow_basecase __MPN(mullow_basecase)
+__GMP_DECLSPEC void mpn_mullow_basecase __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
 
-#define mpn_sqr_n __MPN(sqr_n)
-__GMP_DECLSPEC void mpn_sqr_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
+#define mpn_sqr_n __MPN(sqr)	/* compatibility */
 
 #ifndef mpn_sqr_basecase  /* if not done with cpuvec in a fat binary */
 #define mpn_sqr_basecase __MPN(sqr_basecase)
@@ -918,42 +874,35 @@
 #define mpn_submul_1c __MPN(submul_1c)
 __GMP_DECLSPEC mp_limb_t mpn_submul_1c __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t));
 
+#define mpn_invert_2exp __MPN(invert_2exp)
+__GMP_DECLSPEC void mpn_invert_2exp __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
+
 #define mpn_redc_1 __MPN(redc_1)
-__GMP_DECLSPEC void mpn_redc_1 __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t));
+__GMP_DECLSPEC void mpn_redc_1 __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t);)
 
 #define mpn_redc_2 __MPN(redc_2)
 __GMP_DECLSPEC void mpn_redc_2 __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_srcptr));
-#define mpn_redc_n __MPN(redc_n)
-__GMP_DECLSPEC void mpn_redc_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_srcptr));
 
 
-#define mpn_mod_1_1p_cps __MPN(mod_1_1p_cps)
-__GMP_DECLSPEC void mpn_mod_1_1p_cps __GMP_PROTO ((mp_limb_t [4], mp_limb_t));
-#define mpn_mod_1_1p __MPN(mod_1_1p)
-__GMP_DECLSPEC mp_limb_t mpn_mod_1_1p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [4])) __GMP_ATTRIBUTE_PURE;
+#define mpn_mod_1s_1p_cps __MPN(mod_1s_1p_cps)
+__GMP_DECLSPEC void mpn_mod_1s_1p_cps __GMP_PROTO ((mp_limb_t [4], mp_limb_t));
+#define mpn_mod_1s_1p __MPN(mod_1s_1p)
+__GMP_DECLSPEC mp_limb_t mpn_mod_1s_1p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [4]));
 
 #define mpn_mod_1s_2p_cps __MPN(mod_1s_2p_cps)
 __GMP_DECLSPEC void mpn_mod_1s_2p_cps __GMP_PROTO ((mp_limb_t [5], mp_limb_t));
 #define mpn_mod_1s_2p __MPN(mod_1s_2p)
-__GMP_DECLSPEC mp_limb_t mpn_mod_1s_2p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [5])) __GMP_ATTRIBUTE_PURE;
+__GMP_DECLSPEC mp_limb_t mpn_mod_1s_2p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [5]));
 
 #define mpn_mod_1s_3p_cps __MPN(mod_1s_3p_cps)
 __GMP_DECLSPEC void mpn_mod_1s_3p_cps __GMP_PROTO ((mp_limb_t [6], mp_limb_t));
 #define mpn_mod_1s_3p __MPN(mod_1s_3p)
-__GMP_DECLSPEC mp_limb_t mpn_mod_1s_3p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [6])) __GMP_ATTRIBUTE_PURE;
+__GMP_DECLSPEC mp_limb_t mpn_mod_1s_3p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [6]));
 
 #define mpn_mod_1s_4p_cps __MPN(mod_1s_4p_cps)
 __GMP_DECLSPEC void mpn_mod_1s_4p_cps __GMP_PROTO ((mp_limb_t [7], mp_limb_t));
 #define mpn_mod_1s_4p __MPN(mod_1s_4p)
-__GMP_DECLSPEC mp_limb_t mpn_mod_1s_4p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [7])) __GMP_ATTRIBUTE_PURE;
-
-#define mpn_bc_mulmod_bnm1 __MPN(bc_mulmod_bnm1)
-__GMP_DECLSPEC void mpn_bc_mulmod_bnm1 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_ptr));
-#define mpn_mulmod_bnm1 __MPN(mulmod_bnm1)
-__GMP_DECLSPEC void mpn_mulmod_bnm1 __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-#define mpn_mulmod_bnm1_next_size __MPN(mulmod_bnm1_next_size)
-__GMP_DECLSPEC mp_size_t mpn_mulmod_bnm1_next_size __GMP_PROTO ((mp_size_t)) ATTRIBUTE_CONST;
-#define mpn_mulmod_bnm1_itch(n) (2*(n) + 2*GMP_LIMB_BITS +2)
+__GMP_DECLSPEC mp_limb_t mpn_mod_1s_4p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t [7]));
 
 
 typedef __gmp_randstate_struct *gmp_randstate_ptr;
@@ -1017,240 +966,289 @@
   } while (0)
 
 
-/* For a threshold between algorithms A and B, size>=thresh is where B
-   should be used.  Special value MP_SIZE_T_MAX means only ever use A, or
-   value 0 means only ever use B.  The tests for these special values will
-   be compile-time constants, so the compiler should be able to eliminate
-   the code for the unwanted algorithm.  */
+/* FIXME: Make these itch functions less conservative.  Also consider making
+   them dependent on just 'an', and compute the allocation directly from 'an'
+   instead of via n.  */
+static inline mp_size_t
+mpn_toom22_mul_itch (mp_size_t an, mp_size_t bn)
+{
+  mp_size_t n = 1 + (2 * an >= 3 * bn ? (an - 1) / (size_t) 3 : (bn - 1) >> 1);
+  return 4 * n + 2;
+}
 
-#define ABOVE_THRESHOLD(size,thresh)    \
-  ((thresh) == 0                        \
-   || ((thresh) != MP_SIZE_T_MAX        \
-       && (size) >= (thresh)))
-#define BELOW_THRESHOLD(size,thresh)  (! ABOVE_THRESHOLD (size, thresh))
+static inline mp_size_t
+mpn_toom33_mul_itch (mp_size_t an, mp_size_t bn)
+{
+  /* We could trim this to 4n+3 if HAVE_NATIVE_mpn_sublsh1_n, since
+     mpn_toom_interpolate_5pts only needs scratch otherwise.  */
+  mp_size_t n = (an + 2) / (size_t) 3;
+  return 6 * n + GMP_NUMB_BITS;
+}
 
-#if WANT_FFT
-#define MPN_TOOM44_MAX_N 285405
-#endif /* WANT_FFT */
+static inline mp_size_t
+mpn_toom44_mul_itch (mp_size_t an, mp_size_t bn)
+{
+  mp_size_t n = (an + 3) >> 2;
+  return 12 * n + GMP_NUMB_BITS;
+}
 
-#define MPN_TOOM22_MUL_MINSIZE    4
-#define MPN_TOOM2_SQR_MINSIZE     4
+static inline mp_size_t
+mpn_toom32_mul_itch (mp_size_t an, mp_size_t bn)
+{
+  mp_size_t n = 1 + (2 * an >= 3 * bn ? (an - 1) / (size_t) 3 : (bn - 1) >> 1);
+  return 4 * n + 2;
+}
 
-#define MPN_TOOM33_MUL_MINSIZE   17
-#define MPN_TOOM3_SQR_MINSIZE    17
+static inline mp_size_t
+mpn_toom42_mul_itch (mp_size_t an, mp_size_t bn)
+{
+  /* We could trim this to 4n+3 if HAVE_NATIVE_mpn_sublsh1_n, since
+     mpn_toom_interpolate_5pts only needs scratch otherwise.  */
+  mp_size_t n = an >= 2 * bn ? (an + 3) >> 2 : (bn + 1) >> 1;
+  return 6 * n + 3;
+}
 
-#define MPN_TOOM44_MUL_MINSIZE   30
-#define MPN_TOOM4_SQR_MINSIZE    30
+static inline mp_size_t
+mpn_toom53_mul_itch (mp_size_t an, mp_size_t bn)
+{
+  mp_size_t n = 1 + (3 * an >= 5 * bn ? (an - 1) / (size_t) 5 : (bn - 1) / (size_t) 3);
+  return 10 * n + 10;
+}
 
-#define MPN_TOOM32_MUL_MINSIZE   10
-#define MPN_TOOM42_MUL_MINSIZE   10
+static inline mp_size_t
+mpn_toom2_sqr_itch (mp_size_t an)
+{
+  mp_size_t n = 1 + ((an - 1) >> 1);
+  return 4 * n + 2;
+}
 
-#define   mpn_sqr_diagonal __MPN(sqr_diagonal)
-__GMP_DECLSPEC void      mpn_sqr_diagonal __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
+static inline mp_size_t
+mpn_toom3_sqr_itch (mp_size_t an)
+{
+  /* We could trim this to 4n+3 if HAVE_NATIVE_mpn_sublsh1_n, since
+     mpn_toom_interpolate_5pts only needs scratch otherwise.  */
+  mp_size_t n = (an + 2) / (size_t) 3;
+  return 6 * n + GMP_NUMB_BITS;
+}
 
-#define   mpn_toom_interpolate_5pts __MPN(toom_interpolate_5pts)
-__GMP_DECLSPEC void      mpn_toom_interpolate_5pts __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_size_t, int, mp_limb_t));
+static inline mp_size_t
+mpn_toom4_sqr_itch (mp_size_t an)
+{
+  mp_size_t n = (an + 3) >> 2;
+  return 12 * n + GMP_NUMB_BITS;
+}
 
-enum toom6_flags {toom6_all_pos = 0, toom6_vm1_neg = 1, toom6_vm2_neg = 2};
-#define   mpn_toom_interpolate_6pts __MPN(toom_interpolate_6pts)
-__GMP_DECLSPEC void      mpn_toom_interpolate_6pts __GMP_PROTO ((mp_ptr, mp_size_t, enum toom6_flags, mp_ptr, mp_ptr, mp_ptr, mp_size_t));
 
-enum toom7_flags { toom7_w1_neg = 1, toom7_w3_neg = 2 };
-#define   mpn_toom_interpolate_7pts __MPN(toom_interpolate_7pts)
-__GMP_DECLSPEC void      mpn_toom_interpolate_7pts __GMP_PROTO ((mp_ptr, mp_size_t, enum toom7_flags, mp_ptr, mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
+/* kara uses n+1 limbs of temporary space and then recurses with the balance,
+   so need (n+1) + (ceil(n/2)+1) + (ceil(n/4)+1) + ...  This can be solved to
+   2n + o(n).  Since n is very limited, o(n) in practice could be around 15.
+   For now, assume n is arbitrarily large.  */
+#define MPN_KARA_MUL_N_TSIZE(n)   (2*(n) + 2*GMP_LIMB_BITS)
+#define MPN_KARA_SQR_N_TSIZE(n)   (2*(n) + 2*GMP_LIMB_BITS)
+
+/* toom3 uses 2n + 2n/3 + o(n) limbs of temporary space if mpn_sublsh1_n is
+   unavailable, but just 2n + o(n) if mpn_sublsh1_n is available.  It is hard
+   to pin down the value of o(n), since it is a complex function of
+   MUL_TOOM3_THRESHOLD and n.  Normally toom3 is used between kara and fft; in
+   that case o(n) will be really limited.  If toom3 is used for arbitrarily
+   large operands, o(n) will be larger.  These definitions handle operands of
+   up to 8956264246117233 limbs.  A single multiplication using toom3 on the
+   fastest hardware currently (2008) would need 10 million years, which
+   suggests that these limits are acceptable.  */
+#if WANT_FFT
+#if HAVE_NATIVE_mpn_sublsh1_n
+#define MPN_TOOM3_MUL_N_TSIZE(n)  (2*(n) + 63)
+#define MPN_TOOM3_SQR_N_TSIZE(n)  (2*(n) + 63)
+#else
+#define MPN_TOOM3_MUL_N_TSIZE(n)  (2*(n) + 2*(n/3) + 63)
+#define MPN_TOOM3_SQR_N_TSIZE(n)  (2*(n) + 2*(n/3) + 63)
+#endif
+#else /* WANT_FFT */
+#if HAVE_NATIVE_mpn_sublsh1_n
+#define MPN_TOOM3_MUL_N_TSIZE(n)  (2*(n) + 255)
+#define MPN_TOOM3_SQR_N_TSIZE(n)  (2*(n) + 255)
+#else
+#define MPN_TOOM3_MUL_N_TSIZE(n)  (2*(n) + 2*(n/3) + 255)
+#define MPN_TOOM3_SQR_N_TSIZE(n)  (2*(n) + 2*(n/3) + 255)
+#endif
+#define MPN_TOOM44_MAX_N 285405
+#endif /* WANT_FFT */
 
-#define   mpn_toom_eval_dgr3_pm1 __MPN(toom_eval_dgr3_pm1)
-__GMP_DECLSPEC int mpn_toom_eval_dgr3_pm1 __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_size_t, mp_ptr));
+/* need 2 so that n2>=1 */
+#define MPN_KARA_MUL_N_MINSIZE    2
+#define MPN_KARA_SQR_N_MINSIZE    2
+
+/* Need l>=1, ls>=1, and 2*ls > l (the latter for the tD MPN_INCR_U) */
+#define MPN_TOOM3_MUL_N_MINSIZE   17
+#define MPN_TOOM3_SQR_N_MINSIZE   17
 
-#define   mpn_toom_eval_dgr3_pm2 __MPN(toom_eval_dgr3_pm2)
-__GMP_DECLSPEC int mpn_toom_eval_dgr3_pm2 __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_size_t, mp_ptr));
+#define MPN_TOOM44_MUL_N_MINSIZE  30	/* ??? */
+#define MPN_TOOM4_SQR_N_MINSIZE   30	/* ??? */
 
-#define   mpn_toom_eval_pm1 __MPN(toom_eval_pm1)
-__GMP_DECLSPEC int mpn_toom_eval_pm1 __GMP_PROTO ((mp_ptr, mp_ptr, unsigned, mp_srcptr, mp_size_t, mp_size_t, mp_ptr));
+#define   mpn_sqr_diagonal __MPN(sqr_diagonal)
+void      mpn_sqr_diagonal __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
 
-#define   mpn_toom_eval_pm2 __MPN(toom_eval_pm2)
-__GMP_DECLSPEC int mpn_toom_eval_pm2 __GMP_PROTO ((mp_ptr, mp_ptr, unsigned, mp_srcptr, mp_size_t, mp_size_t, mp_ptr));
+#define   mpn_kara_mul_n __MPN(kara_mul_n)
+void      mpn_kara_mul_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_ptr));
 
-#define   mpn_toom_eval_pm2exp __MPN(toom_eval_pm2exp)
-__GMP_DECLSPEC int mpn_toom_eval_pm2exp __GMP_PROTO ((mp_ptr, mp_ptr, unsigned, mp_srcptr, mp_size_t, mp_size_t, unsigned, mp_ptr));
+#define   mpn_kara_sqr_n __MPN(kara_sqr_n)
+void      mpn_kara_sqr_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
 
-#define   mpn_toom22_mul __MPN(toom22_mul)
-__GMP_DECLSPEC void      mpn_toom22_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+#define   mpn_toom_interpolate_5pts __MPN(toom_interpolate_5pts)
+void      mpn_toom_interpolate_5pts __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_size_t, int, mp_limb_t, mp_ptr));
 
-#define   mpn_toom32_mul __MPN(toom32_mul)
-__GMP_DECLSPEC void      mpn_toom32_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+enum toom4_flags { toom4_w1_neg = 1, toom4_w3_neg = 2 }; /* FIXME */
+#define   mpn_toom_interpolate_7pts __MPN(toom_interpolate_7pts)
+void      mpn_toom_interpolate_7pts __GMP_PROTO ((mp_ptr, mp_size_t, enum toom4_flags, mp_ptr, mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
 
-#define   mpn_toom42_mul __MPN(toom42_mul)
-__GMP_DECLSPEC void      mpn_toom42_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+#define   mpn_toom3_mul_n __MPN(toom3_mul_n)
+void      mpn_toom3_mul_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t,mp_ptr));
 
-#define   mpn_toom52_mul __MPN(toom52_mul)
-__GMP_DECLSPEC void      mpn_toom52_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+#define   mpn_toom3_sqr_n __MPN(toom3_sqr_n)
+void      mpn_toom3_sqr_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
 
-#define   mpn_toom62_mul __MPN(toom62_mul)
-__GMP_DECLSPEC void      mpn_toom62_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+#define   mpn_toom22_mul __MPN(toom22_mul)
+void      mpn_toom22_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 
 #define   mpn_toom2_sqr __MPN(toom2_sqr)
-__GMP_DECLSPEC void      mpn_toom2_sqr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_toom2_sqr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
 
 #define   mpn_toom33_mul __MPN(toom33_mul)
-__GMP_DECLSPEC void      mpn_toom33_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-
-#define   mpn_toom43_mul __MPN(toom43_mul)
-__GMP_DECLSPEC void      mpn_toom43_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-
-#define   mpn_toom53_mul __MPN(toom53_mul)
-__GMP_DECLSPEC void      mpn_toom53_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_toom33_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 
 #define   mpn_toom3_sqr __MPN(toom3_sqr)
-__GMP_DECLSPEC void      mpn_toom3_sqr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_toom3_sqr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
 
 #define   mpn_toom44_mul __MPN(toom44_mul)
-__GMP_DECLSPEC void      mpn_toom44_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_toom44_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+
+#define   mpn_toom32_mul __MPN(toom32_mul)
+void      mpn_toom32_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+
+#define   mpn_toom42_mul __MPN(toom42_mul)
+void      mpn_toom42_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+
+#define   mpn_toom53_mul __MPN(toom53_mul)
+void      mpn_toom53_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+
+#define   mpn_toom62_mul __MPN(toom62_mul)
+void      mpn_toom62_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 
 #define   mpn_toom4_sqr __MPN(toom4_sqr)
-__GMP_DECLSPEC void      mpn_toom4_sqr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_toom4_sqr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
 
 #define   mpn_fft_best_k __MPN(fft_best_k)
-__GMP_DECLSPEC int       mpn_fft_best_k __GMP_PROTO ((mp_size_t, int)) ATTRIBUTE_CONST;
+int       mpn_fft_best_k __GMP_PROTO ((mp_size_t, int)) ATTRIBUTE_CONST;
 
 #define   mpn_mul_fft __MPN(mul_fft)
-__GMP_DECLSPEC mp_limb_t mpn_mul_fft __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, int));
+mp_limb_t mpn_mul_fft __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, int));
 
 #define   mpn_mul_fft_full __MPN(mul_fft_full)
-__GMP_DECLSPEC void      mpn_mul_fft_full __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
+void      mpn_mul_fft_full __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
 
 #define   mpn_fft_next_size __MPN(fft_next_size)
-__GMP_DECLSPEC mp_size_t mpn_fft_next_size __GMP_PROTO ((mp_size_t, int)) ATTRIBUTE_CONST;
-
-#define   mpn_sbpi1_div_qr __MPN(sbpi1_div_qr)
-__GMP_DECLSPEC mp_limb_t mpn_sbpi1_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
-
-#define   mpn_sbpi1_div_q __MPN(sbpi1_div_q)
-__GMP_DECLSPEC mp_limb_t mpn_sbpi1_div_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
-
-#define   mpn_sbpi1_divappr_q __MPN(sbpi1_divappr_q)
-__GMP_DECLSPEC mp_limb_t mpn_sbpi1_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
+mp_size_t mpn_fft_next_size __GMP_PROTO ((mp_size_t, int)) ATTRIBUTE_CONST;
 
-#define   mpn_dcpi1_div_qr __MPN(dcpi1_div_qr)
-__GMP_DECLSPEC mp_limb_t mpn_dcpi1_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, gmp_pi1_t *));
-#define   mpn_dcpi1_div_qr_n __MPN(dcpi1_div_qr_n)
-__GMP_DECLSPEC mp_limb_t mpn_dcpi1_div_qr_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, gmp_pi1_t *, mp_ptr));
+#define   mpn_sb_divrem_mn __MPN(sb_divrem_mn)
+mp_limb_t mpn_sb_divrem_mn __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t));
 
-#define   mpn_dcpi1_div_q __MPN(dcpi1_div_q)
-__GMP_DECLSPEC mp_limb_t mpn_dcpi1_div_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, gmp_pi1_t *));
-
-#define   mpn_dcpi1_divappr_q __MPN(dcpi1_divappr_q)
-__GMP_DECLSPEC mp_limb_t mpn_dcpi1_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, gmp_pi1_t *));
-#define   mpn_dcpi1_divappr_q_n __MPN(dcpi1_divappr_q_n)
-__GMP_DECLSPEC mp_limb_t mpn_dcpi1_divappr_q_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, gmp_pi1_t *, mp_ptr));
+#define   mpn_dc_divrem_n __MPN(dc_divrem_n)
+mp_limb_t mpn_dc_divrem_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t));
 
+#define   mpn_sb_div_qr __MPN(sb_div_qr)
+mp_limb_t mpn_sb_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr));
+#define   mpn_sb_div_q __MPN(sb_div_q)
+mp_limb_t mpn_sb_div_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr));
+#define   mpn_sb_divappr_q __MPN(sb_divappr_q)
+mp_limb_t mpn_sb_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr));
+#define   mpn_dc_div_qr __MPN(dc_div_qr)
+mp_limb_t mpn_dc_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t));
+#define   mpn_dc_div_qr_n __MPN(dc_div_qr_n)
+mp_limb_t mpn_dc_div_qr_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_ptr));
+#define   mpn_dc_div_q __MPN(dc_div_q)
+mp_limb_t mpn_dc_div_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t));
+#define   mpn_preinv_dc_div_qr __MPN(preinv_dc_div_qr)
+mp_limb_t mpn_preinv_dc_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr));
+#define   mpn_dc_divappr_q __MPN(dc_divappr_q)
+mp_limb_t mpn_dc_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t));
+#define   mpn_dc_divappr_q_n __MPN(dc_divappr_q_n)
+mp_limb_t mpn_dc_divappr_q_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_ptr));
+#define   mpn_preinv_dc_divappr_q __MPN(preinv_dc_divappr_q)
+mp_limb_t mpn_preinv_dc_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr));
 #define   mpn_mu_div_qr __MPN(mu_div_qr)
-__GMP_DECLSPEC mp_limb_t mpn_mu_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+mp_limb_t mpn_mu_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_mu_div_qr_itch __MPN(mu_div_qr_itch)
-__GMP_DECLSPEC mp_size_t mpn_mu_div_qr_itch __GMP_PROTO ((mp_size_t, mp_size_t, int));
+mp_size_t mpn_mu_div_qr_itch __GMP_PROTO ((mp_size_t, mp_size_t, int));
 #define   mpn_mu_div_qr_choose_in __MPN(mu_div_qr_choose_in)
-__GMP_DECLSPEC mp_size_t mpn_mu_div_qr_choose_in __GMP_PROTO ((mp_size_t, mp_size_t, int));
-
+mp_size_t mpn_mu_div_qr_choose_in __GMP_PROTO ((mp_size_t, mp_size_t, int));
 #define   mpn_preinv_mu_div_qr __MPN(preinv_mu_div_qr)
-__GMP_DECLSPEC void      mpn_preinv_mu_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-
+void      mpn_preinv_mu_div_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_mu_divappr_q __MPN(mu_divappr_q)
-__GMP_DECLSPEC mp_limb_t mpn_mu_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+mp_limb_t mpn_mu_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_mu_divappr_q_itch __MPN(mu_divappr_q_itch)
-__GMP_DECLSPEC mp_size_t mpn_mu_divappr_q_itch __GMP_PROTO ((mp_size_t, mp_size_t, int));
+mp_size_t mpn_mu_divappr_q_itch __GMP_PROTO ((mp_size_t, mp_size_t, int));
 #define   mpn_mu_divappr_q_choose_in __MPN(mu_divappr_q_choose_in)
-__GMP_DECLSPEC mp_size_t mpn_mu_divappr_q_choose_in __GMP_PROTO ((mp_size_t, mp_size_t, int));
-
+mp_size_t mpn_mu_divappr_q_choose_in __GMP_PROTO ((mp_size_t, mp_size_t, int));
 #define   mpn_preinv_mu_divappr_q __MPN(preinv_mu_divappr_q)
-__GMP_DECLSPEC void      mpn_preinv_mu_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-
+void      mpn_preinv_mu_divappr_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_mu_div_q __MPN(mu_div_q)
-__GMP_DECLSPEC mp_limb_t mpn_mu_div_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-
+mp_limb_t mpn_mu_div_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_invert __MPN(invert)
-__GMP_DECLSPEC void      mpn_invert __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
-#define mpn_invert_itch(n)  mpn_invertappr_itch(n)
-
-#define   mpn_ni_invertappr __MPN(ni_invertappr)
-__GMP_DECLSPEC mp_limb_t mpn_ni_invertappr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
-#define   mpn_invertappr __MPN(invertappr)
-__GMP_DECLSPEC mp_limb_t mpn_invertappr __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
-#define mpn_invertappr_itch(n)  (3 * (n) + 2)
+void      mpn_invert __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
+#define   mpn_invert_itch __MPN(invert_itch)
+mp_size_t mpn_invert_itch __GMP_PROTO ((mp_size_t));
 
 #define   mpn_binvert __MPN(binvert)
-__GMP_DECLSPEC void      mpn_binvert __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_binvert __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_binvert_itch __MPN(binvert_itch)
-__GMP_DECLSPEC mp_size_t mpn_binvert_itch __GMP_PROTO ((mp_size_t));
-
-#define mpn_bdiv_q_1 __MPN(bdiv_q_1)
-__GMP_DECLSPEC mp_limb_t mpn_bdiv_q_1 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t));
-
-#define mpn_bdiv_q_1_pi1 __MPN(bdiv_q_1_pi1)
-__GMP_DECLSPEC mp_limb_t mpn_bdiv_q_1_pi1 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t, int));
-
-#define   mpn_sbpi1_bdiv_qr __MPN(sbpi1_bdiv_qr)
-__GMP_DECLSPEC mp_limb_t mpn_sbpi1_bdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
-
-#define   mpn_sbpi1_bdiv_q __MPN(sbpi1_bdiv_q)
-__GMP_DECLSPEC void      mpn_sbpi1_bdiv_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
-
-#define   mpn_dcpi1_bdiv_qr __MPN(dcpi1_bdiv_qr)
-__GMP_DECLSPEC mp_limb_t mpn_dcpi1_bdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
-#define   mpn_dcpi1_bdiv_qr_n_itch __MPN(dcpi1_bdiv_qr_n_itch)
-__GMP_DECLSPEC mp_size_t mpn_dcpi1_bdiv_qr_n_itch __GMP_PROTO ((mp_size_t));
-
-#define   mpn_dcpi1_bdiv_qr_n __MPN(dcpi1_bdiv_qr_n)
-__GMP_DECLSPEC mp_limb_t mpn_dcpi1_bdiv_qr_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_ptr));
-#define   mpn_dcpi1_bdiv_q __MPN(dcpi1_bdiv_q)
-__GMP_DECLSPEC void      mpn_dcpi1_bdiv_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
-
-#define   mpn_dcpi1_bdiv_q_n_itch __MPN(dcpi1_bdiv_q_n_itch)
-__GMP_DECLSPEC mp_size_t mpn_dcpi1_bdiv_q_n_itch __GMP_PROTO ((mp_size_t));
-#define   mpn_dcpi1_bdiv_q_n __MPN(dcpi1_bdiv_q_n)
-__GMP_DECLSPEC void      mpn_dcpi1_bdiv_q_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_ptr));
-
+mp_size_t mpn_binvert_itch __GMP_PROTO ((mp_size_t));
+#define   mpn_sb_bdiv_qr __MPN(sb_bdiv_qr)
+mp_limb_t mpn_sb_bdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
+#define   mpn_sb_bdiv_q __MPN(sb_bdiv_q)
+void      mpn_sb_bdiv_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
+#define   mpn_dc_bdiv_qr __MPN(dc_bdiv_qr)
+mp_limb_t mpn_dc_bdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
+#define   mpn_dc_bdiv_qr_n_itch __MPN(dc_bdiv_qr_n_itch)
+mp_size_t mpn_dc_bdiv_qr_n_itch __GMP_PROTO ((mp_size_t));
+#define   mpn_dc_bdiv_qr_n __MPN(dc_bdiv_qr_n)
+mp_limb_t mpn_dc_bdiv_qr_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_ptr));
+#define   mpn_dc_bdiv_q __MPN(dc_bdiv_q)
+void      mpn_dc_bdiv_q __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t));
+#define   mpn_dc_bdiv_q_n_itch __MPN(dc_bdiv_q_n_itch)
+mp_size_t mpn_dc_bdiv_q_n_itch __GMP_PROTO ((mp_size_t));
+#define   mpn_dc_bdiv_q_n __MPN(dc_bdiv_q_n)
+void      mpn_dc_bdiv_q_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_ptr));
 #define   mpn_mu_bdiv_qr __MPN(mu_bdiv_qr)
-__GMP_DECLSPEC mp_limb_t mpn_mu_bdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_mu_bdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_mu_bdiv_qr_itch __MPN(mu_bdiv_qr_itch)
-__GMP_DECLSPEC mp_size_t mpn_mu_bdiv_qr_itch __GMP_PROTO ((mp_size_t, mp_size_t));
-
+mp_size_t mpn_mu_bdiv_qr_itch __GMP_PROTO ((mp_size_t, mp_size_t));
 #define   mpn_mu_bdiv_q __MPN(mu_bdiv_q)
-__GMP_DECLSPEC void      mpn_mu_bdiv_q __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_mu_bdiv_q __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_mu_bdiv_q_itch __MPN(mu_bdiv_q_itch)
-__GMP_DECLSPEC mp_size_t mpn_mu_bdiv_q_itch __GMP_PROTO ((mp_size_t, mp_size_t));
-
-#define   mpn_bdiv_qr __MPN(bdiv_qr)
-__GMP_DECLSPEC mp_limb_t mpn_bdiv_qr __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-#define   mpn_bdiv_qr_itch __MPN(bdiv_qr_itch)
-__GMP_DECLSPEC mp_size_t mpn_bdiv_qr_itch __GMP_PROTO ((mp_size_t, mp_size_t));
-
-#define   mpn_bdiv_q __MPN(bdiv_q)
-__GMP_DECLSPEC void      mpn_bdiv_q __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
-#define   mpn_bdiv_q_itch __MPN(bdiv_q_itch)
-__GMP_DECLSPEC mp_size_t mpn_bdiv_q_itch __GMP_PROTO ((mp_size_t, mp_size_t));
+mp_size_t mpn_mu_bdiv_q_itch __GMP_PROTO ((mp_size_t, mp_size_t));
 
 #define   mpn_divexact __MPN(divexact)
-__GMP_DECLSPEC void      mpn_divexact __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
+void      mpn_divexact __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_divexact_itch __MPN(divexact_itch)
-__GMP_DECLSPEC mp_size_t mpn_divexact_itch __GMP_PROTO ((mp_size_t, mp_size_t));
+mp_size_t mpn_divexact_itch __GMP_PROTO ((mp_size_t, mp_size_t));
+
 
 #define   mpn_bdiv_dbm1c __MPN(bdiv_dbm1c)
-__GMP_DECLSPEC mp_limb_t mpn_bdiv_dbm1c __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t));
+mp_limb_t mpn_bdiv_dbm1c __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t));
 #define   mpn_bdiv_dbm1(dst, src, size, divisor) \
   mpn_bdiv_dbm1c (dst, src, size, divisor, __GMP_CAST (mp_limb_t, 0))
 
 #define   mpn_powm __MPN(powm)
-__GMP_DECLSPEC void      mpn_powm __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_powm __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_powlo __MPN(powlo)
-__GMP_DECLSPEC void      mpn_powlo __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_size_t, mp_ptr));
+void      mpn_powlo __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_size_t, mp_ptr));
+
 #define   mpn_powm_sec __MPN(powm_sec)
-__GMP_DECLSPEC void      mpn_powm_sec __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_powm_sec __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_subcnd_n __MPN(subcnd_n)
-__GMP_DECLSPEC mp_limb_t mpn_subcnd_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_limb_t));
+mp_limb_t mpn_subcnd_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_limb_t));
 #define   mpn_tabselect __MPN(tabselect)
-__GMP_DECLSPEC void      mpn_tabselect __GMP_PROTO ((volatile mp_limb_t *, volatile mp_limb_t *, mp_size_t, mp_size_t, mp_size_t));
-#define mpn_redc_1_sec __MPN(redc_1_sec)
-__GMP_DECLSPEC void mpn_redc_1_sec __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t));
+void      mpn_tabselect __GMP_PROTO ((volatile mp_limb_t *, volatile mp_limb_t *, mp_size_t, mp_size_t, mp_size_t));
 
 #ifndef DIVEXACT_BY3_METHOD
 #if GMP_NUMB_BITS % 2 == 0 && ! defined (HAVE_NATIVE_mpn_divexact_by3c)
@@ -1303,18 +1301,18 @@
 #endif
 
 #define mpz_divexact_gcd  __gmpz_divexact_gcd
-__GMP_DECLSPEC void    mpz_divexact_gcd __GMP_PROTO ((mpz_ptr, mpz_srcptr, mpz_srcptr));
+void    mpz_divexact_gcd __GMP_PROTO ((mpz_ptr, mpz_srcptr, mpz_srcptr));
 
 #define mpz_inp_str_nowhite __gmpz_inp_str_nowhite
 #ifdef _GMP_H_HAVE_FILE
-__GMP_DECLSPEC size_t  mpz_inp_str_nowhite __GMP_PROTO ((mpz_ptr, FILE *, int, int, size_t));
+size_t  mpz_inp_str_nowhite __GMP_PROTO ((mpz_ptr, FILE *, int, int, size_t));
 #endif
 
 #define mpn_divisible_p __MPN(divisible_p)
-__GMP_DECLSPEC int     mpn_divisible_p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_srcptr, mp_size_t)) __GMP_ATTRIBUTE_PURE;
+int     mpn_divisible_p __GMP_PROTO ((mp_srcptr, mp_size_t, mp_srcptr, mp_size_t)) __GMP_ATTRIBUTE_PURE;
 
 #define   mpn_rootrem __MPN(rootrem)
-__GMP_DECLSPEC mp_size_t mpn_rootrem __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t));
+mp_size_t mpn_rootrem __GMP_PROTO ((mp_ptr, mp_ptr, mp_srcptr, mp_size_t, mp_limb_t));
 
 
 #if defined (_CRAY)
@@ -1564,7 +1562,7 @@
     mpz_ptr __x = (X);                                                  \
     ASSERT ((NLIMBS) >= 1);                                             \
     __x->_mp_alloc = (NLIMBS);                                          \
-    __x->_mp_d = TMP_ALLOC_LIMBS (NLIMBS);				\
+    __x->_mp_d = (mp_ptr) TMP_ALLOC ((NLIMBS) * BYTES_PER_MP_LIMB);     \
   } while (0)
 
 /* Realloc for an mpz_t WHAT if it has less than NEEDED limbs.  */
@@ -1608,149 +1606,149 @@
 __GMP_DECLSPEC extern const mp_limb_t __gmp_fib_table[];
 #define FIB_TABLE(n)  (__gmp_fib_table[(n)+1])
 
-#define SIEVESIZE 512		/* FIXME: Allow gmp_init_primesieve to choose */
-typedef struct
-{
-  unsigned long d;		   /* current index in s[] */
-  unsigned long s0;		   /* number corresponding to s[0] */
-  unsigned long sqrt_s0;	   /* misnomer for sqrt(s[SIEVESIZE-1]) */
-  unsigned char s[SIEVESIZE + 1];  /* sieve table */
-} gmp_primesieve_t;
-
-#define gmp_init_primesieve __gmp_init_primesieve
-__GMP_DECLSPEC void gmp_init_primesieve (gmp_primesieve_t *);
-
-#define gmp_nextprime __gmp_nextprime
-__GMP_DECLSPEC unsigned long int gmp_nextprime (gmp_primesieve_t *);
 
+/* For a threshold between algorithms A and B, size>=thresh is where B
+   should be used.  Special value MP_SIZE_T_MAX means only ever use A, or
+   value 0 means only ever use B.  The tests for these special values will
+   be compile-time constants, so the compiler should be able to eliminate
+   the code for the unwanted algorithm.  */
 
-#ifndef MUL_TOOM22_THRESHOLD
-#define MUL_TOOM22_THRESHOLD             30
-#endif
+#define ABOVE_THRESHOLD(size,thresh)    \
+  ((thresh) == 0                        \
+   || ((thresh) != MP_SIZE_T_MAX        \
+       && (size) >= (thresh)))
+#define BELOW_THRESHOLD(size,thresh)  (! ABOVE_THRESHOLD (size, thresh))
 
-#ifndef MUL_TOOM33_THRESHOLD
-#define MUL_TOOM33_THRESHOLD            100
+/* Usage: int  use_foo = BELOW_THRESHOLD (size, FOO_THRESHOLD);
+          ...
+          if (CACHED_BELOW_THRESHOLD (use_foo, size, FOO_THRESHOLD))
+
+   When "use_foo" is a constant (thresh is 0 or MP_SIZE_T), gcc prior to
+   version 3.3 doesn't optimize away a test "if (use_foo)" when within a
+   loop.  CACHED_BELOW_THRESHOLD helps it do so.  */
+
+#define CACHED_ABOVE_THRESHOLD(cache, thresh)           \
+  ((thresh) == 0 || (thresh) == MP_SIZE_T_MAX           \
+   ? ABOVE_THRESHOLD (0, thresh)                        \
+   : (cache))
+#define CACHED_BELOW_THRESHOLD(cache, thresh)           \
+  ((thresh) == 0 || (thresh) == MP_SIZE_T_MAX           \
+   ? BELOW_THRESHOLD (0, thresh)                        \
+   : (cache))
+
+
+/* If MUL_KARATSUBA_THRESHOLD is not already defined, define it to a
+   value which is good on most machines.  */
+#ifndef MUL_KARATSUBA_THRESHOLD
+#define MUL_KARATSUBA_THRESHOLD 32
+#endif
+
+/* If MUL_TOOM3_THRESHOLD is not already defined, define it to a
+   value which is good on most machines.  */
+#ifndef MUL_TOOM3_THRESHOLD
+#define MUL_TOOM3_THRESHOLD 128
 #endif
 
 #ifndef MUL_TOOM44_THRESHOLD
-#define MUL_TOOM44_THRESHOLD            300
+#define MUL_TOOM44_THRESHOLD 500
 #endif
 
-/* MUL_TOOM22_THRESHOLD_LIMIT is the maximum for MUL_TOOM22_THRESHOLD.  In a
-   normal build MUL_TOOM22_THRESHOLD is a constant and we use that.  In a fat
-   binary or tune program build MUL_TOOM22_THRESHOLD is a variable and a
-   separate hard limit will have been defined.  Similarly for TOOM3.  */
-#ifndef MUL_TOOM22_THRESHOLD_LIMIT
-#define MUL_TOOM22_THRESHOLD_LIMIT  MUL_TOOM22_THRESHOLD
+/* Source compatibility while source is in flux.  */
+#define MUL_TOOM22_THRESHOLD MUL_KARATSUBA_THRESHOLD
+#define MUL_TOOM33_THRESHOLD MUL_TOOM3_THRESHOLD
+#define SQR_TOOM2_THRESHOLD SQR_KARATSUBA_THRESHOLD
+
+/* MUL_KARATSUBA_THRESHOLD_LIMIT is the maximum for MUL_KARATSUBA_THRESHOLD.
+   In a normal build MUL_KARATSUBA_THRESHOLD is a constant and we use that.
+   In a fat binary or tune program build MUL_KARATSUBA_THRESHOLD is a
+   variable and a separate hard limit will have been defined.  Similarly for
+   TOOM3.  */
+#ifndef MUL_KARATSUBA_THRESHOLD_LIMIT
+#define MUL_KARATSUBA_THRESHOLD_LIMIT  MUL_KARATSUBA_THRESHOLD
 #endif
-#ifndef MUL_TOOM33_THRESHOLD_LIMIT
-#define MUL_TOOM33_THRESHOLD_LIMIT  MUL_TOOM33_THRESHOLD
+#ifndef MUL_TOOM3_THRESHOLD_LIMIT
+#define MUL_TOOM3_THRESHOLD_LIMIT  MUL_TOOM3_THRESHOLD
 #endif
-#ifndef MULLO_BASECASE_THRESHOLD_LIMIT
-#define MULLO_BASECASE_THRESHOLD_LIMIT  MULLO_BASECASE_THRESHOLD
+#ifndef MULLOW_BASECASE_THRESHOLD_LIMIT
+#define MULLOW_BASECASE_THRESHOLD_LIMIT  MULLOW_BASECASE_THRESHOLD
 #endif
 
 /* SQR_BASECASE_THRESHOLD is where mpn_sqr_basecase should take over from
-   mpn_mul_basecase.  Default is to use mpn_sqr_basecase from 0.  (Note that we
-   certainly always want it if there's a native assembler mpn_sqr_basecase.)
-
-   If it turns out that mpn_toom2_sqr becomes faster than mpn_mul_basecase
-   before mpn_sqr_basecase does, then SQR_BASECASE_THRESHOLD is the toom2
-   threshold and SQR_TOOM2_THRESHOLD is 0.  This oddity arises more or less
-   because SQR_TOOM2_THRESHOLD represents the size up to which mpn_sqr_basecase
-   should be used, and that may be never.  */
+   mpn_mul_basecase in mpn_sqr_n.  Default is to use mpn_sqr_basecase
+   always.  (Note that we certainly always want it if there's a native
+   assembler mpn_sqr_basecase.)
+
+   If it turns out that mpn_kara_sqr_n becomes faster than mpn_mul_basecase
+   before mpn_sqr_basecase does, then SQR_BASECASE_THRESHOLD is the
+   karatsuba threshold and SQR_KARATSUBA_THRESHOLD is 0.  This oddity arises
+   more or less because SQR_KARATSUBA_THRESHOLD represents the size up to
+   which mpn_sqr_basecase should be used, and that may be never.  */
 
 #ifndef SQR_BASECASE_THRESHOLD
-#define SQR_BASECASE_THRESHOLD            0
+#define SQR_BASECASE_THRESHOLD 0
 #endif
 
-#ifndef SQR_TOOM2_THRESHOLD
-#define SQR_TOOM2_THRESHOLD              50
+#ifndef SQR_KARATSUBA_THRESHOLD
+#define SQR_KARATSUBA_THRESHOLD (2*MUL_KARATSUBA_THRESHOLD)
 #endif
 
 #ifndef SQR_TOOM3_THRESHOLD
-#define SQR_TOOM3_THRESHOLD             120
+#define SQR_TOOM3_THRESHOLD 128
 #endif
 
 #ifndef SQR_TOOM4_THRESHOLD
-#define SQR_TOOM4_THRESHOLD             400
+#define SQR_TOOM4_THRESHOLD 500
 #endif
 
-/* See comments above about MUL_TOOM33_THRESHOLD_LIMIT.  */
+/* See comments above about MUL_TOOM3_THRESHOLD_LIMIT.  */
 #ifndef SQR_TOOM3_THRESHOLD_LIMIT
 #define SQR_TOOM3_THRESHOLD_LIMIT  SQR_TOOM3_THRESHOLD
 #endif
 
-#ifndef DC_DIVAPPR_Q_THRESHOLD
-#define DC_DIVAPPR_Q_THRESHOLD          200
+#ifndef DC_DIV_QR_THRESHOLD
+#define DC_DIV_QR_THRESHOLD       43
 #endif
 
-#ifndef DC_DIV_QR_THRESHOLD
-#define DC_DIV_QR_THRESHOLD              50
+#ifndef DC_DIVAPPR_Q_THRESHOLD
+#define DC_DIVAPPR_Q_THRESHOLD   208
 #endif
 
 #ifndef DC_DIV_Q_THRESHOLD
-#define DC_DIV_Q_THRESHOLD              228
+#define DC_DIV_Q_THRESHOLD       228
 #endif
 
 #ifndef DC_BDIV_QR_THRESHOLD
-#define DC_BDIV_QR_THRESHOLD             50
+#define DC_BDIV_QR_THRESHOLD      52
 #endif
 
 #ifndef DC_BDIV_Q_THRESHOLD
-#define DC_BDIV_Q_THRESHOLD             180
+#define DC_BDIV_Q_THRESHOLD      224
 #endif
 
 #ifndef DIVEXACT_JEB_THRESHOLD
-#define DIVEXACT_JEB_THRESHOLD           25
+#define DIVEXACT_JEB_THRESHOLD    25
 #endif
 
 #ifndef INV_NEWTON_THRESHOLD
-#define INV_NEWTON_THRESHOLD            200
+#define INV_NEWTON_THRESHOLD     654
 #endif
 
 #ifndef BINV_NEWTON_THRESHOLD
-#define BINV_NEWTON_THRESHOLD           300
+#define BINV_NEWTON_THRESHOLD    807
 #endif
 
 #ifndef MU_DIVAPPR_Q_THRESHOLD
-#define MU_DIVAPPR_Q_THRESHOLD         4000
+#define MU_DIVAPPR_Q_THRESHOLD  4000
 #endif
 
 #ifndef MU_DIV_Q_THRESHOLD
-#define MU_DIV_Q_THRESHOLD             4000
+#define MU_DIV_Q_THRESHOLD      4000
 #endif
 
 #ifndef MU_BDIV_Q_THRESHOLD
-#define MU_BDIV_Q_THRESHOLD            2000
-#endif
-
-#ifndef MU_BDIV_QR_THRESHOLD
-#define MU_BDIV_QR_THRESHOLD           2000
-#endif
-
-#ifndef MULMOD_BNM1_THRESHOLD
-#define MULMOD_BNM1_THRESHOLD            16
-#endif
-
-#if HAVE_NATIVE_mpn_addmul_2 || HAVE_NATIVE_mpn_redc_2
-
-#ifndef REDC_1_TO_REDC_2_THRESHOLD
-#define REDC_1_TO_REDC_2_THRESHOLD       15
-#endif
-#ifndef REDC_2_TO_REDC_N_THRESHOLD
-#define REDC_2_TO_REDC_N_THRESHOLD      100
-#endif
-
-#else
-
-#ifndef REDC_1_TO_REDC_N_THRESHOLD
-#define REDC_1_TO_REDC_N_THRESHOLD      100
+#define MU_BDIV_Q_THRESHOLD     2000
 #endif
 
-#endif /* HAVE_NATIVE_mpn_addmul_2 || HAVE_NATIVE_mpn_redc_2 */
-
-
 /* First k to use for an FFT modF multiply.  A modF FFT is an order
    log(2^k)/log(2^(k-1)) algorithm, so k=3 is merely 1.5 like karatsuba,
    whereas k=4 is 1.33 which is faster than toom3 at 1.485.    */
@@ -1758,7 +1756,7 @@
 
 /* Threshold at which FFT should be used to do a modF NxN -> N multiply. */
 #ifndef MUL_FFT_MODF_THRESHOLD
-#define MUL_FFT_MODF_THRESHOLD   (MUL_TOOM33_THRESHOLD * 3)
+#define MUL_FFT_MODF_THRESHOLD   (MUL_TOOM3_THRESHOLD * 3)
 #endif
 #ifndef SQR_FFT_MODF_THRESHOLD
 #define SQR_FFT_MODF_THRESHOLD   (SQR_TOOM3_THRESHOLD * 3)
@@ -1781,12 +1779,12 @@
    etc.  See mpn_fft_best_k(). */
 #ifndef MUL_FFT_TABLE
 #define MUL_FFT_TABLE                           \
-  { MUL_TOOM33_THRESHOLD * 4,   /* k=5 */        \
-    MUL_TOOM33_THRESHOLD * 8,   /* k=6 */        \
-    MUL_TOOM33_THRESHOLD * 16,  /* k=7 */        \
-    MUL_TOOM33_THRESHOLD * 32,  /* k=8 */        \
-    MUL_TOOM33_THRESHOLD * 96,  /* k=9 */        \
-    MUL_TOOM33_THRESHOLD * 288, /* k=10 */       \
+  { MUL_TOOM3_THRESHOLD * 4,   /* k=5 */        \
+    MUL_TOOM3_THRESHOLD * 8,   /* k=6 */        \
+    MUL_TOOM3_THRESHOLD * 16,  /* k=7 */        \
+    MUL_TOOM3_THRESHOLD * 32,  /* k=8 */        \
+    MUL_TOOM3_THRESHOLD * 96,  /* k=9 */        \
+    MUL_TOOM3_THRESHOLD * 288, /* k=10 */       \
     0 }
 #endif
 #ifndef SQR_FFT_TABLE
@@ -1807,8 +1805,15 @@
 #define MPN_FFT_TABLE_SIZE  16
 
 
-#ifndef DC_DIV_QR_THRESHOLD
-#define DC_DIV_QR_THRESHOLD    (3 * MUL_TOOM22_THRESHOLD)
+/* mpn_dc_divrem_n(n) calls 2*mul(n/2)+2*div(n/2), thus to be faster than
+   div(n) = 4*div(n/2), we need mul(n/2) to be faster than the classic way,
+   i.e. n/2 >= MUL_KARATSUBA_THRESHOLD
+
+   Measured values are between 2 and 4 times MUL_KARATSUBA_THRESHOLD, so go
+   for 3 as an average.  */
+
+#ifndef DIV_DC_THRESHOLD
+#define DIV_DC_THRESHOLD    (3 * MUL_KARATSUBA_THRESHOLD)
 #endif
 
 #ifndef GET_STR_DC_THRESHOLD
@@ -1877,7 +1882,7 @@
 #define ASSERT_FILE  ""
 #endif
 
-__GMP_DECLSPEC void __gmp_assert_header __GMP_PROTO ((const char *, int));
+void __gmp_assert_header __GMP_PROTO ((const char *, int));
 __GMP_DECLSPEC void __gmp_assert_fail __GMP_PROTO ((const char *, int, const char *)) ATTRIBUTE_NORETURN;
 
 #if HAVE_STRINGIZE
@@ -2005,7 +2010,7 @@
 
 #if HAVE_NATIVE_mpn_com_n
 #define mpn_com_n __MPN(com_n)
-__GMP_DECLSPEC void    mpn_com_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
+void    mpn_com_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t));
 #else
 #define mpn_com_n(d,s,n)                                \
   do {                                                  \
@@ -2020,82 +2025,84 @@
   } while (0)
 #endif
 
-#define MPN_LOGOPS_N_INLINE(rp, up, vp, n, operation)			\
-  do {									\
-    mp_srcptr	__up = (up);						\
-    mp_srcptr	__vp = (vp);						\
-    mp_ptr	__rp = (rp);						\
-    mp_size_t	__n = (n);						\
-    mp_limb_t __a, __b;							\
-    ASSERT (__n > 0);							\
-    ASSERT (MPN_SAME_OR_SEPARATE_P (__rp, __up, __n));			\
-    ASSERT (MPN_SAME_OR_SEPARATE_P (__rp, __vp, __n));			\
-    __up += __n;							\
-    __vp += __n;							\
-    __rp += __n;							\
-    __n = -__n;								\
-    do {								\
-      __a = __up[__n];							\
-      __b = __vp[__n];							\
-      __rp[__n] = operation;						\
-    } while (++__n);							\
+#define MPN_LOGOPS_N_INLINE(d, s1, s2, n, operation)    \
+  do {                                                  \
+    mp_ptr	 __d = (d);                             \
+    mp_srcptr	 __s1 = (s1);                           \
+    mp_srcptr	 __s2 = (s2);                           \
+    mp_size_t	 __n = (n);                             \
+    ASSERT (__n >= 1);                                  \
+    ASSERT (MPN_SAME_OR_SEPARATE_P (__d, __s1, __n));   \
+    ASSERT (MPN_SAME_OR_SEPARATE_P (__d, __s2, __n));   \
+    do                                                  \
+      operation;                                        \
+    while (--__n);                                      \
   } while (0)
 
-
-#if ! HAVE_NATIVE_mpn_and_n
-#undef mpn_and_n
-#define mpn_and_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, __a & __b)
+#if HAVE_NATIVE_mpn_and_n
+#define mpn_and_n __MPN(and_n)
+void    mpn_and_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_and_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = *__s1++ & *__s2++)
 #endif
 
-#if ! HAVE_NATIVE_mpn_andn_n
-#undef mpn_andn_n
-#define mpn_andn_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, __a & ~__b)
+#if HAVE_NATIVE_mpn_andn_n
+#define mpn_andn_n __MPN(andn_n)
+void    mpn_andn_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_andn_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = *__s1++ & ~*__s2++)
 #endif
 
-#if ! HAVE_NATIVE_mpn_nand_n
-#undef mpn_nand_n
-#define mpn_nand_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, ~(__a & __b) & GMP_NUMB_MASK)
+#if HAVE_NATIVE_mpn_nand_n
+#define mpn_nand_n __MPN(nand_n)
+void    mpn_nand_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_nand_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = ~(*__s1++ & *__s2++) & GMP_NUMB_MASK)
 #endif
 
-#if ! HAVE_NATIVE_mpn_ior_n
-#undef mpn_ior_n
-#define mpn_ior_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, __a | __b)
+#if HAVE_NATIVE_mpn_ior_n
+#define mpn_ior_n __MPN(ior_n)
+void    mpn_ior_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_ior_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = *__s1++ | *__s2++)
 #endif
 
-#if ! HAVE_NATIVE_mpn_iorn_n
-#undef mpn_iorn_n
-#define mpn_iorn_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, (__a | ~__b) & GMP_NUMB_MASK)
+#if HAVE_NATIVE_mpn_iorn_n
+#define mpn_iorn_n __MPN(iorn_n)
+void    mpn_iorn_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_iorn_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = (*__s1++ | ~*__s2++) & GMP_NUMB_MASK)
 #endif
 
-#if ! HAVE_NATIVE_mpn_nior_n
-#undef mpn_nior_n
-#define mpn_nior_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, ~(__a | __b) & GMP_NUMB_MASK)
+#if HAVE_NATIVE_mpn_nior_n
+#define mpn_nior_n __MPN(nior_n)
+void    mpn_nior_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_nior_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = ~(*__s1++ | *__s2++) & GMP_NUMB_MASK)
 #endif
 
-#if ! HAVE_NATIVE_mpn_xor_n
-#undef mpn_xor_n
-#define mpn_xor_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, __a ^ __b)
+#if HAVE_NATIVE_mpn_xor_n
+#define mpn_xor_n __MPN(xor_n)
+void    mpn_xor_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_xor_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = *__s1++ ^ *__s2++)
 #endif
 
-#if ! HAVE_NATIVE_mpn_xnor_n
-#undef mpn_xnor_n
-#define mpn_xnor_n(rp, up, vp, n) \
-  MPN_LOGOPS_N_INLINE (rp, up, vp, n, ~(__a ^ __b) & GMP_NUMB_MASK)
+#if HAVE_NATIVE_mpn_xnor_n
+#define mpn_xnor_n __MPN(xnor_n)
+void    mpn_xnor_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
+#else
+#define mpn_xnor_n(d, s1, s2, n) \
+  MPN_LOGOPS_N_INLINE (d, s1, s2, n, *__d++ = ~(*__s1++ ^ *__s2++) & GMP_NUMB_MASK)
 #endif
 
-#define mpn_trialdiv __MPN(trialdiv)
-__GMP_DECLSPEC mp_limb_t mpn_trialdiv __GMP_PROTO ((mp_srcptr, mp_size_t, mp_size_t, int *));
-
-#define mpn_remove __MPN(remove)
-__GMP_DECLSPEC mp_bitcnt_t mpn_remove __GMP_PROTO ((mp_ptr, mp_size_t *, mp_ptr, mp_size_t, mp_ptr, mp_size_t, mp_bitcnt_t));
-
 
 /* ADDC_LIMB sets w=x+y and cout to 0 or 1 for a carry from that addition. */
 #if GMP_NAIL_BITS == 0
@@ -2156,7 +2163,7 @@
    for the benefit of assertion checking.  */
 
 #if defined (__GNUC__) && HAVE_HOST_CPU_FAMILY_x86 && GMP_NAIL_BITS == 0      \
-  && GMP_LIMB_BITS == 32 && ! defined (NO_ASM) && ! WANT_ASSERT
+  && BITS_PER_MP_LIMB == 32 && ! defined (NO_ASM) && ! WANT_ASSERT
 /* Better flags handling than the generic C gives on i386, saving a few
    bytes of code and maybe a cycle or two.  */
 
@@ -2345,13 +2352,14 @@
      i.e. the number of bits used to represent each digit in the base.  */
   mp_limb_t big_base;
 
-  /* A GMP_LIMB_BITS bit approximation to 1/big_base, represented as a
+  /* A BITS_PER_MP_LIMB bit approximation to 1/big_base, represented as a
      fixed-point number.  Instead of dividing by big_base an application can
      choose to multiply by big_base_inverted.  */
   mp_limb_t big_base_inverted;
 };
 
 #define   mp_bases __MPN(bases)
+#define __mp_bases __MPN(bases)
 __GMP_DECLSPEC extern const struct bases mp_bases[257];
 
 
@@ -2476,7 +2484,7 @@
 
 /* Use a library function for invert_limb, if available. */
 #define   mpn_invert_limb __MPN(invert_limb)
-__GMP_DECLSPEC mp_limb_t mpn_invert_limb __GMP_PROTO ((mp_limb_t)) ATTRIBUTE_CONST;
+mp_limb_t mpn_invert_limb __GMP_PROTO ((mp_limb_t)) ATTRIBUTE_CONST;
 #if ! defined (invert_limb) && HAVE_NATIVE_mpn_invert_limb
 #define invert_limb(invxl,xl)           \
   do {                                  \
@@ -2493,42 +2501,12 @@
   } while (0)
 #endif
 
-#define invert_pi1(dinv, d1, d0)				\
-  do {								\
-    mp_limb_t v, p, t1, t0, mask;				\
-    invert_limb (v, d1);					\
-    (dinv).inv21 = v;						\
-    p = d1 * v;							\
-    p += d0;							\
-    if (p < d0)							\
-      {								\
-	v--;							\
-	mask = -(p >= d1);					\
-	p -= d1;						\
-	v += mask;						\
-	p -= mask & d1;						\
-      }								\
-    umul_ppmm (t1, t0, d0, v);					\
-    p += t1;							\
-    if (p < t1)							\
-      {								\
-        v--;							\
-	if (UNLIKELY (p >= d1))					\
-	  {							\
-	    if (p > d1 || t0 >= d0)				\
-	      v--;						\
-	  }							\
-      }								\
-    (dinv).inv32 = v;						\
-  } while (0)
-
-
 #ifndef udiv_qrnnd_preinv
 #define udiv_qrnnd_preinv udiv_qrnnd_preinv3
 #endif
 
 /* Divide the two-limb number in (NH,,NL) by D, with DI being the largest
-   limb not larger than (2**(2*GMP_LIMB_BITS))/D - (2**GMP_LIMB_BITS).
+   limb not larger than (2**(2*BITS_PER_MP_LIMB))/D - (2**BITS_PER_MP_LIMB).
    If this would yield overflow, DI should be the largest possible number
    (i.e., only ones).  For correct operation, the most significant bit of D
    has to be set.  Put the quotient in Q and the remainder in R.  */
@@ -2585,8 +2563,8 @@
   do {									\
     mp_limb_t _n2, _n10, _nmask, _nadj, _q1;				\
     mp_limb_t _xh, _xl;							\
-    _n2 = ((nh) << (GMP_LIMB_BITS - (lgup))) + ((nl) >> 1 >> (l - 1));	\
-    _n10 = (nl) << (GMP_LIMB_BITS - (lgup));				\
+    _n2 = ((nh) << (BITS_PER_MP_LIMB - (lgup))) + ((nl) >> 1 >> (l - 1));\
+    _n10 = (nl) << (BITS_PER_MP_LIMB - (lgup));				\
     _nmask = LIMB_HIGHBIT_TO_MASK (_n10);				\
     _nadj = _n10 + (_nmask & (dnorm));					\
     umul_ppmm (_xh, _xl, di, _n2 - _nmask);				\
@@ -2659,42 +2637,9 @@
     (r) = _r;								\
   } while (0)
 
-/* Compute quotient the quotient and remainder for n / d. Requires d
-   >= B^2 / 2 and n < d B. di is the inverse
-
-     floor ((B^3 - 1) / (d0 + d1 B)) - B.
-*/
-#define udiv_qr_3by2(q, r1, r0, n2, n1, n0, d1, d0, dinv)		\
-  do {									\
-    mp_limb_t _q1, _q0, _r1, _r0, _t1, _t0, _mask;			\
-    umul_ppmm (_q1, _q0, (n2), (dinv));					\
-    add_ssaaaa (_q1, _q0, _q1, _q0, (n2), (n1));			\
-									\
-    /* Compute the two most significant limbs of n - q'd */		\
-    _r1 = (n1) - _q1 * (d1);						\
-    sub_ddmmss (_r1, _r0, _r1, (n0), (d1), (d0));			\
-    umul_ppmm (_t1, _t0, _q1, (d0));					\
-    sub_ddmmss (_r1, _r0, _r1, _r0, _t1, _t0);				\
-    _q1++;								\
-									\
-    /* Conditionally adjust q and the remainders */			\
-    _mask = - (mp_limb_t) (_r1 >= _q0);					\
-    _q1 += _mask;							\
-    add_ssaaaa (_r1, _r0, _r1, _r0, _mask & (d1), _mask & (d0));	\
-    if (UNLIKELY (_r1 >= (d1)))						\
-      {									\
-	if (_r1 > (d1) || _r0 >= (d0))					\
-	  {								\
-	    _q1++;							\
-	    sub_ddmmss (_r1, _r0, _r1, _r0, (d1), (d0));		\
-	  }								\
-      }									\
-    (q) = _q1; (r1) = _r1; (r0) = _r0;					\
-  } while (0)
-
 #ifndef mpn_preinv_divrem_1  /* if not done with cpuvec in a fat binary */
 #define   mpn_preinv_divrem_1 __MPN(preinv_divrem_1)
-__GMP_DECLSPEC mp_limb_t mpn_preinv_divrem_1 __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t, int));
+mp_limb_t mpn_preinv_divrem_1 __GMP_PROTO ((mp_ptr, mp_size_t, mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t, int));
 #endif
 
 
@@ -2728,7 +2673,7 @@
 
 #ifndef mpn_mod_34lsub1  /* if not done with cpuvec in a fat binary */
 #define   mpn_mod_34lsub1 __MPN(mod_34lsub1)
-__GMP_DECLSPEC mp_limb_t mpn_mod_34lsub1 __GMP_PROTO ((mp_srcptr, mp_size_t)) __GMP_ATTRIBUTE_PURE;
+mp_limb_t mpn_mod_34lsub1 __GMP_PROTO ((mp_srcptr, mp_size_t)) __GMP_ATTRIBUTE_PURE;
 #endif
 
 
@@ -2746,7 +2691,7 @@
 
 #ifndef mpn_divexact_1  /* if not done with cpuvec in a fat binary */
 #define mpn_divexact_1 __MPN(divexact_1)
-__GMP_DECLSPEC void    mpn_divexact_1 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t));
+void    mpn_divexact_1 __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_limb_t));
 #endif
 
 #define MPN_DIVREM_OR_DIVEXACT_1(dst, src, size, divisor)                     \
@@ -2762,12 +2707,12 @@
 
 #ifndef mpn_modexact_1c_odd  /* if not done with cpuvec in a fat binary */
 #define   mpn_modexact_1c_odd __MPN(modexact_1c_odd)
-__GMP_DECLSPEC mp_limb_t mpn_modexact_1c_odd __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t)) __GMP_ATTRIBUTE_PURE;
+mp_limb_t mpn_modexact_1c_odd __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t, mp_limb_t)) __GMP_ATTRIBUTE_PURE;
 #endif
 
 #if HAVE_NATIVE_mpn_modexact_1_odd
 #define   mpn_modexact_1_odd  __MPN(modexact_1_odd)
-__GMP_DECLSPEC mp_limb_t mpn_modexact_1_odd __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t)) __GMP_ATTRIBUTE_PURE;
+mp_limb_t mpn_modexact_1_odd __GMP_PROTO ((mp_srcptr, mp_size_t, mp_limb_t)) __GMP_ATTRIBUTE_PURE;
 #else
 #define mpn_modexact_1_odd(src,size,divisor) \
   mpn_modexact_1c_odd (src, size, divisor, CNST_LIMB(0))
@@ -2778,6 +2723,7 @@
    ? mpn_modexact_1_odd (src, size, divisor)				\
    : mpn_mod_1 (src, size, divisor))
 
+
 /* binvert_limb() sets inv to the multiplicative inverse of n modulo
    2^GMP_NUMB_BITS, ie. satisfying inv*n == 1 mod 2^GMP_NUMB_BITS.
    n must be odd (otherwise such an inverse doesn't exist).
@@ -2949,7 +2895,7 @@
    version 3.1 at least) doesn't seem to know how to generate rlwimi for
    anything other than bit-fields, so use "asm".  */
 #if defined (__GNUC__) && ! defined (NO_ASM)                    \
-  && HAVE_HOST_CPU_FAMILY_powerpc && GMP_LIMB_BITS == 32
+  && HAVE_HOST_CPU_FAMILY_powerpc && BITS_PER_MP_LIMB == 32
 #define BSWAP_LIMB(dst, src)						\
   do {									\
     mp_limb_t  __bswapl_src = (src);					\
@@ -2970,7 +2916,7 @@
    partial register stalls on P6 chips.  */
 #if defined (__GNUC__) && ! defined (NO_ASM)            \
   && HAVE_HOST_CPU_FAMILY_x86 && ! HAVE_HOST_CPU_i386   \
-  && GMP_LIMB_BITS == 32
+  && BITS_PER_MP_LIMB == 32
 #define BSWAP_LIMB(dst, src)						\
   do {									\
     __asm__ ("bswap %0" : "=r" (dst) : "0" (src));			\
@@ -2978,7 +2924,7 @@
 #endif
 
 #if defined (__GNUC__) && ! defined (NO_ASM)            \
-  && defined (__amd64__) && GMP_LIMB_BITS == 64
+  && defined (__amd64__) && BITS_PER_MP_LIMB == 64
 #define BSWAP_LIMB(dst, src)						\
   do {									\
     __asm__ ("bswap %q0" : "=r" (dst) : "0" (src));			\
@@ -2995,7 +2941,7 @@
 
 /* As per glibc. */
 #if defined (__GNUC__) && ! defined (NO_ASM)                    \
-  && HAVE_HOST_CPU_FAMILY_m68k && GMP_LIMB_BITS == 32
+  && HAVE_HOST_CPU_FAMILY_m68k && BITS_PER_MP_LIMB == 32
 #define BSWAP_LIMB(dst, src)						\
   do {									\
     mp_limb_t  __bswapl_src = (src);					\
@@ -3008,17 +2954,17 @@
 #endif
 
 #if ! defined (BSWAP_LIMB)
-#if GMP_LIMB_BITS == 8
+#if BITS_PER_MP_LIMB == 8
 #define BSWAP_LIMB(dst, src)            \
   do { (dst) = (src); } while (0)
 #endif
-#if GMP_LIMB_BITS == 16
+#if BITS_PER_MP_LIMB == 16
 #define BSWAP_LIMB(dst, src)                    \
   do {                                          \
     (dst) = ((src) << 8) + ((src) >> 8);        \
   } while (0)
 #endif
-#if GMP_LIMB_BITS == 32
+#if BITS_PER_MP_LIMB == 32
 #define BSWAP_LIMB(dst, src)    \
   do {                          \
     (dst) =                     \
@@ -3028,7 +2974,7 @@
       + ((src) >> 24);          \
   } while (0)
 #endif
-#if GMP_LIMB_BITS == 64
+#if BITS_PER_MP_LIMB == 64
 #define BSWAP_LIMB(dst, src)            \
   do {                                  \
     (dst) =                             \
@@ -3063,7 +3009,7 @@
 /* Apparently lwbrx might be slow on some PowerPC chips, so restrict it to
    those we know are fast.  */
 #if defined (__GNUC__) && ! defined (NO_ASM)                            \
-  && GMP_LIMB_BITS == 32 && HAVE_LIMB_BIG_ENDIAN                        \
+  && BITS_PER_MP_LIMB == 32 && HAVE_LIMB_BIG_ENDIAN                     \
   && (HAVE_HOST_CPU_powerpc604                                          \
       || HAVE_HOST_CPU_powerpc604e                                      \
       || HAVE_HOST_CPU_powerpc750                                       \
@@ -3088,7 +3034,7 @@
 /* On the same basis that lwbrx might be slow, restrict stwbrx to those we
    know are fast.  FIXME: Is this necessary?  */
 #if defined (__GNUC__) && ! defined (NO_ASM)                            \
-  && GMP_LIMB_BITS == 32 && HAVE_LIMB_BIG_ENDIAN                        \
+  && BITS_PER_MP_LIMB == 32 && HAVE_LIMB_BIG_ENDIAN                     \
   && (HAVE_HOST_CPU_powerpc604                                          \
       || HAVE_HOST_CPU_powerpc604e                                      \
       || HAVE_HOST_CPU_powerpc750                                       \
@@ -3149,7 +3095,7 @@
 /* No processor claiming to be SPARC v9 compliant seems to
    implement the POPC instruction.  Disable pattern for now.  */
 #if 0
-#if defined __GNUC__ && defined __sparc_v9__ && GMP_LIMB_BITS == 64
+#if defined __GNUC__ && defined __sparc_v9__ && BITS_PER_MP_LIMB == 64
 #define popc_limb(result, input)					\
   do {									\
     DItype __res;							\
@@ -3263,7 +3209,7 @@
 
 typedef mp_limb_t UWtype;
 typedef unsigned int UHWtype;
-#define W_TYPE_SIZE GMP_LIMB_BITS
+#define W_TYPE_SIZE BITS_PER_MP_LIMB
 
 /* Define ieee_double_extract and _GMP_IEEE_FLOATS.
 
@@ -3330,10 +3276,10 @@
    We assume doubles have 53 mantissa bits.  */
 #define LIMBS_PER_DOUBLE ((53 + GMP_NUMB_BITS - 2) / GMP_NUMB_BITS + 1)
 
-__GMP_DECLSPEC int __gmp_extract_double __GMP_PROTO ((mp_ptr, double));
+int __gmp_extract_double __GMP_PROTO ((mp_ptr, double));
 
 #define mpn_get_d __gmpn_get_d
-__GMP_DECLSPEC double mpn_get_d __GMP_PROTO ((mp_srcptr, mp_size_t, mp_size_t, long)) __GMP_ATTRIBUTE_PURE;
+double mpn_get_d __GMP_PROTO ((mp_srcptr, mp_size_t, mp_size_t, long)) __GMP_ATTRIBUTE_PURE;
 
 
 /* DOUBLE_NAN_INF_ACTION executes code a_nan if x is a NaN, or executes
@@ -3402,12 +3348,12 @@
 #endif
 
 
-__GMP_DECLSPEC extern int __gmp_junk;
-__GMP_DECLSPEC extern const int __gmp_0;
-__GMP_DECLSPEC void __gmp_exception __GMP_PROTO ((int)) ATTRIBUTE_NORETURN;
-__GMP_DECLSPEC void __gmp_divide_by_zero __GMP_PROTO ((void)) ATTRIBUTE_NORETURN;
-__GMP_DECLSPEC void __gmp_sqrt_of_negative __GMP_PROTO ((void)) ATTRIBUTE_NORETURN;
-__GMP_DECLSPEC void __gmp_invalid_operation __GMP_PROTO ((void)) ATTRIBUTE_NORETURN;
+extern int __gmp_junk;
+extern const int __gmp_0;
+void __gmp_exception __GMP_PROTO ((int)) ATTRIBUTE_NORETURN;
+void __gmp_divide_by_zero __GMP_PROTO ((void)) ATTRIBUTE_NORETURN;
+void __gmp_sqrt_of_negative __GMP_PROTO ((void)) ATTRIBUTE_NORETURN;
+void __gmp_invalid_operation __GMP_PROTO ((void)) ATTRIBUTE_NORETURN;
 #define GMP_ERROR(code)   __gmp_exception (code)
 #define DIVIDE_BY_ZERO    __gmp_divide_by_zero ()
 #define SQRT_OF_NEGATIVE  __gmp_sqrt_of_negative ()
@@ -3605,11 +3551,11 @@
 
 /* Matrix multiplication */
 #define   mpn_matrix22_mul __MPN(matrix22_mul)
-__GMP_DECLSPEC void      mpn_matrix22_mul __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_srcptr, mp_srcptr, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_matrix22_mul __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_srcptr, mp_srcptr, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_matrix22_mul_strassen __MPN(matrix22_mul_strassen)
-__GMP_DECLSPEC void      mpn_matrix22_mul_strassen __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_srcptr, mp_srcptr, mp_srcptr, mp_size_t, mp_ptr));
+void      mpn_matrix22_mul_strassen __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_srcptr, mp_srcptr, mp_srcptr, mp_size_t, mp_ptr));
 #define   mpn_matrix22_mul_itch __MPN(matrix22_mul_itch)
-__GMP_DECLSPEC mp_size_t mpn_matrix22_mul_itch __GMP_PROTO ((mp_size_t, mp_size_t));
+mp_size_t mpn_matrix22_mul_itch __GMP_PROTO ((mp_size_t, mp_size_t));
 
 #ifndef MATRIX22_STRASSEN_THRESHOLD
 #define MATRIX22_STRASSEN_THRESHOLD 30
@@ -3647,13 +3593,13 @@
 };
 
 #define mpn_hgcd2 __MPN (hgcd2)
-__GMP_DECLSPEC int mpn_hgcd2 __GMP_PROTO ((mp_limb_t, mp_limb_t, mp_limb_t, mp_limb_t,	struct hgcd_matrix1 *));
+int mpn_hgcd2 __GMP_PROTO ((mp_limb_t, mp_limb_t, mp_limb_t, mp_limb_t,	struct hgcd_matrix1 *));
 
 #define mpn_hgcd_mul_matrix1_vector __MPN (hgcd_mul_matrix1_vector)
-__GMP_DECLSPEC mp_size_t mpn_hgcd_mul_matrix1_vector __GMP_PROTO ((const struct hgcd_matrix1 *, mp_ptr, mp_srcptr, mp_ptr, mp_size_t));
+mp_size_t mpn_hgcd_mul_matrix1_vector __GMP_PROTO ((const struct hgcd_matrix1 *, mp_ptr, mp_srcptr, mp_ptr, mp_size_t));
 
 #define mpn_hgcd_mul_matrix1_inverse_vector __MPN (hgcd_mul_matrix1_inverse_vector)
-__GMP_DECLSPEC mp_size_t mpn_hgcd_mul_matrix1_inverse_vector __GMP_PROTO ((const struct hgcd_matrix1 *, mp_ptr, mp_srcptr, mp_ptr, mp_size_t));
+mp_size_t mpn_hgcd_mul_matrix1_inverse_vector __GMP_PROTO ((const struct hgcd_matrix1 *, mp_ptr, mp_srcptr, mp_ptr, mp_size_t));
 
 struct hgcd_matrix
 {
@@ -3665,43 +3611,43 @@
 #define MPN_HGCD_MATRIX_INIT_ITCH(n) (4 * ((n+1)/2 + 1))
 
 #define mpn_hgcd_matrix_init __MPN (hgcd_matrix_init)
-__GMP_DECLSPEC void mpn_hgcd_matrix_init __GMP_PROTO ((struct hgcd_matrix *, mp_size_t, mp_ptr));
+void mpn_hgcd_matrix_init __GMP_PROTO ((struct hgcd_matrix *, mp_size_t, mp_ptr));
 
 #define mpn_hgcd_matrix_mul __MPN (hgcd_matrix_mul)
-__GMP_DECLSPEC void mpn_hgcd_matrix_mul __GMP_PROTO ((struct hgcd_matrix *, const struct hgcd_matrix *, mp_ptr));
+void mpn_hgcd_matrix_mul __GMP_PROTO ((struct hgcd_matrix *, const struct hgcd_matrix *, mp_ptr));
 
 #define mpn_hgcd_matrix_adjust __MPN (hgcd_matrix_adjust)
-__GMP_DECLSPEC mp_size_t mpn_hgcd_matrix_adjust __GMP_PROTO ((struct hgcd_matrix *, mp_size_t, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
+mp_size_t mpn_hgcd_matrix_adjust __GMP_PROTO ((struct hgcd_matrix *, mp_size_t, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
 
 #define mpn_hgcd_itch __MPN (hgcd_itch)
-__GMP_DECLSPEC mp_size_t mpn_hgcd_itch __GMP_PROTO ((mp_size_t));
+mp_size_t mpn_hgcd_itch __GMP_PROTO ((mp_size_t));
 
 #define mpn_hgcd __MPN (hgcd)
-__GMP_DECLSPEC mp_size_t mpn_hgcd __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, struct hgcd_matrix *, mp_ptr));
+mp_size_t mpn_hgcd __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, struct hgcd_matrix *, mp_ptr));
 
 #define MPN_HGCD_LEHMER_ITCH(n) (n)
 
 #define mpn_hgcd_lehmer __MPN (hgcd_lehmer)
-__GMP_DECLSPEC mp_size_t mpn_hgcd_lehmer __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, struct hgcd_matrix *, mp_ptr));
+mp_size_t mpn_hgcd_lehmer __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, struct hgcd_matrix *, mp_ptr));
 
 /* Needs storage for the quotient */
 #define MPN_GCD_SUBDIV_STEP_ITCH(n) (n)
 
 #define mpn_gcd_subdiv_step __MPN(gcd_subdiv_step)
-__GMP_DECLSPEC mp_size_t mpn_gcd_subdiv_step __GMP_PROTO ((mp_ptr, mp_size_t *, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
+mp_size_t mpn_gcd_subdiv_step __GMP_PROTO ((mp_ptr, mp_size_t *, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
 
 #define MPN_GCD_LEHMER_N_ITCH(n) (n)
 
 #define mpn_gcd_lehmer_n __MPN(gcd_lehmer_n)
-__GMP_DECLSPEC mp_size_t mpn_gcd_lehmer_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
+mp_size_t mpn_gcd_lehmer_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
 
 #define mpn_gcdext_subdiv_step __MPN(gcdext_subdiv_step)
-__GMP_DECLSPEC mp_size_t mpn_gcdext_subdiv_step __GMP_PROTO ((mp_ptr, mp_size_t *, mp_ptr, mp_size_t *, mp_ptr, mp_ptr, mp_size_t, mp_ptr, mp_ptr, mp_size_t *, mp_ptr, mp_ptr));
+mp_size_t mpn_gcdext_subdiv_step __GMP_PROTO ((mp_ptr, mp_size_t *, mp_ptr, mp_size_t *, mp_ptr, mp_ptr, mp_size_t, mp_ptr, mp_ptr, mp_size_t *, mp_ptr, mp_ptr));
 
 #define MPN_GCDEXT_LEHMER_N_ITCH(n) (4*(n) + 3)
 
 #define mpn_gcdext_lehmer_n __MPN(gcdext_lehmer_n)
-__GMP_DECLSPEC mp_size_t mpn_gcdext_lehmer_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t *, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
+mp_size_t mpn_gcdext_lehmer_n __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t *, mp_ptr, mp_ptr, mp_size_t, mp_ptr));
 
 /* 4*(an + 1) + 4*(bn + 1) + an */
 #define MPN_GCDEXT_LEHMER_ITCH(an, bn) (5*(an) + 4*(bn) + 8)
@@ -3734,11 +3680,11 @@
 #define mpn_dc_get_str_itch(n) ((n) + GMP_LIMB_BITS)
 
 #define   mpn_dc_set_str __MPN(dc_set_str)
-__GMP_DECLSPEC mp_size_t mpn_dc_set_str __GMP_PROTO ((mp_ptr, const unsigned char *, size_t, const powers_t *, mp_ptr));
+mp_size_t mpn_dc_set_str __GMP_PROTO ((mp_ptr, const unsigned char *, size_t, const powers_t *, mp_ptr));
 #define   mpn_bc_set_str __MPN(bc_set_str)
-__GMP_DECLSPEC mp_size_t mpn_bc_set_str __GMP_PROTO ((mp_ptr, const unsigned char *, size_t, int));
+mp_size_t mpn_bc_set_str __GMP_PROTO ((mp_ptr, const unsigned char *, size_t, int));
 #define   mpn_set_str_compute_powtab __MPN(set_str_compute_powtab)
-__GMP_DECLSPEC void      mpn_set_str_compute_powtab __GMP_PROTO ((powers_t *, mp_ptr, mp_size_t, int));
+void      mpn_set_str_compute_powtab __GMP_PROTO ((powers_t *, mp_ptr, mp_size_t, int));
 
 
 /* __GMPF_BITS_TO_PREC applies a minimum 53 bits, rounds upwards to a whole
@@ -3750,7 +3696,7 @@
 #define __GMPF_PREC_TO_BITS(n) \
   ((unsigned long) (n) * GMP_NUMB_BITS - GMP_NUMB_BITS)
 
-__GMP_DECLSPEC extern mp_size_t __gmp_default_fp_limb_precision;
+extern mp_size_t __gmp_default_fp_limb_precision;
 
 
 /* Set n to the number of significant digits an mpf of the given _mp_prec
@@ -3762,7 +3708,7 @@
    further +1 is because the limbs usually won't fall on digit boundaries.
 
    FIXME: If base is a power of 2 and the bits per digit divides
-   GMP_LIMB_BITS then the +2 is unnecessary.  This happens always for
+   BITS_PER_MP_LIMB then the +2 is unnecessary.  This happens always for
    base==2, and in base==16 with the current 32 or 64 bit limb sizes. */
 
 #define MPF_SIGNIFICANT_DIGITS(n, base, prec)                           \
@@ -3828,10 +3774,10 @@
 
 #if _GMP_H_HAVE_VA_LIST
 
-__GMP_DECLSPEC typedef int (*doprnt_format_t) __GMP_PROTO ((void *, const char *, va_list));
-__GMP_DECLSPEC typedef int (*doprnt_memory_t) __GMP_PROTO ((void *, const char *, size_t));
-__GMP_DECLSPEC typedef int (*doprnt_reps_t)   __GMP_PROTO ((void *, int, int));
-__GMP_DECLSPEC typedef int (*doprnt_final_t)  __GMP_PROTO ((void *));
+typedef int (*doprnt_format_t) __GMP_PROTO ((void *, const char *, va_list));
+typedef int (*doprnt_memory_t) __GMP_PROTO ((void *, const char *, size_t));
+typedef int (*doprnt_reps_t)   __GMP_PROTO ((void *, int, int));
+typedef int (*doprnt_final_t)  __GMP_PROTO ((void *));
 
 struct doprnt_funs_t {
   doprnt_format_t  format;
@@ -3937,7 +3883,7 @@
 #define __gmp_doprnt_mpf __gmp_doprnt_mpf2
 __GMP_DECLSPEC int __gmp_doprnt_mpf __GMP_PROTO ((const struct doprnt_funs_t *, void *, const struct doprnt_params_t *, const char *, mpf_srcptr));
 
-__GMP_DECLSPEC int __gmp_replacement_vsnprintf __GMP_PROTO ((char *, size_t, const char *, va_list));
+int __gmp_replacement_vsnprintf __GMP_PROTO ((char *, size_t, const char *, va_list));
 #endif /* _GMP_H_HAVE_VA_LIST */
 
 
@@ -3956,7 +3902,8 @@
 extern const struct gmp_doscan_funs_t  __gmp_sscanf_funs;
 
 #if _GMP_H_HAVE_VA_LIST
-__GMP_DECLSPEC int __gmp_doscan __GMP_PROTO ((const struct gmp_doscan_funs_t *, void *, const char *, va_list));
+int __gmp_doscan __GMP_PROTO ((const struct gmp_doscan_funs_t *, void *,
+                          const char *, va_list));
 #endif
 
 
@@ -4012,7 +3959,7 @@
    difference to the gmp code, since hopefully we arrange calls so there's
    no great need for the compiler to move things around.  */
 
-#if WANT_FAT_BINARY && (HAVE_HOST_CPU_FAMILY_x86 || HAVE_HOST_CPU_FAMILY_x86_64)
+#if WANT_FAT_BINARY && HAVE_HOST_CPU_FAMILY_x86
 /* NOTE: The function pointers in this struct are also in CPUVEC_FUNCS_LIST
    in mpn/x86/x86-defs.m4.  Be sure to update that when changing here.  */
 struct cpuvec_t {
@@ -4037,15 +3984,15 @@
   DECL_sub_n           ((*sub_n));
   DECL_submul_1        ((*submul_1));
   int                  initialized;
-  mp_size_t            mul_toom22_threshold;
-  mp_size_t            mul_toom33_threshold;
-  mp_size_t            sqr_toom2_threshold;
+  mp_size_t            mul_karatsuba_threshold;
+  mp_size_t            mul_toom3_threshold;
+  mp_size_t            sqr_karatsuba_threshold;
   mp_size_t            sqr_toom3_threshold;
 };
 __GMP_DECLSPEC extern struct cpuvec_t __gmpn_cpuvec;
 #endif /* x86 fat binary */
 
-__GMP_DECLSPEC void __gmpn_cpuvec_init __GMP_PROTO ((void));
+void __gmpn_cpuvec_init __GMP_PROTO ((void));
 
 /* Get a threshold "field" from __gmpn_cpuvec, running __gmpn_cpuvec_init()
    if that hasn't yet been done (to establish the right values).  */
@@ -4104,13 +4051,13 @@
    on #ifdef in the .c files.  For some this is not so (the defaults are
    instead established above), but all are done this way for consistency. */
 
-#undef  MUL_TOOM22_THRESHOLD
-#define MUL_TOOM22_THRESHOLD         mul_toom22_threshold
-extern mp_size_t                     mul_toom22_threshold;
-
-#undef  MUL_TOOM33_THRESHOLD
-#define MUL_TOOM33_THRESHOLD         mul_toom33_threshold
-extern mp_size_t                     mul_toom33_threshold;
+#undef  MUL_KARATSUBA_THRESHOLD
+#define MUL_KARATSUBA_THRESHOLD      mul_karatsuba_threshold
+extern mp_size_t                     mul_karatsuba_threshold;
+
+#undef  MUL_TOOM3_THRESHOLD
+#define MUL_TOOM3_THRESHOLD          mul_toom3_threshold
+extern mp_size_t                     mul_toom3_threshold;
 
 #undef  MUL_TOOM44_THRESHOLD
 #define MUL_TOOM44_THRESHOLD         mul_toom44_threshold
@@ -4136,12 +4083,12 @@
 #endif
 
 #if TUNE_PROGRAM_BUILD_SQR
-#undef  SQR_TOOM2_THRESHOLD
-#define SQR_TOOM2_THRESHOLD          SQR_TOOM2_MAX_GENERIC
+#undef  SQR_KARATSUBA_THRESHOLD
+#define SQR_KARATSUBA_THRESHOLD      SQR_KARATSUBA_MAX_GENERIC
 #else
-#undef  SQR_TOOM2_THRESHOLD
-#define SQR_TOOM2_THRESHOLD          sqr_toom2_threshold
-extern mp_size_t                     sqr_toom2_threshold;
+#undef  SQR_KARATSUBA_THRESHOLD
+#define SQR_KARATSUBA_THRESHOLD      sqr_karatsuba_threshold
+extern mp_size_t                     sqr_karatsuba_threshold;
 #endif
 
 #undef  SQR_TOOM3_THRESHOLD
@@ -4163,17 +4110,17 @@
 #undef  SQR_FFT_TABLE
 #define SQR_FFT_TABLE                { 0 }
 
-#undef  MULLO_BASECASE_THRESHOLD
-#define MULLO_BASECASE_THRESHOLD     mullo_basecase_threshold
-extern mp_size_t                     mullo_basecase_threshold;
-
-#undef  MULLO_DC_THRESHOLD
-#define MULLO_DC_THRESHOLD           mullo_dc_threshold
-extern mp_size_t                     mullo_dc_threshold;
-
-#undef  MULLO_MUL_N_THRESHOLD
-#define MULLO_MUL_N_THRESHOLD        mullo_mul_n_threshold
-extern mp_size_t                     mullo_mul_n_threshold;
+#undef  MULLOW_BASECASE_THRESHOLD
+#define MULLOW_BASECASE_THRESHOLD    mullow_basecase_threshold
+extern mp_size_t                     mullow_basecase_threshold;
+
+#undef  MULLOW_DC_THRESHOLD
+#define MULLOW_DC_THRESHOLD          mullow_dc_threshold
+extern mp_size_t                     mullow_dc_threshold;
+
+#undef  MULLOW_MUL_N_THRESHOLD
+#define MULLOW_MUL_N_THRESHOLD       mullow_mul_n_threshold
+extern mp_size_t                     mullow_mul_n_threshold;
 
 
 #if ! UDIV_PREINV_ALWAYS
@@ -4182,53 +4129,9 @@
 extern mp_size_t                     div_sb_preinv_threshold;
 #endif
 
-#undef  DC_DIV_QR_THRESHOLD
-#define DC_DIV_QR_THRESHOLD          dc_div_qr_threshold
-extern mp_size_t                     dc_div_qr_threshold;
-
-#undef  DC_DIVAPPR_Q_THRESHOLD
-#define DC_DIVAPPR_Q_THRESHOLD       dc_divappr_q_threshold
-extern mp_size_t                     dc_divappr_q_threshold;
-
-#undef  DC_DIV_Q_THRESHOLD
-#define DC_DIV_Q_THRESHOLD           dc_div_q_threshold
-extern mp_size_t                     dc_div_q_threshold;
-
-#undef  DC_BDIV_Q_THRESHOLD
-#define DC_BDIV_Q_THRESHOLD          dc_bdiv_q_threshold
-extern mp_size_t                     dc_bdiv_q_threshold;
-
-#undef  DC_BDIV_QR_THRESHOLD
-#define DC_BDIV_QR_THRESHOLD         dc_bdiv_qr_threshold
-extern mp_size_t                     dc_bdiv_qr_threshold;
-
-#undef  INV_MULMOD_BNM1_THRESHOLD
-#define INV_MULMOD_BNM1_THRESHOLD    inv_mulmod_bnm1_threshold
-extern mp_size_t                     inv_mulmod_bnm1_threshold;
-
-#undef  INV_NEWTON_THRESHOLD
-#define INV_NEWTON_THRESHOLD         inv_newton_threshold
-extern mp_size_t                     inv_newton_threshold;
-
-#undef  INV_APPR_THRESHOLD
-#define INV_APPR_THRESHOLD           inv_appr_threshold
-extern mp_size_t                     inv_appr_threshold;
-
-#undef  BINV_NEWTON_THRESHOLD
-#define BINV_NEWTON_THRESHOLD        binv_newton_threshold
-extern mp_size_t                     binv_newton_threshold;
-
-#undef  REDC_1_TO_REDC_2_THRESHOLD
-#define REDC_1_TO_REDC_2_THRESHOLD   redc_1_to_redc_2_threshold
-extern mp_size_t                     redc_1_to_redc_2_threshold;
-
-#undef  REDC_2_TO_REDC_N_THRESHOLD
-#define REDC_2_TO_REDC_N_THRESHOLD   redc_2_to_redc_n_threshold
-extern mp_size_t                     redc_2_to_redc_n_threshold;
-
-#undef  REDC_1_TO_REDC_N_THRESHOLD
-#define REDC_1_TO_REDC_N_THRESHOLD   redc_1_to_redc_n_threshold
-extern mp_size_t                     redc_1_to_redc_n_threshold;
+#undef  DIV_DC_THRESHOLD
+#define DIV_DC_THRESHOLD             div_dc_threshold
+extern mp_size_t                     div_dc_threshold;
 
 #undef  POWM_THRESHOLD
 #define POWM_THRESHOLD               powm_threshold
@@ -4292,10 +4195,6 @@
 extern mp_size_t                     divrem_2_threshold;
 #endif
 
-#undef MULMOD_BNM1_THRESHOLD
-#define MULMOD_BNM1_THRESHOLD        mulmod_bnm1_threshold
-extern mp_size_t                     mulmod_bnm1_threshold;
-
 #undef  GET_STR_DC_THRESHOLD
 #define GET_STR_DC_THRESHOLD         get_str_dc_threshold
 extern mp_size_t                     get_str_dc_threshold;
@@ -4321,98 +4220,32 @@
 extern mp_size_t  mpn_fft_table[2][MPN_FFT_TABLE_SIZE];
 
 /* Sizes the tune program tests up to, used in a couple of recompilations. */
-#undef MUL_TOOM22_THRESHOLD_LIMIT
-#undef MUL_TOOM33_THRESHOLD_LIMIT
-#undef MULLO_BASECASE_THRESHOLD_LIMIT
+#undef MUL_KARATSUBA_THRESHOLD_LIMIT
+#undef MUL_TOOM3_THRESHOLD_LIMIT
+#undef MULLOW_BASECASE_THRESHOLD_LIMIT
 #undef SQR_TOOM3_THRESHOLD_LIMIT
-#define SQR_TOOM2_MAX_GENERIC           200
-#define MUL_TOOM22_THRESHOLD_LIMIT      700
-#define MUL_TOOM33_THRESHOLD_LIMIT      700
+#define SQR_KARATSUBA_MAX_GENERIC       200
+#define MUL_KARATSUBA_THRESHOLD_LIMIT   700
+#define MUL_TOOM3_THRESHOLD_LIMIT       700
 #define SQR_TOOM3_THRESHOLD_LIMIT       400
 #define MUL_TOOM44_THRESHOLD_LIMIT     1000
 #define SQR_TOOM4_THRESHOLD_LIMIT      1000
-#define MULLO_BASECASE_THRESHOLD_LIMIT  200
+#define MULLOW_BASECASE_THRESHOLD_LIMIT 200
 #define GET_STR_THRESHOLD_LIMIT         150
 
+/* "thresh" will normally be a variable when tuning, so use the cached
+   result.  This helps mpn_sb_divrem_mn for instance.  */
+#undef  CACHED_ABOVE_THRESHOLD
+#define CACHED_ABOVE_THRESHOLD(cache, thresh)  (cache)
+#undef  CACHED_BELOW_THRESHOLD
+#define CACHED_BELOW_THRESHOLD(cache, thresh)  (cache)
+
 #endif /* TUNE_PROGRAM_BUILD */
 
 #if defined (__cplusplus)
 }
 #endif
 
-/* FIXME: Make these itch functions less conservative.  Also consider making
-   them dependent on just 'an', and compute the allocation directly from 'an'
-   instead of via n.  */
-
-/* toom22/toom2: Scratch need is 2*(an + k), k is the recursion depth.
-   k is ths smallest k such that
-     ceil(an/2^k) < MUL_KARATSUBA_THRESHOLD.
-   which implies that
-     k = bitsize of floor ((an-1)/(MUL_KARATSUBA_THRESHOLD-1))
-       = 1 + floor (log_2 (floor ((an-1)/(MUL_KARATSUBA_THRESHOLD-1))))
-*/
-#define mpn_toom22_mul_itch(an, bn) \
-  (2 * ((an) + GMP_NUMB_BITS))
-#define mpn_toom2_sqr_itch(an) \
-  (2 * ((an) + GMP_NUMB_BITS))
-
-/* Can probably be trimmed to 2 an + O(log an). */
-#define mpn_toom33_mul_itch(an, bn) \
-  ((5 * (an) >> 1) + GMP_NUMB_BITS)
-#define mpn_toom3_sqr_itch(an) \
-  ((5 * (an) >> 1) + GMP_NUMB_BITS)
-
-#define mpn_toom44_mul_itch(an, bn) \
-  (3 * (an) + GMP_NUMB_BITS)
-#define mpn_toom4_sqr_itch(an) \
-  (3 * (an) + GMP_NUMB_BITS)
-
-static inline mp_size_t
-mpn_toom32_mul_itch (mp_size_t an, mp_size_t bn)
-{
-  mp_size_t n = 1 + (2 * an >= 3 * bn ? (an - 1) / (size_t) 3 : (bn - 1) >> 1);
-  mp_size_t itch = 4 * n + 2;
-  if (ABOVE_THRESHOLD (n, MUL_TOOM22_THRESHOLD))
-    itch += mpn_toom22_mul_itch (n, n);
-
-  return itch;
-}
-
-static inline mp_size_t
-mpn_toom42_mul_itch (mp_size_t an, mp_size_t bn)
-{
-  mp_size_t n = an >= 2 * bn ? (an + 3) >> 2 : (bn + 1) >> 1;
-  return 6 * n + 3;
-}
-
-static inline mp_size_t
-mpn_toom43_mul_itch (mp_size_t an, mp_size_t bn)
-{
-  mp_size_t n = 1 + (3 * an >= 4 * bn ? (an - 1) >> 2 : (bn - 1) / (size_t) 3);
-
-  return 6*n + 4;
-}
-
-static inline mp_size_t
-mpn_toom52_mul_itch (mp_size_t an, mp_size_t bn)
-{
-  mp_size_t n = 1 + (2 * an >= 5 * bn ? (an - 1) / (size_t) 5 : (bn - 1) >> 1);
-  return 6*n + 4;
-}
-
-static inline mp_size_t
-mpn_toom53_mul_itch (mp_size_t an, mp_size_t bn)
-{
-  mp_size_t n = 1 + (3 * an >= 5 * bn ? (an - 1) / (size_t) 5 : (bn - 1) / (size_t) 3);
-  return 10 * n + 10;
-}
-
-static inline mp_size_t
-mpn_toom62_mul_itch (mp_size_t an, mp_size_t bn)
-{
-  mp_size_t n = 1 + (an >= 3 * bn ? (an - 1) / (size_t) 6 : (bn - 1) >> 1);
-  return 10 * n + 10;
-}
 
 #ifdef __cplusplus
 
--- 1/longlong.h
+++ 2/longlong.h
@@ -19,25 +19,18 @@
 /* You have to define the following before including this file:
 
    UWtype -- An unsigned type, default type for operations (typically a "word")
-   UHWtype -- An unsigned type, at least half the size of UWtype
+   UHWtype -- An unsigned type, at least half the size of UWtype.
    UDWtype -- An unsigned type, at least twice as large a UWtype
    W_TYPE_SIZE -- size in bits of UWtype
 
-   SItype, USItype -- Signed and unsigned 32 bit types
-   DItype, UDItype -- Signed and unsigned 64 bit types
+   SItype, USItype -- Signed and unsigned 32 bit types.
+   DItype, UDItype -- Signed and unsigned 64 bit types.
 
    On a 32 bit machine UWtype should typically be USItype;
    on a 64 bit machine, UWtype should typically be UDItype.
 
-   Optionally, define:
-
-   LONGLONG_STANDALONE -- Avoid code that needs machine-dependent support files
-   NO_ASM -- Disable inline asm
-
-
-   CAUTION!  Using this version of longlong.h outside of GMP is not safe.  You
-   need to include gmp.h and gmp-impl.h, or certain things might not work as
-   expected.
+   CAUTION!  Using this file outside of GMP is not safe.  You need to include
+   gmp.h and gmp-impl.h, or certain things might not work as expected.
 */
 
 #define __BITS4 (W_TYPE_SIZE / 4)
@@ -513,12 +506,6 @@
 #define UDIV_TIME 200
 #endif /* LONGLONG_STANDALONE */
 #endif
-#if defined (__ARM_ARCH_5__)
-/* This actually requires arm 5 */
-#define count_leading_zeros(count, x) \
-  __asm__ ("clz\t%0, %1" : "=r" (count) : "r" (x))
-#define COUNT_LEADING_ZEROS_0 32
-#endif
 #endif /* __arm__ */
 
 #if defined (__clipper__) && W_TYPE_SIZE == 32
--- 1/Makefile.am
+++ 2/Makefile.am
@@ -65,7 +65,6 @@
 #        4.3.0    8:0:5    5:0:1   4:14:1
 #        4.3.1    8:1:5    5:1:1   4:15:1	WRONG Really used same as 4.3.0
 #        4.3.2    8:2:5    5:2:1   4:16:1
-#        4.4.0    9:0:6    6:0:2   4:20:1	PRELIMINARY
 #
 # Starting at 3:0:0 is a slight abuse of the versioning system, but it
 # ensures we're past soname libgmp.so.2, which was used on Debian GNU/Linux
@@ -78,22 +77,22 @@
 # it's still good to get the shared library filename (like
 # libgmpxx.so.3.0.4) incrementing, to make it clear which GMP it's from.
 
-LIBGMP_LT_CURRENT =  9
-LIBGMP_LT_REVISION = 0
-LIBGMP_LT_AGE =      6
-
-LIBGMPXX_LT_CURRENT =  6
-LIBGMPXX_LT_REVISION = 0
-LIBGMPXX_LT_AGE =      2
+LIBGMP_LT_CURRENT =  8
+LIBGMP_LT_REVISION = 2
+LIBGMP_LT_AGE =      5
+
+LIBGMPXX_LT_CURRENT =  5
+LIBGMPXX_LT_REVISION = 2
+LIBGMPXX_LT_AGE =      1
 
 LIBMP_LT_CURRENT =  4
-LIBMP_LT_REVISION = 20
+LIBMP_LT_REVISION = 16
 LIBMP_LT_AGE =      1
 
 
 SUBDIRS = tests mpn mpz mpq mpf printf scanf cxx mpbsd demos tune doc
 
-EXTRA_DIST = configfsf.guess configfsf.sub .gdbinit INSTALL.autoconf
+EXTRA_DIST = macos configfsf.guess configfsf.sub .gdbinit INSTALL.autoconf
 
 if WANT_CXX
 GMPXX_HEADERS_OPTION = gmpxx.h
@@ -106,7 +105,7 @@
 #
 # $exec_prefix/include is not in the default include path for gcc built to
 # the same $prefix and $exec_prefix, which might mean gmp.h is not found,
-# but anyone knowledgeable enough to be playing with exec_prefix will be able
+# but anyone knowledgable enough to be playing with exec_prefix will be able
 # to address that.
 #
 includeexecdir = $(exec_prefix)/include
@@ -122,14 +121,14 @@
 INCLUDES=-D__GMP_WITHIN_GMP
 
 
-MPF_OBJECTS = mpf/init$U.lo mpf/init2$U.lo mpf/inits$U.lo mpf/set$U.lo	    \
-  mpf/set_ui$U.lo mpf/set_si$U.lo mpf/set_str$U.lo mpf/set_d$U.lo	    \
-  mpf/set_z$U.lo mpf/iset$U.lo mpf/iset_ui$U.lo mpf/iset_si$U.lo	    \
-  mpf/iset_str$U.lo mpf/iset_d$U.lo mpf/clear$U.lo mpf/clears$U.lo	    \
-  mpf/get_str$U.lo mpf/dump$U.lo mpf/size$U.lo mpf/eq$U.lo mpf/reldiff$U.lo \
-  mpf/sqrt$U.lo mpf/random2$U.lo mpf/inp_str$U.lo mpf/out_str$U.lo	    \
-  mpf/add$U.lo mpf/add_ui$U.lo mpf/sub$U.lo mpf/sub_ui$U.lo mpf/ui_sub$U.lo \
-  mpf/mul$U.lo mpf/mul_ui$U.lo mpf/div$U.lo mpf/div_ui$U.lo		    \
+MPF_OBJECTS = mpf/init$U.lo mpf/init2$U.lo mpf/set$U.lo mpf/set_ui$U.lo	    \
+  mpf/set_si$U.lo mpf/set_str$U.lo mpf/set_d$U.lo mpf/set_z$U.lo	    \
+  mpf/iset$U.lo mpf/iset_ui$U.lo mpf/iset_si$U.lo mpf/iset_str$U.lo	    \
+  mpf/iset_d$U.lo mpf/clear$U.lo mpf/get_str$U.lo mpf/dump$U.lo		    \
+  mpf/size$U.lo mpf/eq$U.lo mpf/reldiff$U.lo mpf/sqrt$U.lo mpf/random2$U.lo \
+  mpf/inp_str$U.lo mpf/out_str$U.lo mpf/add$U.lo mpf/add_ui$U.lo	    \
+  mpf/sub$U.lo mpf/sub_ui$U.lo mpf/ui_sub$U.lo mpf/mul$U.lo mpf/mul_ui$U.lo \
+  mpf/div$U.lo mpf/div_ui$U.lo						    \
   mpf/cmp$U.lo mpf/cmp_d$U.lo mpf/cmp_ui$U.lo mpf/cmp_si$U.lo		    \
   mpf/mul_2exp$U.lo mpf/div_2exp$U.lo mpf/abs$U.lo mpf/neg$U.lo		    \
   mpf/set_q$U.lo mpf/get_d$U.lo mpf/get_d_2exp$U.lo mpf/set_dfl_prec$U.lo   \
@@ -149,7 +148,7 @@
   mpz/cdiv_qr$U.lo mpz/cdiv_qr_ui$U.lo					\
   mpz/cdiv_r$U.lo mpz/cdiv_r_ui$U.lo mpz/cdiv_ui$U.lo			\
   mpz/cfdiv_q_2exp$U.lo mpz/cfdiv_r_2exp$U.lo				\
-  mpz/clear$U.lo mpz/clears$U.lo mpz/clrbit$U.lo			\
+  mpz/clear$U.lo mpz/clrbit$U.lo					\
   mpz/cmp$U.lo mpz/cmp_d$U.lo mpz/cmp_si$U.lo mpz/cmp_ui$U.lo		\
   mpz/cmpabs$U.lo mpz/cmpabs_d$U.lo mpz/cmpabs_ui$U.lo			\
   mpz/com$U.lo mpz/combit$U.lo						\
@@ -165,8 +164,8 @@
   mpz/gcd_ui$U.lo mpz/gcdext$U.lo mpz/get_d$U.lo mpz/get_d_2exp$U.lo	\
   mpz/get_si$U.lo mpz/get_str$U.lo mpz/get_ui$U.lo mpz/getlimbn$U.lo	\
   mpz/hamdist$U.lo							\
-  mpz/import$U.lo mpz/init$U.lo mpz/init2$U.lo mpz/inits$U.lo 		\
-  mpz/inp_raw$U.lo mpz/inp_str$U.lo mpz/invert$U.lo			\
+  mpz/import$U.lo mpz/init$U.lo mpz/init2$U.lo mpz/inp_raw$U.lo		\
+  mpz/inp_str$U.lo mpz/invert$U.lo					\
   mpz/ior$U.lo mpz/iset$U.lo mpz/iset_d$U.lo mpz/iset_si$U.lo		\
   mpz/iset_str$U.lo mpz/iset_ui$U.lo mpz/jacobi$U.lo mpz/kronsz$U.lo	\
   mpz/kronuz$U.lo mpz/kronzs$U.lo mpz/kronzu$U.lo			\
@@ -191,10 +190,10 @@
   mpz/urandomm$U.lo mpz/xor$U.lo
 
 MPQ_OBJECTS = mpq/abs$U.lo mpq/aors$U.lo				\
-  mpq/canonicalize$U.lo mpq/clear$U.lo mpq/clears$U.lo			\
+  mpq/canonicalize$U.lo mpq/clear$U.lo					\
   mpq/cmp$U.lo mpq/cmp_si$U.lo mpq/cmp_ui$U.lo mpq/div$U.lo		\
   mpq/get_d$U.lo mpq/get_den$U.lo mpq/get_num$U.lo mpq/get_str$U.lo	\
-  mpq/init$U.lo mpq/inits$U.lo mpq/inp_str$U.lo mpq/inv$U.lo		\
+  mpq/init$U.lo mpq/inp_str$U.lo mpq/inv$U.lo				\
   mpq/md_2exp$U.lo mpq/mul$U.lo mpq/neg$U.lo mpq/out_str$U.lo		\
   mpq/set$U.lo mpq/set_den$U.lo mpq/set_num$U.lo			\
   mpq/set_si$U.lo mpq/set_str$U.lo mpq/set_ui$U.lo			\
@@ -251,8 +250,7 @@
   assert.c compat.c errno.c extract-dbl.c invalid.c memory.c		\
   mp_bpl.c mp_clz_tab.c mp_dv_tab.c mp_minv_tab.c mp_get_fns.c mp_set_fns.c \
   rand.c randclr.c randdef.c randiset.c randlc2s.c randlc2x.c randmt.c	\
-  randmts.c rands.c randsd.c randsdui.c randbui.c randmui.c version.c	\
-  nextprime.c
+  randmts.c rands.c randsd.c randsdui.c randbui.c randmui.c version.c
 EXTRA_libgmp_la_SOURCES = tal-debug.c tal-notreent.c tal-reent.c
 libgmp_la_DEPENDENCIES = @TAL_OBJECT@		\
   $(MPF_OBJECTS) $(MPZ_OBJECTS) $(MPQ_OBJECTS)	\
@@ -287,7 +285,7 @@
 endif
 BUILT_SOURCES += mp.h
 libmp_la_SOURCES = assert.c errno.c memory.c mp_bpl.c mp_clz_tab.c	\
-  mp_dv_tab.c mp_minv_tab.c mp_get_fns.c mp_set_fns.c nextprime.c
+  mp_dv_tab.c mp_minv_tab.c mp_get_fns.c mp_set_fns.c
 libmp_la_DEPENDENCIES = $(srcdir)/libmp.sym				\
   @TAL_OBJECT@ $(MPBSD_OBJECTS) $(MPN_OBJECTS) @mpn_objs_in_libmp@	\
   mpz/add$U.lo mpz/gcdext$U.lo mpz/invert$U.lo mpz/mul$U.lo		\
@@ -339,7 +337,7 @@
 EXTRA_DIST += dumbmp.c
 
 mpz/fac_ui.h: gen-fac_ui$(EXEEXT_FOR_BUILD)
-	./gen-fac_ui $(GMP_LIMB_BITS) $(GMP_NAIL_BITS) >mpz/fac_ui.h || (rm -f mpz/fac_ui.h; exit 1)
+	./gen-fac_ui $(BITS_PER_MP_LIMB) $(GMP_NAIL_BITS) >mpz/fac_ui.h || (rm -f mpz/fac_ui.h; exit 1)
 BUILT_SOURCES += mpz/fac_ui.h
 
 gen-fac_ui$(EXEEXT_FOR_BUILD): gen-fac_ui$(U_FOR_BUILD).c dumbmp.c
@@ -352,11 +350,11 @@
 
 
 fib_table.h: gen-fib$(EXEEXT_FOR_BUILD)
-	./gen-fib header $(GMP_LIMB_BITS) $(GMP_NAIL_BITS) >fib_table.h || (rm -f fib_table.h; exit 1)
+	./gen-fib header $(BITS_PER_MP_LIMB) $(GMP_NAIL_BITS) >fib_table.h || (rm -f fib_table.h; exit 1)
 BUILT_SOURCES += fib_table.h
 
 mpn/fib_table.c: gen-fib$(EXEEXT_FOR_BUILD)
-	./gen-fib table $(GMP_LIMB_BITS) $(GMP_NAIL_BITS) >mpn/fib_table.c || (rm -f mpn/fib_table.c; exit 1)
+	./gen-fib table $(BITS_PER_MP_LIMB) $(GMP_NAIL_BITS) >mpn/fib_table.c || (rm -f mpn/fib_table.c; exit 1)
 BUILT_SOURCES += mpn/fib_table.c
 
 gen-fib$(EXEEXT_FOR_BUILD): gen-fib$(U_FOR_BUILD).c dumbmp.c
@@ -369,11 +367,11 @@
 
 
 mp_bases.h: gen-bases$(EXEEXT_FOR_BUILD)
-	./gen-bases header $(GMP_LIMB_BITS) $(GMP_NAIL_BITS) >mp_bases.h || (rm -f mp_bases.h; exit 1)
+	./gen-bases header $(BITS_PER_MP_LIMB) $(GMP_NAIL_BITS) >mp_bases.h || (rm -f mp_bases.h; exit 1)
 BUILT_SOURCES += mp_bases.h
 
 mpn/mp_bases.c: gen-bases$(EXEEXT_FOR_BUILD)
-	./gen-bases table $(GMP_LIMB_BITS) $(GMP_NAIL_BITS) >mpn/mp_bases.c || (rm -f mpn/mp_bases.c; exit 1)
+	./gen-bases table $(BITS_PER_MP_LIMB) $(GMP_NAIL_BITS) >mpn/mp_bases.c || (rm -f mpn/mp_bases.c; exit 1)
 BUILT_SOURCES += mpn/mp_bases.c
 
 gen-bases$(EXEEXT_FOR_BUILD): gen-bases$(U_FOR_BUILD).c dumbmp.c
@@ -385,24 +383,8 @@
 	$(CPP_FOR_BUILD) `if test -f $(srcdir)/gen-bases.c; then echo $(srcdir)/gen-bases.c; else echo gen-bases.c; fi` | sed 's/^# \([0-9]\)/#line \1/' | $(ANSI2KNR) > gen-bases_.c || rm -f gen-bases_.c
 
 
-
-trialdivtab.h: gen-trialdivtab$(EXEEXT_FOR_BUILD)
-	./gen-trialdivtab $(GMP_LIMB_BITS) 8000 >trialdivtab.h || (rm -f trialdivtab.h; exit 1)
-BUILT_SOURCES += trialdivtab.h
-
-gen-trialdivtab$(EXEEXT_FOR_BUILD): gen-trialdivtab$(U_FOR_BUILD).c dumbmp.c
-	$(CC_FOR_BUILD) `test -f 'gen-trialdivtab$(U_FOR_BUILD).c' || echo '$(srcdir)/'`gen-trialdivtab$(U_FOR_BUILD).c -o gen-trialdivtab$(EXEEXT_FOR_BUILD) $(LIBM_FOR_BUILD)
-DISTCLEANFILES += gen-trialdivtab$(EXEEXT_FOR_BUILD)
-EXTRA_DIST += gen-trialdivtab.c
-
-gen-trialdivtab_.c: gen-trialdivtab.c $(ANSI2KNR)
-	$(CPP_FOR_BUILD) `if test -f $(srcdir)/gen-trialdivtab.c; then echo $(srcdir)/gen-trialdivtab.c; else echo gen-trialdivtab.c; fi` | sed 's/^# \([0-9]\)/#line \1/' | $(ANSI2KNR) > gen-trialdivtab_.c || rm -f gen-trialdivtab_.c
-
-
-
-
 mpn/perfsqr.h: gen-psqr$(EXEEXT_FOR_BUILD)
-	./gen-psqr $(GMP_LIMB_BITS) $(GMP_NAIL_BITS) >mpn/perfsqr.h || (rm -f mpn/perfsqr.h; exit 1)
+	./gen-psqr $(BITS_PER_MP_LIMB) $(GMP_NAIL_BITS) >mpn/perfsqr.h || (rm -f mpn/perfsqr.h; exit 1)
 BUILT_SOURCES += mpn/perfsqr.h
 
 gen-psqr$(EXEEXT_FOR_BUILD): gen-psqr$(U_FOR_BUILD).c dumbmp.c
@@ -424,6 +406,6 @@
 dist-hook:
 	-find $(distdir) \( -name CVS -type d \) -o -name "*~" -o -name ".#*" \
 		| xargs rm -rf
-#	grep -F $(VERSION) $(srcdir)/Makefile.am \
-#		| grep -q "^# *$(VERSION) *$(LIBGMP_LT_CURRENT):$(LIBGMP_LT_REVISION):$(LIBGMP_LT_AGE) *$(LIBGMPXX_LT_CURRENT):$(LIBGMPXX_LT_REVISION):$(LIBGMPXX_LT_AGE) *$(LIBMP_LT_CURRENT):$(LIBMP_LT_REVISION):$(LIBMP_LT_AGE)"
-#	test -z "`sed -n 's/^# *[0-9]*\.[0-9]*\.[0-9]* *\([0-9]*:[0-9]*:[0-9]*\) *\([0-9]*:[0-9]*:[0-9]*\) *\([0-9]*:[0-9]*:[0-9]*\).*/A\1\nB\2\nC\3/p' $(srcdir)/Makefile.am | grep -v 'A6:3:3\|B3:5:0\|C4:7:1' | sort | uniq -d`"
+	grep -F $(VERSION) $(srcdir)/Makefile.am \
+		| grep -q "^# *$(VERSION) *$(LIBGMP_LT_CURRENT):$(LIBGMP_LT_REVISION):$(LIBGMP_LT_AGE) *$(LIBGMPXX_LT_CURRENT):$(LIBGMPXX_LT_REVISION):$(LIBGMPXX_LT_AGE) *$(LIBMP_LT_CURRENT):$(LIBMP_LT_REVISION):$(LIBMP_LT_AGE)"
+	test -z "`sed -n 's/^# *[0-9]*\.[0-9]*\.[0-9]* *\([0-9]*:[0-9]*:[0-9]*\) *\([0-9]*:[0-9]*:[0-9]*\) *\([0-9]*:[0-9]*:[0-9]*\).*/A\1\nB\2\nC\3/p' $(srcdir)/Makefile.am | grep -v 'A6:3:3\|B3:5:0\|C4:7:1' | sort | uniq -d`"
--- 1/missing
+++ 2/missing
@@ -1,9 +1,9 @@
 #! /bin/sh
 # Common stub for a few missing GNU programs while installing.
 
-scriptversion=2009-11-15.01
+scriptversion=2003-09-02.23
 
-# Copyright (C) 1996, 1997, 1999, 2000, 2002, 2003
+# Copyright (C) 1996, 1997, 1999, 2000, 2002, 2003 
 #   Free Software Foundation, Inc.
 # Originally by Fran,cois Pinard <pinard@iro.umontreal.ca>, 1996.
 
--- 1/mp_bpl.c
+++ 2/mp_bpl.c
@@ -19,6 +19,6 @@
 #include "gmp.h"
 #include "gmp-impl.h"
 
-const int mp_bits_per_limb = GMP_LIMB_BITS;
+const int mp_bits_per_limb = BITS_PER_MP_LIMB;
 const int __gmp_0 = 0;
 int __gmp_junk;
--- 1/mpbsd/min.c
+++ 2/mpbsd/min.c
@@ -77,7 +77,7 @@
 
   ungetc (c, stdin);
 
-  dest_size = str_size / mp_bases[10].chars_per_limb + 1;
+  dest_size = str_size / __mp_bases[10].chars_per_limb + 1;
   if (dest->_mp_alloc < dest_size)
     _mp_realloc (dest, dest_size);
 
--- 1/mpbsd/xtom.c
+++ 2/mpbsd/xtom.c
@@ -76,7 +76,7 @@
 
   str_size = s - begs;
 
-  xsize = str_size / mp_bases[16].chars_per_limb + 1;
+  xsize = str_size / __mp_bases[16].chars_per_limb + 1;
   x->_mp_alloc = xsize;
   x->_mp_d = (mp_ptr) (*__gmp_allocate_func) (xsize * BYTES_PER_MP_LIMB);
 
--- 1/mpf/add.c
+++ 2/mpf/add.c
@@ -110,7 +110,7 @@
 
   /* Allocate temp space for the result.  Allocate
      just vsize + ediff later???  */
-  tp = TMP_ALLOC_LIMBS (prec);
+  tp = (mp_ptr) TMP_ALLOC (prec * BYTES_PER_MP_LIMB);
 
   if (ediff >= prec)
     {
--- 1/mpf/div_ui.c
+++ 2/mpf/div_ui.c
@@ -71,7 +71,7 @@
   up = u->_mp_d;
 
   tsize = 1 + prec;
-  tp = TMP_ALLOC_LIMBS (tsize + 1);
+  tp = (mp_ptr) TMP_ALLOC ((tsize + 1) * BYTES_PER_MP_LIMB);
 
   if (usize > tsize)
     {
--- 1/mpf/eq.c
+++ 2/mpf/eq.c
@@ -26,7 +26,7 @@
 mpf_eq (mpf_srcptr u, mpf_srcptr v, unsigned long int n_bits)
 {
   mp_srcptr up, vp, p;
-  mp_size_t usize, vsize, minsize, maxsize, n_limbs, i, size;
+  mp_size_t usize, vsize, minsize, maxsize, n_limbs, i;
   mp_exp_t uexp, vexp;
   mp_limb_t diff;
   int cnt;
@@ -100,38 +100,33 @@
 	return 0;
     }
 
-  n_bits -= (maxsize - 1) * GMP_NUMB_BITS;
-
-  size = maxsize - minsize;
-  if (size != 0)
+  if (minsize != maxsize)
     {
       if (up[0] != vp[0])
 	return 0;
+    }
 
-      /* Now either U or V has its limbs consumed, i.e, continues with an
-	 infinite number of implicit zero limbs.  Check that the other operand
-	 has just zeros in the corresponding, relevant part.  */
-
-      if (usize > vsize)
-	p = up - size;
-      else
-	p = vp - size;
-
-      for (i = size - 1; i > 0; i--)
-	{
-	  if (p[i] != 0)
-	    return 0;
-	}
+  /* Now either U or V has its limbs consumed.  Check the the other operand
+     has just zeros in the corresponding, relevant part.  */
 
-      diff = p[0];
-    }
+  if (usize > vsize)
+    p = up + minsize - maxsize;
   else
-    {
-      /* Both U or V has its limbs consumed.  */
+    p = vp + minsize - maxsize;
 
-      diff = up[0] ^ vp[0];
+  for (i = maxsize - minsize - 1; i > 0; i--)
+    {
+      if (p[i] != 0)
+	return 0;
     }
 
+  n_bits -= (maxsize - 1) * GMP_NUMB_BITS;
+
+  if (minsize != maxsize)
+    diff = p[0];
+  else
+    diff = up[0] ^ vp[0];
+
   if (n_bits < GMP_NUMB_BITS)
     diff >>= GMP_NUMB_BITS - n_bits;
 
--- 1/mpf/Makefile.am
+++ 2/mpf/Makefile.am
@@ -23,9 +23,9 @@
 
 noinst_LTLIBRARIES = libmpf.la
 libmpf_la_SOURCES = \
-  init.c init2.c inits.c set.c set_ui.c set_si.c set_str.c set_d.c set_z.c \
-  set_q.c iset.c iset_ui.c iset_si.c iset_str.c iset_d.c clear.c clears.c \
-  get_str.c dump.c size.c eq.c reldiff.c sqrt.c random2.c inp_str.c out_str.c \
+  init.c init2.c set.c set_ui.c set_si.c set_str.c set_d.c set_z.c \
+  set_q.c iset.c iset_ui.c iset_si.c iset_str.c iset_d.c clear.c get_str.c \
+  dump.c size.c eq.c reldiff.c sqrt.c random2.c inp_str.c out_str.c \
   add.c add_ui.c sub.c sub_ui.c ui_sub.c mul.c mul_ui.c div.c div_ui.c \
   cmp.c cmp_d.c cmp_si.c cmp_ui.c mul_2exp.c div_2exp.c abs.c neg.c get_d.c \
   get_d_2exp.c set_dfl_prec.c set_prc.c set_prc_raw.c get_dfl_prec.c get_prc.c \
--- 1/mpf/mul.c
+++ 2/mpf/mul.c
@@ -63,7 +63,7 @@
       mp_size_t adj;
 
       rsize = usize + vsize;
-      tp = TMP_ALLOC_LIMBS (rsize);
+      tp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
       cy_limb = (usize >= vsize
 		 ? mpn_mul (tp, up, usize, vp, vsize)
 		 : mpn_mul (tp, vp, vsize, up, usize));
--- 1/mpf/sqrt.c
+++ 2/mpf/sqrt.c
@@ -82,7 +82,7 @@
   /* root size is ceil(tsize/2), this will be our desired "prec" limbs */
   ASSERT ((tsize + 1) / 2 == prec);
 
-  tp = TMP_ALLOC_LIMBS (tsize);
+  tp = (mp_ptr) TMP_ALLOC (tsize * BYTES_PER_MP_LIMB);
 
   if (usize > tsize)
     {
--- 1/mpf/sqrt_ui.c
+++ 2/mpf/sqrt_ui.c
@@ -77,7 +77,7 @@
   zeros = 2 * prec - 2;
   rsize = zeros + 1 + U2;
 
-  tp = TMP_ALLOC_LIMBS (rsize);
+  tp = (mp_ptr) TMP_ALLOC (rsize * BYTES_PER_MP_LIMB);
 
   MPN_ZERO (tp, zeros);
   tp[zeros] = u & GMP_NUMB_MASK;
--- 1/mpf/sub.c
+++ 2/mpf/sub.c
@@ -194,7 +194,7 @@
 	  vsize = prec - 1;
 	}
 
-      tp = TMP_ALLOC_LIMBS (prec);
+      tp = (mp_ptr) TMP_ALLOC (prec * BYTES_PER_MP_LIMB);
       {
 	mp_limb_t cy_limb;
 	if (vsize == 0)
@@ -275,7 +275,7 @@
 
   /* Allocate temp space for the result.  Allocate
      just vsize + ediff later???  */
-  tp = TMP_ALLOC_LIMBS (prec);
+  tp = (mp_ptr) TMP_ALLOC (prec * BYTES_PER_MP_LIMB);
 
   if (ediff >= prec)
     {
--- 1/mpf/ui_sub.c
+++ 2/mpf/ui_sub.c
@@ -140,7 +140,7 @@
 
   /* Allocate temp space for the result.  Allocate
      just vsize + ediff later???  */
-  tp = TMP_ALLOC_LIMBS (prec);
+  tp = (mp_ptr) TMP_ALLOC (prec * BYTES_PER_MP_LIMB);
 
   if (ediff >= prec)
     {
--- 1/mp-h.in
+++ 2/mp-h.in
@@ -106,7 +106,6 @@
 typedef long int		mp_limb_signed_t;
 #endif
 #endif
-typedef unsigned long int	mp_bitcnt_t;
 
 typedef struct
 {
--- 1/mpn/a29k/submul_1.s
+++ 2/mpn/a29k/submul_1.s
@@ -82,7 +82,7 @@
 	subc	gr102,gr102,gr110
 	subc	gr103,gr103,gr111
 
-	add	gr104,gr103,gr111	; invert carry from previous sub
+	add	gr104,gr103,gr111	; invert carry from previus sub
 	addc	gr120,gr120,0
 
 	mtsrim	cr,(8-1)
@@ -105,7 +105,7 @@
 	add	gr117,gr117,gr120
 	addc	gr118,gr118,0
 	sub	gr119,gr119,gr117
-	add	gr104,gr119,gr117	; invert carry from previous sub
+	add	gr104,gr119,gr117	; invert carry from previus sub
 	store	0,0,gr119,lr2
 	jmpfdec	lr4,Loop2
 	 addc	gr120,gr118,0
--- 1/mpn/alpha/add_n.asm
+++ 2/mpn/alpha/add_n.asm
@@ -1,7 +1,7 @@
 dnl  Alpha mpn_add_n -- Add two limb vectors of the same length > 0 and
 dnl  store sum in a third limb vector.
 
-dnl  Copyright 1995, 1999, 2000, 2005 Free Software Foundation, Inc.
+dnl  Copyright 1995, 2000, 2002, 2005 Free Software Foundation, Inc.
 
 dnl  This file is part of the GNU MP Library.
 
@@ -21,126 +21,97 @@
 include(`../config.m4')
 
 C      cycles/limb
-C EV4:     ?
-C EV5:     4.75
-C EV6:     3
-
-dnl  INPUT PARAMETERS
-dnl  res_ptr	r16
-dnl  s1_ptr	r17
-dnl  s2_ptr	r18
-dnl  size	r19
+C EV4:     7.75
+C EV5:     5.75
+C EV6:     4
+
+C  INPUT PARAMETERS
+C  rp	r16
+C  up	r17
+C  vp	r18
+C  n	r19
 
 ASM_START()
 PROLOGUE(mpn_add_n)
-	bis	r31,r31,r25		C clear cy
-	subq	r19,4,r19		C decr loop cnt
-	blt	r19,$Lend2		C if less than 4 limbs, goto 2nd loop
-C Start software pipeline for 1st loop
-	ldq	r0,0(r18)
-	ldq	r4,0(r17)
-	ldq	r1,8(r18)
-	ldq	r5,8(r17)
-	addq	r17,32,r17		C update s1_ptr
-	ldq	r2,16(r18)
-	addq	r0,r4,r20		C 1st main add
-	ldq	r3,24(r18)
-	subq	r19,4,r19		C decr loop cnt
-	ldq	r6,-16(r17)
-	cmpult	r20,r0,r25		C compute cy from last add
-	ldq	r7,-8(r17)
-	addq	r1,r5,r28		C 2nd main add
-	addq	r18,32,r18		C update s2_ptr
-	addq	r28,r25,r21		C 2nd carry add
-	cmpult	r28,r5,r8		C compute cy from last add
-	blt	r19,$Lend1		C if less than 4 limbs remain, jump
-C 1st loop handles groups of 4 limbs in a software pipeline
-	ALIGN(16)
-$Loop:	cmpult	r21,r28,r25		C compute cy from last add
-	ldq	r0,0(r18)
-	bis	r8,r25,r25		C combine cy from the two adds
-	ldq	r1,8(r18)
-	addq	r2,r6,r28		C 3rd main add
-	ldq	r4,0(r17)
-	addq	r28,r25,r22		C 3rd carry add
-	ldq	r5,8(r17)
-	cmpult	r28,r6,r8		C compute cy from last add
-	cmpult	r22,r28,r25		C compute cy from last add
-	stq	r20,0(r16)
-	bis	r8,r25,r25		C combine cy from the two adds
-	stq	r21,8(r16)
-	addq	r3,r7,r28		C 4th main add
-	addq	r28,r25,r23		C 4th carry add
-	cmpult	r28,r7,r8		C compute cy from last add
-	cmpult	r23,r28,r25		C compute cy from last add
-		addq	r17,32,r17		C update s1_ptr
-	bis	r8,r25,r25		C combine cy from the two adds
-		addq	r16,32,r16		C update res_ptr
-	addq	r0,r4,r28		C 1st main add
-	ldq	r2,16(r18)
-	addq	r25,r28,r20		C 1st carry add
-	ldq	r3,24(r18)
-	cmpult	r28,r4,r8		C compute cy from last add
-	ldq	r6,-16(r17)
-	cmpult	r20,r28,r25		C compute cy from last add
-	ldq	r7,-8(r17)
-	bis	r8,r25,r25		C combine cy from the two adds
-	subq	r19,4,r19		C decr loop cnt
-	stq	r22,-16(r16)
-	addq	r1,r5,r28		C 2nd main add
-	stq	r23,-8(r16)
-	addq	r25,r28,r21		C 2nd carry add
-		addq	r18,32,r18		C update s2_ptr
-	cmpult	r28,r5,r8		C compute cy from last add
-	bge	r19,$Loop
-C Finish software pipeline for 1st loop
-$Lend1:	cmpult	r21,r28,r25		C compute cy from last add
-	bis	r8,r25,r25		C combine cy from the two adds
-	addq	r2,r6,r28		C 3rd main add
-	addq	r28,r25,r22		C 3rd carry add
-	cmpult	r28,r6,r8		C compute cy from last add
-	cmpult	r22,r28,r25		C compute cy from last add
-	stq	r20,0(r16)
-	bis	r8,r25,r25		C combine cy from the two adds
-	stq	r21,8(r16)
-	addq	r3,r7,r28		C 4th main add
-	addq	r28,r25,r23		C 4th carry add
-	cmpult	r28,r7,r8		C compute cy from last add
-	cmpult	r23,r28,r25		C compute cy from last add
-	bis	r8,r25,r25		C combine cy from the two adds
-	addq	r16,32,r16		C update res_ptr
-	stq	r22,-16(r16)
-	stq	r23,-8(r16)
-$Lend2:	addq	r19,4,r19		C restore loop cnt
-	beq	r19,$Lret
-C Start software pipeline for 2nd loop
-	ldq	r0,0(r18)
-	ldq	r4,0(r17)
+	ldq	r3,0(r17)
+	ldq	r4,0(r18)
+
 	subq	r19,1,r19
-	beq	r19,$Lend0
-C 2nd loop handles remaining 1-3 limbs
-	ALIGN(16)
-$Loop0:	addq	r0,r4,r28		C main add
-	ldq	r0,8(r18)
-	cmpult	r28,r4,r8		C compute cy from last add
-	ldq	r4,8(r17)
-	addq	r28,r25,r20		C carry add
-	addq	r18,8,r18
+	and	r19,4-1,r2	C number of limbs in first loop
+	bis	r31,r31,r0
+	beq	r2,$L0		C if multiple of 4 limbs, skip first loop
+
+	subq	r19,r2,r19
+
+$Loop0:	subq	r2,1,r2
+	ldq	r5,8(r17)
+	addq	r4,r0,r4
+	ldq	r6,8(r18)
+	cmpult	r4,r0,r1
+	addq	r3,r4,r4
+	cmpult	r4,r3,r0
+	stq	r4,0(r16)
+	bis	r0,r1,r0
+
 	addq	r17,8,r17
-	stq	r20,0(r16)
-	cmpult	r20,r28,r25		C compute cy from last add
-	subq	r19,1,r19		C decr loop cnt
-	bis	r8,r25,r25		C combine cy from the two adds
+	addq	r18,8,r18
+	bis	r5,r5,r3
+	bis	r6,r6,r4
 	addq	r16,8,r16
-	bne	r19,$Loop0
-$Lend0:	addq	r0,r4,r28		C main add
-	addq	r28,r25,r20		C carry add
-	cmpult	r28,r4,r8		C compute cy from last add
-	cmpult	r20,r28,r25		C compute cy from last add
-	stq	r20,0(r16)
-	bis	r8,r25,r25		C combine cy from the two adds
+	bne	r2,$Loop0
 
-$Lret:	bis	r25,r31,r0		C return cy
+$L0:	beq	r19,$Lend
+
+	ALIGN(8)
+$Loop:	subq	r19,4,r19
+
+	ldq	r5,8(r17)
+	addq	r4,r0,r4
+	ldq	r6,8(r18)
+	cmpult	r4,r0,r1
+	addq	r3,r4,r4
+	cmpult	r4,r3,r0
+	stq	r4,0(r16)
+	bis	r0,r1,r0
+
+	ldq	r3,16(r17)
+	addq	r6,r0,r6
+	ldq	r4,16(r18)
+	cmpult	r6,r0,r1
+	addq	r5,r6,r6
+	cmpult	r6,r5,r0
+	stq	r6,8(r16)
+	bis	r0,r1,r0
+
+	ldq	r5,24(r17)
+	addq	r4,r0,r4
+	ldq	r6,24(r18)
+	cmpult	r4,r0,r1
+	addq	r3,r4,r4
+	cmpult	r4,r3,r0
+	stq	r4,16(r16)
+	bis	r0,r1,r0
+
+	ldq	r3,32(r17)
+	addq	r6,r0,r6
+	ldq	r4,32(r18)
+	cmpult	r6,r0,r1
+	addq	r5,r6,r6
+	cmpult	r6,r5,r0
+	stq	r6,24(r16)
+	bis	r0,r1,r0
+
+	addq	r17,32,r17
+	addq	r18,32,r18
+	addq	r16,32,r16
+	bne	r19,$Loop
+
+$Lend:	addq	r4,r0,r4
+	cmpult	r4,r0,r1
+	addq	r3,r4,r4
+	cmpult	r4,r3,r0
+	stq	r4,0(r16)
+	bis	r0,r1,r0
 	ret	r31,(r26),1
 EPILOGUE(mpn_add_n)
 ASM_END()
--- 1/mpn/alpha/ev5/gmp-mparam.h
+++ 2/mpn/alpha/ev5/gmp-mparam.h
@@ -18,51 +18,35 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 
 /* 600 MHz 21164A */
 
-/* Generated by tuneup.c, 2009-12-10, gcc 3.3 */
+/* Generated by tuneup.c, 2009-01-15, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             14
-#define MUL_TOOM33_THRESHOLD             74
+#define MUL_KARATSUBA_THRESHOLD          14
+#define MUL_TOOM3_THRESHOLD              74
 #define MUL_TOOM44_THRESHOLD            118
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              28
+#define SQR_KARATSUBA_THRESHOLD          28
 #define SQR_TOOM3_THRESHOLD              77
 #define SQR_TOOM4_THRESHOLD             136
 
-#define MUL_FFT_TABLE  { 240, 480, 1344, 1792, 5120, 20480, 81920, 196608, 0 }
-#define MUL_FFT_MODF_THRESHOLD          240
-#define MUL_FFT_THRESHOLD              1920
-
-#define SQR_FFT_TABLE  { 240, 480, 1216, 1792, 5120, 12288, 81920, 196608, 0 }
-#define SQR_FFT_MODF_THRESHOLD          208
-#define SQR_FFT_THRESHOLD              1408
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               44
-#define MULLO_MUL_N_THRESHOLD           246
-
-#define MULMOD_BNM1_THRESHOLD             9
-
-#define DC_DIV_QR_THRESHOLD              54
-#define DC_DIVAPPR_Q_THRESHOLD          180
-#define DC_BDIV_QR_THRESHOLD             47
-#define DC_BDIV_Q_THRESHOLD             181
-#define INV_MULMOD_BNM1_THRESHOLD       100
-#define INV_NEWTON_THRESHOLD            196
-#define INV_APPR_THRESHOLD               35
-#define BINV_NEWTON_THRESHOLD           214
-#define REDC_1_TO_REDC_N_THRESHOLD       77
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              44
+#define MULLOW_MUL_N_THRESHOLD          246
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* preinv always */
+#define DIV_DC_THRESHOLD                 53
+#define POWM_THRESHOLD                   85
 
-#define MATRIX22_STRASSEN_THRESHOLD      15
-#define HGCD_THRESHOLD                  108
+#define MATRIX22_STRASSEN_THRESHOLD      17
+#define HGCD_THRESHOLD                  104
 #define GCD_DC_THRESHOLD                321
-#define GCDEXT_DC_THRESHOLD             217
+#define GCDEXT_DC_THRESHOLD             298
 #define JACOBI_BASE_METHOD                3
 
 #define DIVREM_1_NORM_THRESHOLD           0  /* preinv always */
@@ -82,6 +66,14 @@
 #define SET_STR_DC_THRESHOLD            532
 #define SET_STR_PRECOMPUTE_THRESHOLD   1501
 
+#define MUL_FFT_TABLE  { 240, 480, 1344, 1792, 5120, 20480, 81920, 196608, 0 }
+#define MUL_FFT_MODF_THRESHOLD          240
+#define MUL_FFT_THRESHOLD              1920
+
+#define SQR_FFT_TABLE  { 240, 480, 1216, 1792, 5120, 12288, 81920, 196608, 0 }
+#define SQR_FFT_MODF_THRESHOLD          208
+#define SQR_FFT_THRESHOLD              1408
+
 /* These tables need to be updated.  */
 
 #define MUL_FFT_TABLE2 {{1, 4}, {177, 5}, {193, 4}, {209, 5}, {353, 6}, {385, 5}, {417, 6}, {833, 7}, {897, 6}, {961, 7}, {1025, 6}, {1089, 7}, {1665, 8}, {1793, 7}, {2177, 8}, {2305, 7}, {2433, 8}, {2817, 7}, {2945, 8}, {3329, 9}, {3457, 8}, {4865, 9}, {5633, 8}, {6401, 10}, {7169, 9}, {11777, 10}, {12801, 9}, {13825, 10}, {15361, 9}, {19969, 10}, {23553, 9}, {24065, 11}, {30721, 10}, {48129, 11}, {63489, 10}, {72705, 11}, {96257, 12}, {126977, 11}, {194561, 12}, {258049, 11}, {325633, 12}, {389121, 13}, {516097, 12}, {MP_SIZE_T_MAX,0}}
--- 1/mpn/alpha/ev6/gmp-mparam.h
+++ 2/mpn/alpha/ev6/gmp-mparam.h
@@ -18,67 +18,59 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 #define DIVEXACT_BY3_METHOD 0	/* override ../diveby3.asm */
 
-/* 500 MHz 21164 (agnesi.math.su.se) */
+/* 500 MHz 21164 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 3.3 */
+/* Generated by tuneup.c, 2009-01-12, gcc 3.3 */
 
-#define MUL_TOOM22_THRESHOLD             32
-#define MUL_TOOM33_THRESHOLD             73
-#define MUL_TOOM44_THRESHOLD            166
-
-#define SQR_BASECASE_THRESHOLD            5
-#define SQR_TOOM2_THRESHOLD              60
-#define SQR_TOOM3_THRESHOLD             105
-#define SQR_TOOM4_THRESHOLD             167
-
-#define MUL_FFT_TABLE  { 304, 864, 1728, 3328, 9216, 28672, 147456, 327680, 0 }
-#define MUL_FFT_MODF_THRESHOLD          432
-#define MUL_FFT_THRESHOLD              3968
-
-#define SQR_FFT_TABLE  { 400, 864, 1600, 3840, 9216, 28672, 114688, 327680, 0 }
-#define SQR_FFT_MODF_THRESHOLD          376
-#define SQR_FFT_THRESHOLD              3712
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD              129
-#define MULLO_MUL_N_THRESHOLD          7842
-
-#define MULMOD_BNM1_THRESHOLD            16
-
-#define DC_DIV_QR_THRESHOLD             112
-#define DC_DIVAPPR_Q_THRESHOLD          396
-#define DC_BDIV_QR_THRESHOLD            110
-#define DC_BDIV_Q_THRESHOLD             315
-#define INV_MULMOD_BNM1_THRESHOLD        90
-#define INV_NEWTON_THRESHOLD            387
-#define INV_APPR_THRESHOLD               39
-#define BINV_NEWTON_THRESHOLD           406
-#define REDC_1_TO_REDC_N_THRESHOLD      110
-
-#define MATRIX22_STRASSEN_THRESHOLD      16
-#define HGCD_THRESHOLD                  276
-#define GCD_DC_THRESHOLD               1197
-#define GCDEXT_DC_THRESHOLD             799
+#define MUL_KARATSUBA_THRESHOLD          31
+#define MUL_TOOM3_THRESHOLD             101
+#define MUL_TOOM44_THRESHOLD            168
+
+#define SQR_BASECASE_THRESHOLD            6
+#define SQR_KARATSUBA_THRESHOLD          60
+#define SQR_TOOM3_THRESHOLD             102
+#define SQR_TOOM4_THRESHOLD             172
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD             102
+#define MULLOW_MUL_N_THRESHOLD          399
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* preinv always */
+#define DIV_DC_THRESHOLD                134
+#define POWM_THRESHOLD                  257
+
+#define MATRIX22_STRASSEN_THRESHOLD      19
+#define HGCD_THRESHOLD                  303
+#define GCD_DC_THRESHOLD               1258
+#define GCDEXT_DC_THRESHOLD             807
 #define JACOBI_BASE_METHOD                3
 
 #define DIVREM_1_NORM_THRESHOLD           0  /* preinv always */
 #define DIVREM_1_UNNORM_THRESHOLD         0  /* always */
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 2
-#define MOD_1_2_THRESHOLD                 6
-#define MOD_1_4_THRESHOLD                30
+#define MOD_1_1_THRESHOLD                13
+#define MOD_1_2_THRESHOLD                14
+#define MOD_1_4_THRESHOLD                40
 #define USE_PREINV_DIVREM_1               1  /* preinv always */
 #define USE_PREINV_MOD_1                  1  /* preinv always */
 #define DIVEXACT_1_THRESHOLD              0  /* always */
-#define MODEXACT_1_ODD_THRESHOLD      MP_SIZE_T_MAX  /* never */
+#define MODEXACT_1_ODD_THRESHOLD          0  /* always */
 
-#define GET_STR_DC_THRESHOLD             18
+#define GET_STR_DC_THRESHOLD             16
 #define GET_STR_PRECOMPUTE_THRESHOLD     23
-#define SET_STR_DC_THRESHOLD           2797
-#define SET_STR_PRECOMPUTE_THRESHOLD  10681
+#define SET_STR_DC_THRESHOLD           4615
+#define SET_STR_PRECOMPUTE_THRESHOLD   8178
+
+#define MUL_FFT_TABLE  { 432, 864, 1856, 3840, 11264, 28672, 81920, 327680, 0 }
+#define MUL_FFT_MODF_THRESHOLD          448
+#define MUL_FFT_THRESHOLD              4992
+
+#define SQR_FFT_TABLE  { 432, 864, 1728, 3840, 9216, 20480, 81920, 327680, 786432, 0 }
+#define SQR_FFT_MODF_THRESHOLD          344
+#define SQR_FFT_THRESHOLD              3712
--- 1/mpn/alpha/ev6/mul_1.asm
+++ 2/mpn/alpha/ev6/mul_1.asm
@@ -49,7 +49,7 @@
 C
 C   We're doing 7 of the 8 carry propagations with a br fixup code and 1 with a
 C   put-the-carry-into-hi.  The idea is that these branches are very rarely
-C   taken, and since a non-taken branch consumes no resources, that is better
+C   taken, and since a non-taken branch consumes no resurces, that is better
 C   than an addq.
 C
 C   Software pipeline: a load in cycle #09, feeds a mul in cycle #16, feeds an
@@ -126,7 +126,7 @@
 	mulq	r2,r19,r3	C r3 = prod_low
 	umulh	r2,r19,r21	C r21 = prod_high
 	beq	r20,$Le1b	C jump if size was == 1
-	bis	r31, r31, r0	C FIXME: shouldn't need this
+	bis	r31, r31, r0	C FIXME: shouldtn't need this
 	ldq	r2,0(r17)	C r2 = s1_limb
 	lda	r17,8(r17)	C s1_ptr++
 	lda	r20,-1(r20)	C size--
--- 1/mpn/alpha/ev6/nails/addmul_1.asm
+++ 2/mpn/alpha/ev6/nails/addmul_1.asm
@@ -25,7 +25,7 @@
 C EV6:     4
 
 C TODO
-C  * Reroll loop for 3.75 c/l with current 4-way unrolling.
+C  * Reroll loop for 3.75 c/l with current 4-way unrulling.
 C  * The loop is overscheduled wrt loads and wrt multiplies, in particular
 C    umulh.
 C  * Use FP loop count and multiple exit points, that would simplify feed-in lp0
--- 1/mpn/alpha/ev6/nails/gmp-mparam.h
+++ 2/mpn/alpha/ev6/nails/gmp-mparam.h
@@ -18,16 +18,16 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 /* Generated by tuneup.c, 2004-02-07, gcc 3.3 */
 
-#define MUL_TOOM22_THRESHOLD             40
-#define MUL_TOOM33_THRESHOLD            236
+#define MUL_KARATSUBA_THRESHOLD          40
+#define MUL_TOOM3_THRESHOLD             236
 
 #define SQR_BASECASE_THRESHOLD            7  /* karatsuba */
-#define SQR_TOOM2_THRESHOLD               0  /* never sqr_basecase */
+#define SQR_KARATSUBA_THRESHOLD           0  /* never sqr_basecase */
 #define SQR_TOOM3_THRESHOLD             120
 
 #define DIV_SB_PREINV_THRESHOLD       MP_SIZE_T_MAX  /* no preinv with nails */
--- 1/mpn/alpha/ev6/nails/mul_1.asm
+++ 2/mpn/alpha/ev6/nails/mul_1.asm
@@ -25,10 +25,10 @@
 C EV6:     3.25
 
 C TODO
-C  * Reroll loop for 3.0 c/l with current 4-way unrolling.
+C  * Reroll loop for 3.0 c/l with current 4-way unrulling.
 C  * The loop is overscheduled wrt loads and wrt multiplies, in particular
 C    umulh.
-C  * Use FP loop count and multiple exit points, that would simplify feed-in lp0
+C  * Use FP loop count and multiple exit points, that would simpily feed-in lp0
 C    and would work since the loop structure is really regular.
 
 C  INPUT PARAMETERS
--- 1/mpn/alpha/ev6/nails/submul_1.asm
+++ 2/mpn/alpha/ev6/nails/submul_1.asm
@@ -25,10 +25,10 @@
 C EV6:     4
 
 C TODO
-C  * Reroll loop for 3.75 c/l with current 4-way unrolling.
+C  * Reroll loop for 3.75 c/l with current 4-way unrulling.
 C  * The loop is overscheduled wrt loads and wrt multiplies, in particular
 C    umulh.
-C  * Use FP loop count and multiple exit points, that would simplify feed-in lp0
+C  * Use FP loop count and multiple exit points, that would simpily feed-in lp0
 C    and would work since the loop structure is really regular.
 
 C  INPUT PARAMETERS
--- 1/mpn/alpha/gmp-mparam.h
+++ 2/mpn/alpha/gmp-mparam.h
@@ -18,7 +18,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 
@@ -26,18 +26,18 @@
 
 /* Generated by tuneup.c, 2009-01-15, gcc 3.2 */
 
-#define MUL_TOOM22_THRESHOLD             12
-#define MUL_TOOM33_THRESHOLD             69
+#define MUL_KARATSUBA_THRESHOLD          12
+#define MUL_TOOM3_THRESHOLD              69
 #define MUL_TOOM44_THRESHOLD             88
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              20
+#define SQR_KARATSUBA_THRESHOLD          20
 #define SQR_TOOM3_THRESHOLD              62
 #define SQR_TOOM4_THRESHOLD             155
 
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               40
-#define MULLO_MUL_N_THRESHOLD           202
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              40
+#define MULLOW_MUL_N_THRESHOLD          202
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* preinv always */
 #define DIV_DC_THRESHOLD                 38
--- 1/mpn/alpha/lshift.asm
+++ 2/mpn/alpha/lshift.asm
@@ -1,6 +1,6 @@
 dnl  Alpha mpn_lshift -- Shift a number left.
 
-dnl  Copyright 1994, 1995, 2000, 2003, 2009 Free Software Foundation, Inc.
+dnl  Copyright 1994, 1995, 2000, 2002, 2003 Free Software Foundation, Inc.
 
 dnl  This file is part of the GNU MP Library.
 
@@ -20,9 +20,9 @@
 include(`../config.m4')
 
 C      cycles/limb
-C EV4:     ?
-C EV5:     3.25
-C EV6:     1.75
+C EV4:     4.75
+C EV5:     4
+C EV6:     2
 
 C  INPUT PARAMETERS
 C  rp	r16
@@ -35,137 +35,63 @@
 PROLOGUE(mpn_lshift)
 	s8addq	r18,r17,r17	C make r17 point at end of s1
 	ldq	r4,-8(r17)	C load first limb
-	subq	r31,r19,r20
+	subq	r17,8,r17
+	subq	r31,r19,r7
 	s8addq	r18,r16,r16	C make r16 point at end of RES
 	subq	r18,1,r18
-	and	r18,4-1,r28	C number of limbs in first loop
-	srl	r4,r20,r0	C compute function result
+	and	r18,4-1,r20	C number of limbs in first loop
+	srl	r4,r7,r0	C compute function result
 
-	beq	r28,L(L0)
-	subq	r18,r28,r18
+	beq	r20,$L0
+	subq	r18,r20,r18
 
 	ALIGN(8)
-L(top0):
-	ldq	r3,-16(r17)
+$Loop0:	ldq	r3,-8(r17)
 	subq	r16,8,r16
-	sll	r4,r19,r5
 	subq	r17,8,r17
-	subq	r28,1,r28
-	srl	r3,r20,r6
+	subq	r20,1,r20
+	sll	r4,r19,r5
+	srl	r3,r7,r6
 	bis	r3,r3,r4
 	bis	r5,r6,r8
 	stq	r8,0(r16)
-	bne	r28,L(top0)
+	bne	r20,$Loop0
 
-L(L0):	sll	r4,r19,r24
-	beq	r18,L(end)
-C warm up phase 1
-	ldq	r1,-16(r17)
-	subq	r18,4,r18
-	ldq	r2,-24(r17)
-	ldq	r3,-32(r17)
-	ldq	r4,-40(r17)
-C warm up phase 2
-	srl	r1,r20,r7
-	sll	r1,r19,r21
-	srl	r2,r20,r8
-	beq	r18,L(end1)
-	ldq	r1,-48(r17)
-	sll	r2,r19,r22
-	ldq	r2,-56(r17)
-	srl	r3,r20,r5
-	bis	r7,r24,r7
-	sll	r3,r19,r23
-	bis	r8,r21,r8
-	srl	r4,r20,r6
-	ldq	r3,-64(r17)
-	sll	r4,r19,r24
-	ldq	r4,-72(r17)
-	subq	r18,4,r18
-	beq	r18,L(end2)
-	ALIGN(16)
-C main loop
-L(top):	stq	r7,-8(r16)
-	bis	r5,r22,r5
-	stq	r8,-16(r16)
-	bis	r6,r23,r6
+$L0:	beq	r18,$Lend
 
-	srl	r1,r20,r7
+	ALIGN(8)
+$Loop:	ldq	r3,-8(r17)
+	subq	r16,32,r16
 	subq	r18,4,r18
-	sll	r1,r19,r21
-	unop	C ldq	r31,-96(r17)
+	sll	r4,r19,r5
+	srl	r3,r7,r6
 
-	srl	r2,r20,r8
-	ldq	r1,-80(r17)
-	sll	r2,r19,r22
-	ldq	r2,-88(r17)
-
-	stq	r5,-24(r16)
-	bis	r7,r24,r7
-	stq	r6,-32(r16)
-	bis	r8,r21,r8
-
-	srl	r3,r20,r5
-	unop	C ldq	r31,-96(r17)
-	sll	r3,r19,r23
-	subq	r16,32,r16
+	ldq	r4,-16(r17)
+	sll	r3,r19,r1
+	bis	r5,r6,r8
+	stq	r8,24(r16)
+	srl	r4,r7,r2
+
+	ldq	r3,-24(r17)
+	sll	r4,r19,r5
+	bis	r1,r2,r8
+	stq	r8,16(r16)
+	srl	r3,r7,r6
 
-	srl	r4,r20,r6
-	ldq	r3,-96(r17)
-	sll	r4,r19,r24
-	ldq	r4,-104(r17)
+	ldq	r4,-32(r17)
+	sll	r3,r19,r1
+	bis	r5,r6,r8
+	stq	r8,8(r16)
+	srl	r4,r7,r2
 
 	subq	r17,32,r17
-	bne	r18,L(top)
-C cool down phase 2/1
-L(end2):
-	stq	r7,-8(r16)
-	bis	r5,r22,r5
-	stq	r8,-16(r16)
-	bis	r6,r23,r6
-	srl	r1,r20,r7
-	sll	r1,r19,r21
-	srl	r2,r20,r8
-	sll	r2,r19,r22
-	stq	r5,-24(r16)
-	bis	r7,r24,r7
-	stq	r6,-32(r16)
-	bis	r8,r21,r8
-	srl	r3,r20,r5
-	sll	r3,r19,r23
-	srl	r4,r20,r6
-	sll	r4,r19,r24
-C cool down phase 2/2
-	stq	r7,-40(r16)
-	bis	r5,r22,r5
-	stq	r8,-48(r16)
-	bis	r6,r23,r6
-	stq	r5,-56(r16)
-	stq	r6,-64(r16)
-C cool down phase 2/3
-	stq	r24,-72(r16)
-	ret	r31,(r26),1
+	bis	r1,r2,r8
+	stq	r8,0(r16)
 
-C cool down phase 1/1
-L(end1):
-	sll	r2,r19,r22
-	srl	r3,r20,r5
-	bis	r7,r24,r7
-	sll	r3,r19,r23
-	bis	r8,r21,r8
-	srl	r4,r20,r6
-	sll	r4,r19,r24
-C cool down phase 1/2
-	stq	r7,-8(r16)
-	bis	r5,r22,r5
-	stq	r8,-16(r16)
-	bis	r6,r23,r6
-	stq	r5,-24(r16)
-	stq	r6,-32(r16)
-	stq	r24,-40(r16)
-	ret	r31,(r26),1
+	bgt	r18,$Loop
 
-L(end):	stq	r24,-8(r16)
+$Lend:	sll	r4,r19,r8
+	stq	r8,-8(r16)
 	ret	r31,(r26),1
 EPILOGUE(mpn_lshift)
 ASM_END()
--- 1/mpn/alpha/README
+++ 2/mpn/alpha/README
@@ -36,7 +36,7 @@
 them to "$6" or "$f6" where necessary.
 
 "0x" introduces a hex constant in gas and DEC as, but on Unicos "^X" is
-required.  The X() macro accommodates this difference.
+required.  The X() macro accomodates this difference.
 
 "cvttqc" is required by DEC as, "cvttq/c" is required by Unicos, and gas will
 accept either.  We use cvttqc and have an m4 define expand to cvttq/c where
@@ -60,7 +60,7 @@
 EV4
 
 1. This chip has very limited store bandwidth.  The on-chip L1 cache is write-
-   through, and a cache line is transferred from the store buffer to the off-
+   through, and a cache line is transfered from the store buffer to the off-
    chip L2 in as much 15 cycles on most systems.  This delay hurts mpn_add_n,
    mpn_sub_n, mpn_lshift, and mpn_rshift.
 
--- 1/mpn/alpha/rshift.asm
+++ 2/mpn/alpha/rshift.asm
@@ -1,6 +1,6 @@
 dnl  Alpha mpn_rshift -- Shift a number right.
 
-dnl  Copyright 1994, 1995, 2000, 2009 Free Software Foundation, Inc.
+dnl  Copyright 1994, 1995, 2000, 2002 Free Software Foundation, Inc.
 
 dnl  This file is part of the GNU MP Library.
 
@@ -20,9 +20,9 @@
 include(`../config.m4')
 
 C      cycles/limb
-C EV4:     ?
-C EV5:     3.25
-C EV6:     1.75
+C EV4:     4.75
+C EV5:     3.75
+C EV6:     2
 
 C  INPUT PARAMETERS
 C  rp	r16
@@ -34,136 +34,62 @@
 ASM_START()
 PROLOGUE(mpn_rshift)
 	ldq	r4,0(r17)	C load first limb
-	subq	r31,r19,r20
+	addq	r17,8,r17
+	subq	r31,r19,r7
 	subq	r18,1,r18
-	and	r18,4-1,r28	C number of limbs in first loop
-	sll	r4,r20,r0	C compute function result
+	and	r18,4-1,r20	C number of limbs in first loop
+	sll	r4,r7,r0	C compute function result
 
-	beq	r28,L(L0)
-	subq	r18,r28,r18
+	beq	r20,$L0
+	subq	r18,r20,r18
 
 	ALIGN(8)
-L(top0):
-	ldq	r3,8(r17)
+$Loop0:	ldq	r3,0(r17)
 	addq	r16,8,r16
-	srl	r4,r19,r5
 	addq	r17,8,r17
-	subq	r28,1,r28
-	sll	r3,r20,r6
+	subq	r20,1,r20
+	srl	r4,r19,r5
+	sll	r3,r7,r6
 	bis	r3,r3,r4
 	bis	r5,r6,r8
 	stq	r8,-8(r16)
-	bne	r28,L(top0)
+	bne	r20,$Loop0
 
-L(L0):	srl	r4,r19,r24
-	beq	r18,L(end)
-C warm up phase 1
-	ldq	r1,8(r17)
-	subq	r18,4,r18
-	ldq	r2,16(r17)
-	ldq	r3,24(r17)
-	ldq	r4,32(r17)
-C warm up phase 2
-	sll	r1,r20,r7
-	srl	r1,r19,r21
-	sll	r2,r20,r8
-	beq	r18,L(end1)
-	ldq	r1,40(r17)
-	srl	r2,r19,r22
-	ldq	r2,48(r17)
-	sll	r3,r20,r5
-	bis	r7,r24,r7
-	srl	r3,r19,r23
-	bis	r8,r21,r8
-	sll	r4,r20,r6
-	ldq	r3,56(r17)
-	srl	r4,r19,r24
-	ldq	r4,64(r17)
-	subq	r18,4,r18
-	beq	r18,L(end2)
-	ALIGN(16)
-C main loop
-L(top):	stq	r7,0(r16)
-	bis	r5,r22,r5
-	stq	r8,8(r16)
-	bis	r6,r23,r6
+$L0:	beq	r18,$Lend
 
-	sll	r1,r20,r7
+	ALIGN(8)
+$Loop:	ldq	r3,0(r17)
+	addq	r16,32,r16
 	subq	r18,4,r18
-	srl	r1,r19,r21
-	unop	C ldq	r31,-96(r17)
+	srl	r4,r19,r5
+	sll	r3,r7,r6
 
-	sll	r2,r20,r8
-	ldq	r1,72(r17)
-	srl	r2,r19,r22
-	ldq	r2,80(r17)
-
-	stq	r5,16(r16)
-	bis	r7,r24,r7
-	stq	r6,24(r16)
-	bis	r8,r21,r8
-
-	sll	r3,r20,r5
-	unop	C ldq	r31,-96(r17)
-	srl	r3,r19,r23
-	addq	r16,32,r16
+	ldq	r4,8(r17)
+	srl	r3,r19,r1
+	bis	r5,r6,r8
+	stq	r8,-32(r16)
+	sll	r4,r7,r2
+
+	ldq	r3,16(r17)
+	srl	r4,r19,r5
+	bis	r1,r2,r8
+	stq	r8,-24(r16)
+	sll	r3,r7,r6
 
-	sll	r4,r20,r6
-	ldq	r3,88(r17)
-	srl	r4,r19,r24
-	ldq	r4,96(r17)
+	ldq	r4,24(r17)
+	srl	r3,r19,r1
+	bis	r5,r6,r8
+	stq	r8,-16(r16)
+	sll	r4,r7,r2
 
 	addq	r17,32,r17
-	bne	r18,L(top)
-C cool down phase 2/1
-L(end2):
-	stq	r7,0(r16)
-	bis	r5,r22,r5
-	stq	r8,8(r16)
-	bis	r6,r23,r6
-	sll	r1,r20,r7
-	srl	r1,r19,r21
-	sll	r2,r20,r8
-	srl	r2,r19,r22
-	stq	r5,16(r16)
-	bis	r7,r24,r7
-	stq	r6,24(r16)
-	bis	r8,r21,r8
-	sll	r3,r20,r5
-	srl	r3,r19,r23
-	sll	r4,r20,r6
-	srl	r4,r19,r24
-C cool down phase 2/2
-	stq	r7,32(r16)
-	bis	r5,r22,r5
-	stq	r8,40(r16)
-	bis	r6,r23,r6
-	stq	r5,48(r16)
-	stq	r6,56(r16)
-C cool down phase 2/3
-	stq	r24,64(r16)
-	ret	r31,(r26),1
+	bis	r1,r2,r8
+	stq	r8,-8(r16)
 
-C cool down phase 1/1
-L(end1):
-	srl	r2,r19,r22
-	sll	r3,r20,r5
-	bis	r7,r24,r7
-	srl	r3,r19,r23
-	bis	r8,r21,r8
-	sll	r4,r20,r6
-	srl	r4,r19,r24
-C cool down phase 1/2
-	stq	r7,0(r16)
-	bis	r5,r22,r5
-	stq	r8,8(r16)
-	bis	r6,r23,r6
-	stq	r5,16(r16)
-	stq	r6,24(r16)
-	stq	r24,32(r16)
-	ret	r31,(r26),1
+	bgt	r18,$Loop
 
-L(end):	stq	r24,0(r16)
+$Lend:	srl	r4,r19,r8
+	stq	r8,0(r16)
 	ret	r31,(r26),1
 EPILOGUE(mpn_rshift)
 ASM_END()
--- 1/mpn/alpha/sub_n.asm
+++ 2/mpn/alpha/sub_n.asm
@@ -1,7 +1,7 @@
-dnl  Alpha mpn_sub_n -- Subtract two limb vectors of the same length > 0
-dnl  and store difference in a third limb vector.
+dnl  Alpha mpn_sub_n -- Subtract two limb vectors of the same length > 0 and
+dnl  store difference in a third limb vector.
 
-dnl  Copyright 1995, 1999, 2000, 2005 Free Software Foundation, Inc.
+dnl  Copyright 1995, 2000, 2002, 2005 Free Software Foundation, Inc.
 
 dnl  This file is part of the GNU MP Library.
 
@@ -21,126 +21,97 @@
 include(`../config.m4')
 
 C      cycles/limb
-C EV4:     ?
-C EV5:     4.75
-C EV6:     3
-
-dnl  INPUT PARAMETERS
-dnl  res_ptr	r16
-dnl  s1_ptr	r17
-dnl  s2_ptr	r18
-dnl  size	r19
+C EV4:     7.75
+C EV5:     5.75
+C EV6:     4
+
+C  INPUT PARAMETERS
+C  rp	r16
+C  up	r17
+C  vp	r18
+C  n	r19
 
 ASM_START()
 PROLOGUE(mpn_sub_n)
-	bis	r31,r31,r25		C clear cy
-	subq	r19,4,r19		C decr loop cnt
-	blt	r19,$Lend2		C if less than 4 limbs, goto 2nd loop
-C Start software pipeline for 1st loop
-	ldq	r0,0(r18)
-	ldq	r4,0(r17)
-	ldq	r1,8(r18)
-	ldq	r5,8(r17)
-	addq	r17,32,r17		C update s1_ptr
-	ldq	r2,16(r18)
-	subq	r4,r0,r20		C 1st main subtract
-	ldq	r3,24(r18)
-	subq	r19,4,r19		C decr loop cnt
-	ldq	r6,-16(r17)
-	cmpult	r4,r0,r25		C compute cy from last subtract
-	ldq	r7,-8(r17)
-	subq	r5,r1,r28		C 2nd main subtract
-	addq	r18,32,r18		C update s2_ptr
-	subq	r28,r25,r21		C 2nd carry subtract
-	cmpult	r5,r1,r8		C compute cy from last subtract
-	blt	r19,$Lend1		C if less than 4 limbs remain, jump
-C 1st loop handles groups of 4 limbs in a software pipeline
-	ALIGN(16)
-$Loop:	cmpult	r28,r25,r25		C compute cy from last subtract
-	ldq	r0,0(r18)
-	bis	r8,r25,r25		C combine cy from the two subtracts
-	ldq	r1,8(r18)
-	subq	r6,r2,r28		C 3rd main subtract
-	ldq	r4,0(r17)
-	subq	r28,r25,r22		C 3rd carry subtract
-	ldq	r5,8(r17)
-	cmpult	r6,r2,r8		C compute cy from last subtract
-	cmpult	r28,r25,r25		C compute cy from last subtract
-	stq	r20,0(r16)
-	bis	r8,r25,r25		C combine cy from the two subtracts
-	stq	r21,8(r16)
-	subq	r7,r3,r28		C 4th main subtract
-	subq	r28,r25,r23		C 4th carry subtract
-	cmpult	r7,r3,r8		C compute cy from last subtract
-	cmpult	r28,r25,r25		C compute cy from last subtract
-		addq	r17,32,r17		C update s1_ptr
-	bis	r8,r25,r25		C combine cy from the two subtracts
-		addq	r16,32,r16		C update res_ptr
-	subq	r4,r0,r28		C 1st main subtract
-	ldq	r2,16(r18)
-	subq	r28,r25,r20		C 1st carry subtract
-	ldq	r3,24(r18)
-	cmpult	r4,r0,r8		C compute cy from last subtract
-	ldq	r6,-16(r17)
-	cmpult	r28,r25,r25		C compute cy from last subtract
-	ldq	r7,-8(r17)
-	bis	r8,r25,r25		C combine cy from the two subtracts
-	subq	r19,4,r19		C decr loop cnt
-	stq	r22,-16(r16)
-	subq	r5,r1,r28		C 2nd main subtract
-	stq	r23,-8(r16)
-	subq	r28,r25,r21		C 2nd carry subtract
-		addq	r18,32,r18		C update s2_ptr
-	cmpult	r5,r1,r8		C compute cy from last subtract
-	bge	r19,$Loop
-C Finish software pipeline for 1st loop
-$Lend1:	cmpult	r28,r25,r25		C compute cy from last subtract
-	bis	r8,r25,r25		C combine cy from the two subtracts
-	subq	r6,r2,r28		C cy add
-	subq	r28,r25,r22		C 3rd main subtract
-	cmpult	r6,r2,r8		C compute cy from last subtract
-	cmpult	r28,r25,r25		C compute cy from last subtract
-	stq	r20,0(r16)
-	bis	r8,r25,r25		C combine cy from the two subtracts
-	stq	r21,8(r16)
-	subq	r7,r3,r28		C cy add
-	subq	r28,r25,r23		C 4th main subtract
-	cmpult	r7,r3,r8		C compute cy from last subtract
-	cmpult	r28,r25,r25		C compute cy from last subtract
-	bis	r8,r25,r25		C combine cy from the two subtracts
-	addq	r16,32,r16		C update res_ptr
-	stq	r22,-16(r16)
-	stq	r23,-8(r16)
-$Lend2:	addq	r19,4,r19		C restore loop cnt
-	beq	r19,$Lret
-C Start software pipeline for 2nd loop
-	ldq	r0,0(r18)
-	ldq	r4,0(r17)
+	ldq	r3,0(r17)
+	ldq	r4,0(r18)
+
 	subq	r19,1,r19
-	beq	r19,$Lend0
-C 2nd loop handles remaining 1-3 limbs
-	ALIGN(16)
-$Loop0:	subq	r4,r0,r28		C main subtract
-	cmpult	r4,r0,r8		C compute cy from last subtract
-	ldq	r0,8(r18)
-	ldq	r4,8(r17)
-	subq	r28,r25,r20		C carry subtract
-	addq	r18,8,r18
+	and	r19,4-1,r2	C number of limbs in first loop
+	bis	r31,r31,r0
+	beq	r2,$L0		C if multiple of 4 limbs, skip first loop
+
+	subq	r19,r2,r19
+
+$Loop0:	subq	r2,1,r2
+	ldq	r5,8(r17)
+	addq	r4,r0,r4
+	ldq	r6,8(r18)
+	cmpult	r4,r0,r1
+	subq	r3,r4,r4
+	cmpult	r3,r4,r0
+	stq	r4,0(r16)
+	bis	r0,r1,r0
+
 	addq	r17,8,r17
-	stq	r20,0(r16)
-	cmpult	r28,r25,r25		C compute cy from last subtract
-	subq	r19,1,r19		C decr loop cnt
-	bis	r8,r25,r25		C combine cy from the two subtracts
+	addq	r18,8,r18
+	bis	r5,r5,r3
+	bis	r6,r6,r4
 	addq	r16,8,r16
-	bne	r19,$Loop0
-$Lend0:	subq	r4,r0,r28		C main subtract
-	subq	r28,r25,r20		C carry subtract
-	cmpult	r4,r0,r8		C compute cy from last subtract
-	cmpult	r28,r25,r25		C compute cy from last subtract
-	stq	r20,0(r16)
-	bis	r8,r25,r25		C combine cy from the two subtracts
+	bne	r2,$Loop0
 
-$Lret:	bis	r25,r31,r0		C return cy
+$L0:	beq	r19,$Lend
+
+	ALIGN(8)
+$Loop:	subq	r19,4,r19
+
+	ldq	r5,8(r17)
+	addq	r4,r0,r4
+	ldq	r6,8(r18)
+	cmpult	r4,r0,r1
+	subq	r3,r4,r4
+	cmpult	r3,r4,r0
+	stq	r4,0(r16)
+	bis	r0,r1,r0
+
+	ldq	r3,16(r17)
+	addq	r6,r0,r6
+	ldq	r4,16(r18)
+	cmpult	r6,r0,r1
+	subq	r5,r6,r6
+	cmpult	r5,r6,r0
+	stq	r6,8(r16)
+	bis	r0,r1,r0
+
+	ldq	r5,24(r17)
+	addq	r4,r0,r4
+	ldq	r6,24(r18)
+	cmpult	r4,r0,r1
+	subq	r3,r4,r4
+	cmpult	r3,r4,r0
+	stq	r4,16(r16)
+	bis	r0,r1,r0
+
+	ldq	r3,32(r17)
+	addq	r6,r0,r6
+	ldq	r4,32(r18)
+	cmpult	r6,r0,r1
+	subq	r5,r6,r6
+	cmpult	r5,r6,r0
+	stq	r6,24(r16)
+	bis	r0,r1,r0
+
+	addq	r17,32,r17
+	addq	r18,32,r18
+	addq	r16,32,r16
+	bne	r19,$Loop
+
+$Lend:	addq	r4,r0,r4
+	cmpult	r4,r0,r1
+	subq	r3,r4,r4
+	cmpult	r3,r4,r0
+	stq	r4,0(r16)
+	bis	r0,r1,r0
 	ret	r31,(r26),1
 EPILOGUE(mpn_sub_n)
 ASM_END()
--- 1/mpn/arm/gmp-mparam.h
+++ 2/mpn/arm/gmp-mparam.h
@@ -18,66 +18,58 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 /* 593MHz ARM (gcc50.fsffrance.org) */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.3 */
+/* Generated by tuneup.c, 2009-03-05, gcc 4.3 */
 
-#define MUL_TOOM22_THRESHOLD             34
-#define MUL_TOOM33_THRESHOLD            121
-#define MUL_TOOM44_THRESHOLD            191
-
-#define SQR_BASECASE_THRESHOLD           13
-#define SQR_TOOM2_THRESHOLD              78
-#define SQR_TOOM3_THRESHOLD             141
+#define MUL_KARATSUBA_THRESHOLD          34
+#define MUL_TOOM3_THRESHOLD             125
+#define MUL_TOOM44_THRESHOLD            184
+
+#define SQR_BASECASE_THRESHOLD           15
+#define SQR_KARATSUBA_THRESHOLD          82
+#define SQR_TOOM3_THRESHOLD             147
 #define SQR_TOOM4_THRESHOLD             212
 
-#define MUL_FFT_TABLE  { 368, 928, 1920, 4608, 14336, 40960, 163840, 655360, 0 }
-#define MUL_FFT_MODF_THRESHOLD          384
-#define MUL_FFT_THRESHOLD              6912
-
-#define SQR_FFT_TABLE  { 432, 928, 1664, 4608, 10240, 40960, 163840, 655360, 0 }
-#define SQR_FFT_MODF_THRESHOLD          376
-#define SQR_FFT_THRESHOLD              7168
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD              120
-#define MULLO_MUL_N_THRESHOLD         11138
-
-#define MULMOD_BNM1_THRESHOLD            19
-
-#define DC_DIV_QR_THRESHOLD             128
-#define DC_DIVAPPR_Q_THRESHOLD          430
-#define DC_BDIV_QR_THRESHOLD            127
-#define DC_BDIV_Q_THRESHOLD             296
-#define INV_MULMOD_BNM1_THRESHOLD        74
-#define INV_NEWTON_THRESHOLD            474
-#define INV_APPR_THRESHOLD               78
-#define BINV_NEWTON_THRESHOLD           942
-#define REDC_1_TO_REDC_N_THRESHOLD      115
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD             128
+#define MULLOW_MUL_N_THRESHOLD         1095
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* preinv always */
+#define DIV_DC_THRESHOLD                130
+#define POWM_THRESHOLD                  200
 
 #define MATRIX22_STRASSEN_THRESHOLD      19
 #define HGCD_THRESHOLD                  110
-#define GCD_DC_THRESHOLD                680
-#define GCDEXT_DC_THRESHOLD             521
+#define GCD_DC_THRESHOLD                734
+#define GCDEXT_DC_THRESHOLD             748
 #define JACOBI_BASE_METHOD                2
 
 #define DIVREM_1_NORM_THRESHOLD           0  /* preinv always */
 #define DIVREM_1_UNNORM_THRESHOLD         0  /* always */
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 8
-#define MOD_1_2_THRESHOLD             MP_SIZE_T_MAX  /* never */
-#define MOD_1_4_THRESHOLD             MP_SIZE_T_MAX  /* never */
+#define MOD_1_1_THRESHOLD                10
+#define MOD_1_2_THRESHOLD               996
+#define MOD_1_4_THRESHOLD               997
 #define USE_PREINV_DIVREM_1               1  /* preinv always */
 #define USE_PREINV_MOD_1                  1  /* preinv always */
 #define DIVREM_2_THRESHOLD                0  /* preinv always */
 #define DIVEXACT_1_THRESHOLD              0  /* always */
-#define MODEXACT_1_ODD_THRESHOLD      MP_SIZE_T_MAX  /* never */
+#define MODEXACT_1_ODD_THRESHOLD          0  /* always */
 
-#define GET_STR_DC_THRESHOLD             14
-#define GET_STR_PRECOMPUTE_THRESHOLD     28
-#define SET_STR_DC_THRESHOLD            309
-#define SET_STR_PRECOMPUTE_THRESHOLD   1037
+#define GET_STR_DC_THRESHOLD             18
+#define GET_STR_PRECOMPUTE_THRESHOLD     35
+#define SET_STR_DC_THRESHOLD            321
+#define SET_STR_PRECOMPUTE_THRESHOLD   1057
+
+#define MUL_FFT_TABLE  { 400, 928, 1920, 4608, 14336, 40960, 0 }
+#define MUL_FFT_MODF_THRESHOLD          416
+#define MUL_FFT_THRESHOLD              5888
+
+#define SQR_FFT_TABLE  { 432, 928, 1664, 4608, 10240, 40960, 0 }
+#define SQR_FFT_MODF_THRESHOLD          376
+#define SQR_FFT_THRESHOLD              4352
--- 1/mpn/arm/invert_limb.asm
+++ 2/mpn/arm/invert_limb.asm
@@ -1,6 +1,6 @@
 dnl  ARM mpn_invert_limb -- Invert a normalized limb.
 
-dnl  Copyright 2001, 2009 Free Software Foundation, Inc.
+dnl  Copyright 2001 Free Software Foundation, Inc.
 
 dnl  This file is part of the GNU MP Library.
 
@@ -19,68 +19,77 @@
 
 include(`../config.m4')
 
-ASM_START()
+C INPUT PARAMETERS
+define(`d',`r0')	C number to be inverted
+
+
 PROLOGUE(mpn_invert_limb)
-	ldr	r2, L(4)
-L(2):	add	r2, pc, r2
-	mov	r3, r0, lsr #23
-	mov	r3, r3, asl #1
-	ldrh	r3, [r3, r2]
-	mov	r1, r3, asl #17
-	mul	r3, r3, r3
-	umull	r12, r2, r3, r0
-	sub	r1, r1, r2, asl #1
-	umull	r12, r2, r1, r1
-	umull	r3, r12, r0, r12
-	umull	r2, r3, r0, r2
-	adds	r2, r2, r12
-	adc	r3, r3, #0
-	rsb	r1, r3, r1
-	mvn	r2, r2, lsr #30
-	add	r2, r2, r1, asl #2
-	umull	r3, r12, r0, r2
-	adds	r1, r3, r0
-	adc	r12, r12, r0
-	rsb	r0, r12, r2
-	bx	lr
-
-	ALIGN(4)
-L(4):	.word	approx_tab-8-512-L(2)
-EPILOGUE()
-
-	.section .rodata
-	ALIGN(2)
-approx_tab:
-	.short    0xffc0,0xfec0,0xfdc0,0xfcc0,0xfbc0,0xfac0,0xfa00,0xf900
-	.short    0xf800,0xf700,0xf640,0xf540,0xf440,0xf380,0xf280,0xf180
-	.short    0xf0c0,0xefc0,0xef00,0xee00,0xed40,0xec40,0xeb80,0xeac0
-	.short    0xe9c0,0xe900,0xe840,0xe740,0xe680,0xe5c0,0xe500,0xe400
-	.short    0xe340,0xe280,0xe1c0,0xe100,0xe040,0xdf80,0xdec0,0xde00
-	.short    0xdd40,0xdc80,0xdbc0,0xdb00,0xda40,0xd980,0xd8c0,0xd800
-	.short    0xd740,0xd680,0xd600,0xd540,0xd480,0xd3c0,0xd340,0xd280
-	.short    0xd1c0,0xd140,0xd080,0xcfc0,0xcf40,0xce80,0xcdc0,0xcd40
-	.short    0xcc80,0xcc00,0xcb40,0xcac0,0xca00,0xc980,0xc8c0,0xc840
-	.short    0xc780,0xc700,0xc640,0xc5c0,0xc540,0xc480,0xc400,0xc380
-	.short    0xc2c0,0xc240,0xc1c0,0xc100,0xc080,0xc000,0xbf80,0xbec0
-	.short    0xbe40,0xbdc0,0xbd40,0xbc80,0xbc00,0xbb80,0xbb00,0xba80
-	.short    0xba00,0xb980,0xb900,0xb840,0xb7c0,0xb740,0xb6c0,0xb640
-	.short    0xb5c0,0xb540,0xb4c0,0xb440,0xb3c0,0xb340,0xb2c0,0xb240
-	.short    0xb1c0,0xb140,0xb0c0,0xb080,0xb000,0xaf80,0xaf00,0xae80
-	.short    0xae00,0xad80,0xad40,0xacc0,0xac40,0xabc0,0xab40,0xaac0
-	.short    0xaa80,0xaa00,0xa980,0xa900,0xa8c0,0xa840,0xa7c0,0xa740
-	.short    0xa700,0xa680,0xa600,0xa5c0,0xa540,0xa4c0,0xa480,0xa400
-	.short    0xa380,0xa340,0xa2c0,0xa240,0xa200,0xa180,0xa140,0xa0c0
-	.short    0xa080,0xa000,0x9f80,0x9f40,0x9ec0,0x9e80,0x9e00,0x9dc0
-	.short    0x9d40,0x9d00,0x9c80,0x9c40,0x9bc0,0x9b80,0x9b00,0x9ac0
-	.short    0x9a40,0x9a00,0x9980,0x9940,0x98c0,0x9880,0x9840,0x97c0
-	.short    0x9780,0x9700,0x96c0,0x9680,0x9600,0x95c0,0x9580,0x9500
-	.short    0x94c0,0x9440,0x9400,0x93c0,0x9340,0x9300,0x92c0,0x9240
-	.short    0x9200,0x91c0,0x9180,0x9100,0x90c0,0x9080,0x9000,0x8fc0
-	.short    0x8f80,0x8f40,0x8ec0,0x8e80,0x8e40,0x8e00,0x8d80,0x8d40
-	.short    0x8d00,0x8cc0,0x8c80,0x8c00,0x8bc0,0x8b80,0x8b40,0x8b00
-	.short    0x8a80,0x8a40,0x8a00,0x89c0,0x8980,0x8940,0x88c0,0x8880
-	.short    0x8840,0x8800,0x87c0,0x8780,0x8740,0x8700,0x8680,0x8640
-	.short    0x8600,0x85c0,0x8580,0x8540,0x8500,0x84c0,0x8480,0x8440
-	.short    0x8400,0x8380,0x8340,0x8300,0x82c0,0x8280,0x8240,0x8200
-	.short    0x81c0,0x8180,0x8140,0x8100,0x80c0,0x8080,0x8040,0x8000
-ASM_END()
+	stmfd	sp!, {r4, lr}
+	mov	r3, d, lsr #23
+	sub	r3, r3, #256
+	add	r2, pc, #invtab-.-8
+	mov	r3, r3, lsl #1
+	ldrh	r1, [r2, r3]		C get initial approximation from table
+	mov	r2, r1, lsl #6		C start iteration 1
+	mul	ip, r2, r2
+	umull	lr, r4, ip, d
+	mov	r2, r4, lsl #1
+	rsb	r2, r2, r1, lsl #23	C iteration 1 complete
+	umull	ip, r3, r2, r2		C start iteration 2
+	umull	lr, r4, r3, d
+	umull	r3, r1, ip, d
+	adds	lr, lr, r1
+	addcs	r4, r4, #1
+	mov	r3, lr, lsr #30
+	orr	r4, r3, r4, lsl #2
+	mov	lr, lr, lsl #2
+	cmn	lr, #1
+	rsc	r2, r4, r2, lsl #2	C iteration 2 complete
+	umull	ip, r1, d, r2		C start adjustment step
+	add	r1, r1, d
+	cmn	r1, #1
+	beq	L(1)
+	adds	ip, ip, d
+	adc	r1, r1, #0
+	add	r2, r2, #1
+L(1):
+	adds	r3, ip, d
+	adcs	r1, r1, #0
+	moveq	r0, r2
+	addne	r0, r2, #1
+	ldmfd	sp!, {r4, pc}
+
+invtab:
+	.short	1023,1020,1016,1012,1008,1004,1000,996
+	.short	992,989,985,981,978,974,970,967
+	.short	963,960,956,953,949,946,942,939
+	.short	936,932,929,926,923,919,916,913
+	.short	910,907,903,900,897,894,891,888
+	.short	885,882,879,876,873,870,868,865
+	.short	862,859,856,853,851,848,845,842
+	.short	840,837,834,832,829,826,824,821
+	.short	819,816,814,811,809,806,804,801
+	.short	799,796,794,791,789,787,784,782
+	.short	780,777,775,773,771,768,766,764
+	.short	762,759,757,755,753,751,748,746
+	.short	744,742,740,738,736,734,732,730
+	.short	728,726,724,722,720,718,716,714
+	.short	712,710,708,706,704,702,700,699
+	.short	697,695,693,691,689,688,686,684
+	.short	682,680,679,677,675,673,672,670
+	.short	668,667,665,663,661,660,658,657
+	.short	655,653,652,650,648,647,645,644
+	.short	642,640,639,637,636,634,633,631
+	.short	630,628,627,625,624,622,621,619
+	.short	618,616,615,613,612,611,609,608
+	.short	606,605,604,602,601,599,598,597
+	.short	595,594,593,591,590,589,587,586
+	.short	585,583,582,581,579,578,577,576
+	.short	574,573,572,571,569,568,567,566
+	.short	564,563,562,561,560,558,557,556
+	.short	555,554,553,551,550,549,548,547
+	.short	546,544,543,542,541,540,539,538
+	.short	537,536,534,533,532,531,530,529
+	.short	528,527,526,525,524,523,522,521
+	.short	520,519,518,517,516,515,514,513
+EPILOGUE(mpn_invert_limb)
--- 1/mpn/asm-defs.m4
+++ 2/mpn/asm-defs.m4
@@ -49,7 +49,7 @@
 dnl  But note that when a quoted string is being read, a # isn't special, so
 dnl  apostrophes in comments in quoted strings must be avoided or they'll be
 dnl  interpreted as a closing quote mark.  But when the quoted text is
-dnl  re-read # will still act like a normal comment, suppressing macro
+dnl  re-read # will still act like a normal comment, supressing macro
 dnl  expansion.
 dnl
 dnl  For example,
@@ -219,7 +219,7 @@
 
 dnl  Usage: m4wrap_prepend(string)
 dnl
-dnl  Prepend the given string to what will be expanded under m4wrap at the
+dnl  Prepend the given string to what will be exapanded under m4wrap at the
 dnl  end of input.
 dnl
 dnl  This macro exists to work around variations in m4wrap() behaviour in
@@ -1079,12 +1079,6 @@
 dnl  aorslsh1_n
 m4_not_for_expansion(`OPERATION_addlsh1_n')
 m4_not_for_expansion(`OPERATION_sublsh1_n')
-m4_not_for_expansion(`OPERATION_rsblsh1_n')
-
-dnl  aorslsh2_n
-m4_not_for_expansion(`OPERATION_addlsh2_n')
-m4_not_for_expansion(`OPERATION_sublsh2_n')
-m4_not_for_expansion(`OPERATION_rsblsh2_n')
 
 dnl  rsh1aors_n
 m4_not_for_expansion(`OPERATION_rsh1add_n')
@@ -1097,7 +1091,7 @@
 dnl  terminate immediately.  The error message explains that the symbol
 dnl  should be in config.m4, copied from gmp-mparam.h.
 dnl
-dnl  Termination is immediate since missing say SQR_TOOM2_THRESHOLD can
+dnl  Termination is immediate since missing say SQR_KARATSUBA_THRESHOLD can
 dnl  lead to infinite loops and endless error messages.
 
 define(m4_config_gmp_mparam,
@@ -1309,20 +1303,16 @@
 define_mpn(add_n)
 define_mpn(add_nc)
 define_mpn(addlsh1_n)
-define_mpn(addlsh2_n)
-define_mpn(addlsh_n)
 define_mpn(addmul_1)
 define_mpn(addmul_1c)
 define_mpn(addmul_2)
 define_mpn(addmul_3)
 define_mpn(addmul_4)
-define_mpn(add_n_sub_n)
-define_mpn(add_n_sub_nc)
+define_mpn(addsub_n)
+define_mpn(addsub_nc)
 define_mpn(addaddmul_1msb0)
 define_mpn(and_n)
 define_mpn(andn_n)
-define_mpn(bdiv_q_1)
-define_mpn(bdiv_q_1_pi1)
 define_mpn(bdiv_dbm1c)
 define_mpn(bdivmod)
 define_mpn(cmp)
@@ -1352,14 +1342,10 @@
 define_mpn(kara_sqr_n)
 define_mpn(lshift)
 define_mpn(lshiftc)
-define_mpn(mod_1_1p)
-define_mpn(mod_1_1p_cps)
-define_mpn(mod_1s_2p)
-define_mpn(mod_1s_2p_cps)
-define_mpn(mod_1s_3p)
-define_mpn(mod_1s_3p_cps)
-define_mpn(mod_1s_4p)
-define_mpn(mod_1s_4p_cps)
+define_mpn(mod_1_1)
+define_mpn(mod_1_2)
+define_mpn(mod_1_3)
+define_mpn(mod_1_4)
 define_mpn(mod_1)
 define_mpn(mod_1c)
 define_mpn(mod_34lsub1)
@@ -1373,7 +1359,6 @@
 define_mpn(mul_4)
 define_mpn(mul_basecase)
 define_mpn(mul_n)
-define_mpn(mullo_basecase)
 define_mpn(perfect_square_p)
 define_mpn(popcount)
 define_mpn(preinv_divrem_1)
@@ -1387,9 +1372,6 @@
 define_mpn(random2)
 define_mpn(redc_1)
 define_mpn(redc_2)
-define_mpn(rsblsh1_n)
-define_mpn(rsblsh2_n)
-define_mpn(rsblsh_n)
 define_mpn(rsh1add_n)
 define_mpn(rsh1sub_n)
 define_mpn(rshift)
@@ -1401,7 +1383,6 @@
 define_mpn(sqr_diagonal)
 define_mpn(sub_n)
 define_mpn(sublsh1_n)
-define_mpn(sublsh2_n)
 define_mpn(sqrtrem)
 define_mpn(sub)
 define_mpn(sub_1)
--- 1/mpn/cray/gmp-mparam.h
+++ 2/mpn/cray/gmp-mparam.h
@@ -18,7 +18,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 #if 0
@@ -30,11 +30,11 @@
 
 /* Generated by tuneup.c, 2004-02-07, system compiler */
 
-#define MUL_TOOM22_THRESHOLD             71
-#define MUL_TOOM33_THRESHOLD            131
+#define MUL_KARATSUBA_THRESHOLD          71
+#define MUL_TOOM3_THRESHOLD             131
 
 #define SQR_BASECASE_THRESHOLD           32
-#define SQR_TOOM2_THRESHOLD             199
+#define SQR_KARATSUBA_THRESHOLD         199
 #define SQR_TOOM3_THRESHOLD             363
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* (preinv always) */
--- 1/mpn/cray/ieee/gmp-mparam.h
+++ 2/mpn/cray/ieee/gmp-mparam.h
@@ -18,16 +18,16 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 /* Generated by tuneup.c, 2004-02-07, system compiler */
 
-#define MUL_TOOM22_THRESHOLD            130
-#define MUL_TOOM33_THRESHOLD            260
+#define MUL_KARATSUBA_THRESHOLD         130
+#define MUL_TOOM3_THRESHOLD             260
 
 #define SQR_BASECASE_THRESHOLD            9  /* karatsuba */
-#define SQR_TOOM2_THRESHOLD               0  /* never sqr_basecase */
+#define SQR_KARATSUBA_THRESHOLD           0  /* never sqr_basecase */
 #define SQR_TOOM3_THRESHOLD              34
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* preinv always */
--- 1/mpn/cray/ieee/invert_limb.c
+++ 2/mpn/cray/ieee/invert_limb.c
@@ -73,14 +73,14 @@
   mp_limb_t xh, xl;
   mp_limb_t zh, zl;
 
-#if GMP_LIMB_BITS == 32
+#if BITS_PER_MP_LIMB == 32
   z = approx_tab[(d >> 23) - 0x100] << 6;	/* z < 2^16 */
 
   z2l = z * z;					/* z2l < 2^32 */
   umul_ppmm (th, tl, z2l, d);
   z = (z << 17) - (th << 1);
 #endif
-#if GMP_LIMB_BITS == 64
+#if BITS_PER_MP_LIMB == 64
   z = approx_tab[(d >> 55) - 0x100] << 6;	/* z < 2^16 */
 
   z2l = z * z;					/* z2l < 2^32 */
@@ -97,7 +97,7 @@
   umul_ppmm (xh, xl, z2l, d);
   tl += xh;
   th += tl < xh;
-  th = (th << 2) | (tl >> GMP_LIMB_BITS - 2);
+  th = (th << 2) | (tl >> BITS_PER_MP_LIMB - 2);
   tl = tl << 2;
   sub_ddmmss (zh, zl, z << 2, 0, th, tl);
 
--- 1/mpn/cray/lshift.c
+++ 2/mpn/cray/lshift.c
@@ -29,7 +29,7 @@
   mp_limb_t retval;
 
   sh_1 = cnt;
-  sh_2 = GMP_LIMB_BITS - sh_1;
+  sh_2 = BITS_PER_MP_LIMB - sh_1;
   retval = up[n - 1] >> sh_2;
 
 #pragma _CRI ivdep
--- 1/mpn/cray/README
+++ 2/mpn/cray/README
@@ -34,7 +34,7 @@
 128-bit products.  For IEEE systems, adding, and in particular
 computing carry is the main issue.  There are no vectorizing
 unsigned-less-than instructions, and the sequence that implement that
-operation is very long.
+opetration is very long.
 
 Shifting is the only operation that is simple to make fast.  All Cray
 systems have a bitblt instructions (Vi Vj,Vj<Ak and Vi Vj,Vj>Ak) that
--- 1/mpn/cray/rshift.c
+++ 2/mpn/cray/rshift.c
@@ -29,7 +29,7 @@
   mp_limb_t retval;
 
   sh_1 = cnt;
-  sh_2 = GMP_LIMB_BITS - sh_1;
+  sh_2 = BITS_PER_MP_LIMB - sh_1;
   retval = up[0] << sh_2;
 
 #pragma _CRI ivdep
--- 1/mpn/generic/add_n.c
+++ 2/mpn/generic/add_n.c
@@ -1,7 +1,6 @@
 /* mpn_add_n -- Add equal length limb vectors.
 
-Copyright 1992, 1993, 1994, 1996, 2000, 2002, 2009 Free Software Foundation,
-Inc.
+Copyright 1992, 1993, 1994, 1996, 2000, 2002 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -30,8 +29,8 @@
   mp_limb_t ul, vl, sl, rl, cy, cy1, cy2;
 
   ASSERT (n >= 1);
-  ASSERT (MPN_SAME_OR_INCR_P (rp, up, n));
-  ASSERT (MPN_SAME_OR_INCR_P (rp, vp, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, up, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, vp, n));
 
   cy = 0;
   do
@@ -60,8 +59,8 @@
   mp_limb_t ul, vl, rl, cy;
 
   ASSERT (n >= 1);
-  ASSERT (MPN_SAME_OR_INCR_P (rp, up, n));
-  ASSERT (MPN_SAME_OR_INCR_P (rp, vp, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, up, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, vp, n));
 
   cy = 0;
   do
--- 1/mpn/generic/addsub_n.c
+++ 2/mpn/generic/addsub_n.c
@@ -1,4 +1,4 @@
-/* mpn_add_n_sub_n -- Add and Subtract two limb vectors of equal, non-zero length.
+/* mpn_addsub_n -- Add and Subtract two limb vectors of equal, non-zero length.
 
    THE FUNCTION IN THIS FILE IS INTERNAL WITH A MUTABLE INTERFACE.  IT IS ONLY
    SAFE TO REACH IT THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
@@ -31,13 +31,13 @@
 #define PART_SIZE (L1_CACHE_SIZE / BYTES_PER_MP_LIMB / 6)
 
 
-/* mpn_add_n_sub_n.
+/* mpn_addsub_n.
    r1[] = s1[] + s2[]
    r2[] = s1[] - s2[]
    All operands have n limbs.
    In-place operations allowed.  */
 mp_limb_t
-mpn_add_n_sub_n (mp_ptr r1p, mp_ptr r2p, mp_srcptr s1p, mp_srcptr s2p, mp_size_t n)
+mpn_addsub_n (mp_ptr r1p, mp_ptr r2p, mp_srcptr s1p, mp_srcptr s2p, mp_size_t n)
 {
   mp_limb_t acyn, acyo;		/* carry for add */
   mp_limb_t scyn, scyo;		/* carry for subtract */
@@ -148,13 +148,13 @@
   s2p = malloc (n * BYTES_PER_MP_LIMB);
   TIME (t,(mpn_add_n(r1p,s1p,s2p,n),mpn_sub_n(r1p,s1p,s2p,n)));
   printf ("              separate add and sub: %.3f\n", t);
-  TIME (t,mpn_add_n_sub_n(r1p,r2p,s1p,s2p,n));
+  TIME (t,mpn_addsub_n(r1p,r2p,s1p,s2p,n));
   printf ("combined addsub separate variables: %.3f\n", t);
-  TIME (t,mpn_add_n_sub_n(r1p,r2p,r1p,s2p,n));
+  TIME (t,mpn_addsub_n(r1p,r2p,r1p,s2p,n));
   printf ("        combined addsub r1 overlap: %.3f\n", t);
-  TIME (t,mpn_add_n_sub_n(r1p,r2p,r1p,s2p,n));
+  TIME (t,mpn_addsub_n(r1p,r2p,r1p,s2p,n));
   printf ("        combined addsub r2 overlap: %.3f\n", t);
-  TIME (t,mpn_add_n_sub_n(r1p,r2p,r1p,r2p,n));
+  TIME (t,mpn_addsub_n(r1p,r2p,r1p,r2p,n));
   printf ("          combined addsub in-place: %.3f\n", t);
 
   return 0;
--- 1/mpn/generic/bdivmod.c
+++ 2/mpn/generic/bdivmod.c
@@ -20,23 +20,23 @@
 
 /* q_high = mpn_bdivmod (qp, up, usize, vp, vsize, d).
 
-   Puts the low d/GMP_LIMB_BITS limbs of Q = U / V mod 2^d at qp, and
-   returns the high d%GMP_LIMB_BITS bits of Q as the result.
+   Puts the low d/BITS_PER_MP_LIMB limbs of Q = U / V mod 2^d at qp, and
+   returns the high d%BITS_PER_MP_LIMB bits of Q as the result.
 
-   Also, U - Q * V mod 2^(usize*GMP_LIMB_BITS) is placed at up.  Since the
-   low d/GMP_LIMB_BITS limbs of this difference are zero, the code allows
+   Also, U - Q * V mod 2^(usize*BITS_PER_MP_LIMB) is placed at up.  Since the
+   low d/BITS_PER_MP_LIMB limbs of this difference are zero, the code allows
    the limb vectors at qp to overwrite the low limbs at up, provided qp <= up.
 
    Preconditions:
    1.  V is odd.
-   2.  usize * GMP_LIMB_BITS >= d.
+   2.  usize * BITS_PER_MP_LIMB >= d.
    3.  If Q and U overlap, qp <= up.
 
    Ken Weber (kweber@mat.ufrgs.br, kweber@mcs.kent.edu)
 
    Funding for this work has been partially provided by Conselho Nacional
    de Desenvolvimento Cienti'fico e Tecnolo'gico (CNPq) do Brazil, Grant
-   301314194-2, and was done while I was a visiting researcher in the Instituto
+   301314194-2, and was done while I was a visiting reseacher in the Instituto
    de Matema'tica at Universidade Federal do Rio Grande do Sul (UFRGS).
 
    References:
--- 1/mpn/generic/binvert.c
+++ 2/mpn/generic/binvert.c
@@ -1,12 +1,13 @@
-/* Compute {up,n}^(-1) mod B^n.
+/* Compute {up,n}^(-1) mod 2(n*GMP_NUMB_BITS).
 
    Contributed to the GNU project by Torbjorn Granlund.
 
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
+   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH A MUTABLE INTERFACE.  IT IS
+   ONLY SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS
+   ALMOST GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP
+   RELEASE.
 
-Copyright (C) 2004, 2005, 2006, 2007, 2009 Free Software Foundation, Inc.
+Copyright (C) 2004, 2005, 2006, 2007 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -51,9 +52,12 @@
 mp_size_t
 mpn_binvert_itch (mp_size_t n)
 {
-  mp_size_t itch_local = mpn_mulmod_bnm1_next_size (n);
-  mp_size_t itch_out = mpn_mulmod_bnm1_itch (itch_local);
-  return itch_local + itch_out;
+#if WANT_FFT
+  if (ABOVE_THRESHOLD (n, 2 * MUL_FFT_MODF_THRESHOLD))
+    return mpn_fft_next_size (n, mpn_fft_best_k (n, 0));
+  else
+#endif
+    return 3 * (n - (n >> 1));
 }
 
 void
@@ -72,28 +76,42 @@
 
   xp = scratch;
 
-  /* Compute a base value of rn limbs.  */
+  /* Compute a base value using a low-overhead O(n^2) algorithm.  FIXME: We
+     should call some divide-and-conquer lsb division function here for an
+     operand subrange.  */
   MPN_ZERO (xp, rn);
   xp[0] = 1;
   binvert_limb (di, up[0]);
   if (BELOW_THRESHOLD (rn, DC_BDIV_Q_THRESHOLD))
-    mpn_sbpi1_bdiv_q (rp, xp, rn, up, rn, -di);
+    mpn_sb_bdiv_q (rp, xp, rn, up, rn, -di);
   else
-    mpn_dcpi1_bdiv_q (rp, xp, rn, up, rn, -di);
+    mpn_dc_bdiv_q (rp, xp, rn, up, rn, -di);
 
   /* Use Newton iterations to get the desired precision.  */
   for (; rn < n; rn = newrn)
     {
-      mp_size_t m;
       newrn = *--sizp;
 
-      /* X <- UR. */
-      m = mpn_mulmod_bnm1_next_size (newrn);
-      mpn_mulmod_bnm1 (xp, m, up, newrn, rp, rn, xp + m);
-      mpn_sub_1 (xp + m, xp, rn - (m - newrn), 1);
-
-      /* R = R(X/B^rn) */
-      mpn_mullo_n (rp + rn, rp, xp + rn, newrn - rn);
+#if WANT_FFT
+      if (ABOVE_THRESHOLD (newrn, 2 * MUL_FFT_MODF_THRESHOLD))
+	{
+	  int k;
+	  mp_size_t m, i;
+
+	  k = mpn_fft_best_k (newrn, 0);
+	  m = mpn_fft_next_size (newrn, k);
+	  mpn_mul_fft (xp, m, up, newrn, rp, rn, k);
+	  for (i = rn - 1; i >= 0; i--)
+	    if (xp[i] > (i == 0))
+	      {
+		mpn_add_1 (xp + rn, xp + rn, newrn - rn, 1);
+		break;
+	      }
+	}
+      else
+#endif
+	mpn_mul (xp, up, newrn, rp, rn);
+      mpn_mullow_n (rp + rn, rp, xp + rn, newrn - rn);
       mpn_neg_n (rp + rn, rp + rn, newrn - rn);
     }
 }
--- 1/mpn/generic/dive_1.c
+++ 2/mpn/generic/dive_1.c
@@ -30,7 +30,7 @@
 /* Divide a={src,size} by d=divisor and store the quotient in q={dst,size}.
    q will only be correct if d divides a exactly.
 
-   A separate loop is used for shift==0 because n<<GMP_LIMB_BITS doesn't
+   A separate loop is used for shift==0 because n<<BITS_PER_MP_LIMB doesn't
    give zero on all CPUs (for instance it doesn't on the x86s).  This
    separate loop might run faster too, helping odd divisors.
 
@@ -50,7 +50,7 @@
    faster on some CPUs and would mean just the shift==0 style loop would be
    needed.
 
-   If n<<GMP_LIMB_BITS gives zero on a particular CPU then the separate
+   If n<<BITS_PER_MP_LIMB gives zero on a particular CPU then the separate
    shift==0 loop is unnecessary, and could be eliminated if there's no great
    speed difference.
 
--- 1/mpn/generic/divexact.c
+++ 2/mpn/generic/divexact.c
@@ -4,11 +4,12 @@
 
    Contributed to the GNU project by Torbjorn Granlund.
 
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
+   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH A MUTABLE INTERFACE.  IT IS
+   ONLY SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS
+   ALMOST GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP
+   RELEASE.
 
-Copyright 2006, 2007, 2009 Free Software Foundation, Inc.
+Copyright 2006, 2007 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -26,72 +27,6 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#include "gmp.h"
-#include "gmp-impl.h"
-#include "longlong.h"
-
-#if 1
-void
-mpn_divexact (mp_ptr qp,
-	      mp_srcptr np, mp_size_t nn,
-	      mp_srcptr dp, mp_size_t dn)
-{
-  unsigned shift;
-  mp_size_t qn;
-  mp_ptr tp;
-  TMP_DECL;
-
-  ASSERT (dn > 0);
-  ASSERT (nn >= dn);
-  ASSERT (dp[dn-1] > 0);
-  ASSERT (np[nn-1] > 0);
-
-  qn = nn + 1 - dn;
-
-  while (dp[0] == 0)
-    {
-      ASSERT (np[0] == 0);
-      dp++;
-      np++;
-      dn--;
-      nn--;
-    }
-  count_trailing_zeros (shift, dp[0]);
-
-  TMP_MARK;
-  if (shift > 0)
-    {
-      tp = TMP_ALLOC_LIMBS (dn);
-      mpn_rshift (tp, dp, dn, shift);
-      dp = tp;
-
-      /* FIXME: It's sufficient to get the qn least significant
-	 limbs. */
-      tp = TMP_ALLOC_LIMBS (nn);
-      mpn_rshift (tp, np, nn, shift);
-      np = tp;
-    }
-  else
-    {
-      mp_ptr tp = TMP_ALLOC_LIMBS (qn);
-      MPN_COPY (tp, np, qn);
-      np = tp;
-    }
-  if (nn > qn)
-    nn = qn;
-  if (dn > qn)
-    dn = qn;
-
-  if (qn > nn)
-    MPN_ZERO (qp + nn, qn - nn);
-
-  tp = TMP_ALLOC_LIMBS (mpn_bdiv_q_itch (nn, dn));
-  mpn_bdiv_q (qp, np, nn, dp, dn, tp);
-  TMP_FREE;
-}
-
-#else
-
 /* We use the Jebelean's bidirectional exact division algorithm.  This is
    somewhat naively implemented, with equal quotient parts done by 2-adic
    division and truncating division.  Since 2-adic division is faster, it
@@ -109,8 +44,17 @@
    * It makes the msb part 1 or 2 limbs larger than the lsb part, in spite of
      that the latter is faster.  We should at least reverse this, but perhaps
      we should make the lsb part considerably larger.  (How do we tune this?)
+
+   Perhaps we could somehow use 2-adic division for both parts, not as now
+   truncating division for the upper part and 2-adic for the lower part.
 */
 
+
+#include "gmp.h"
+#include "gmp-impl.h"
+#include "longlong.h"
+
+
 mp_size_t
 mpn_divexact_itch (mp_size_t nn, mp_size_t dn)
 {
@@ -132,8 +76,7 @@
   int cnt;
   mp_ptr xdp;
   mp_limb_t di;
-  mp_limb_t cy;
-  gmp_pi1_t dinv;
+  mp_limb_t dip[2], xp[2], cy;
   TMP_DECL;
 
   TMP_MARK;
@@ -147,7 +90,7 @@
       MPN_COPY (tp, np, qn);
       binvert_limb (di, dp[0]);  di = -di;
       dn = MIN (dn, qn);
-      mpn_sbpi1_bdiv_q (qp, tp, qn, dp, dn, di);
+      mpn_sb_bdiv_q (qp, tp, qn, dp, dn, di);
       TMP_FREE;
       return;
     }
@@ -164,14 +107,14 @@
 	  MPN_COPY (tp, np, qn);
 	  binvert_limb (di, dp[0]);  di = -di;
 	  dn = MIN (dn, qn);
-	  mpn_sbpi1_bdiv_q (qp, tp, qn, dp, dn, di);
+	  mpn_sb_bdiv_q (qp, tp, qn, dp, dn, di);
 	}
       else if (BELOW_THRESHOLD (dn, MU_BDIV_Q_THRESHOLD))
 	{
 	  tp = scratch;
 	  MPN_COPY (tp, np, qn);
 	  binvert_limb (di, dp[0]);  di = -di;
-	  mpn_dcpi1_bdiv_q (qp, tp, qn, dp, dn, di);
+	  mpn_dc_bdiv_q (qp, tp, qn, dp, dn, di);
 	}
       else
 	{
@@ -237,14 +180,23 @@
       MPN_COPY (tp, np + nn - nn1, nn1);
     }
 
-  invert_pi1 (dinv, xdp[qn1 - 1], xdp[qn1 - 2]);
   if (BELOW_THRESHOLD (qn1, DC_DIVAPPR_Q_THRESHOLD))
     {
-      qp[qn0 - 1 + nn1 - qn1] = mpn_sbpi1_divappr_q (qp + qn0 - 1, tp, nn1, xdp, qn1, dinv.inv32);
+      /* Compute divisor inverse.  */
+      cy = mpn_add_1 (xp, xdp + qn1 - 2, 2, 1);
+      if (cy != 0)
+	dip[0] = dip[1] = 0;
+      else
+	{
+	  mp_limb_t scratch[10];	/* FIXME */
+	  mpn_invert (dip, xp, 2, scratch);
+	}
+
+      qp[qn0 - 1 + nn1 - qn1] = mpn_sb_divappr_q (qp + qn0 - 1, tp, nn1, xdp, qn1, dip);
     }
   else if (BELOW_THRESHOLD (qn1, MU_DIVAPPR_Q_THRESHOLD))
     {
-      qp[qn0 - 1 + nn1 - qn1] = mpn_dcpi1_divappr_q (qp + qn0 - 1, tp, nn1, xdp, qn1, &dinv);
+      qp[qn0 - 1 + nn1 - qn1] = mpn_dc_divappr_q (qp + qn0 - 1, tp, nn1, xdp, qn1);
     }
   else
     {
@@ -263,12 +215,12 @@
   if (BELOW_THRESHOLD (qn0, DC_BDIV_Q_THRESHOLD))
     {
       MPN_COPY (tp, np, qn0);
-      mpn_sbpi1_bdiv_q (qp, tp, qn0, dp, qn0, di);
+      mpn_sb_bdiv_q (qp, tp, qn0, dp, qn0, di);
     }
   else if (BELOW_THRESHOLD (qn0, MU_BDIV_Q_THRESHOLD))
     {
       MPN_COPY (tp, np, qn0);
-      mpn_dcpi1_bdiv_q (qp, tp, qn0, dp, qn0, di);
+      mpn_dc_bdiv_q (qp, tp, qn0, dp, qn0, di);
     }
   else
     {
@@ -280,4 +232,3 @@
 
   TMP_FREE;
 }
-#endif
--- 1/mpn/generic/divis.c
+++ 2/mpn/generic/divis.c
@@ -4,7 +4,7 @@
    CERTAIN TO BE SUBJECT TO INCOMPATIBLE CHANGES OR DISAPPEAR COMPLETELY IN
    FUTURE GNU MP RELEASES.
 
-Copyright 2001, 2002, 2005, 2009 Free Software Foundation, Inc.
+Copyright 2001, 2002, 2005 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -26,8 +26,8 @@
 #include "longlong.h"
 
 
-/* Determine whether {ap,an} is divisible by {dp,dn}.  Must have both
-   operands normalized, meaning high limbs non-zero, except that an==0 is
+/* Determine whether {ap,asize} is divisible by {dp,dsize}.  Must have both
+   operands normalized, meaning high limbs non-zero, except that asize==0 is
    allowed.
 
    There usually won't be many low zero bits on d, but the checks for this
@@ -36,6 +36,14 @@
 
    Future:
 
+   This is currently not much faster than the user doing an mpz_tdiv_r
+   and testing for a zero remainder, but hopefully it can be improved.
+
+   mpn_bdivmod is one possibility, but it only trades udiv_qrnnd's for
+   multiplies, it won't save crossproducts the way it can in mpz_divexact.
+   Definitely worthwhile on small operands for most processors, but a
+   sub-quadratic version will be wanted before it can be used on all sizes.
+
    Getting the remainder limb by limb would make an early exit possible on
    finding a non-zero.  This would probably have to be bdivmod style so
    there's no addback, but it would need a multi-precision inverse and so
@@ -45,30 +53,37 @@
    just append a low zero limb to "a" rather than bit-shifting as
    mpn_tdiv_qr does internally, so long as it's already been checked that a
    has at least as many trailing zeros bits as d.  Or equivalently, pass
-   qxn==1 to mpn_tdiv_qr, if/when it accepts that.  */
+   qxn==1 to mpn_tdiv_qr, if/when it accepts that.
+
+   When called from mpz_congruent_p, {ap,asize} is a temporary which can be
+   destroyed.  Maybe it'd be possible to get into mpn_tdiv_qr at a lower
+   level to save copying it, or maybe that function could accept rp==ap.
+
+   Could use __attribute__ ((regparm (2))) on i386, so the parameters
+   wouldn't need extra stack when called from mpz_divisible_p, but a
+   pre-release gcc 3 didn't generate particularly good register juggling in
+   that case, so this isn't done for now.  */
 
 int
-mpn_divisible_p (mp_srcptr ap, mp_size_t an,
-		 mp_srcptr dp, mp_size_t dn)
+mpn_divisible_p (mp_srcptr ap, mp_size_t asize,
+		 mp_srcptr dp, mp_size_t dsize)
 {
   mp_limb_t  alow, dlow, dmask;
-  mp_ptr     qp, rp, tp;
+  mp_ptr     qp, rp;
   mp_size_t  i;
-  mp_limb_t di;
-  unsigned  twos;
   TMP_DECL;
 
-  ASSERT (an >= 0);
-  ASSERT (an == 0 || ap[an-1] != 0);
-  ASSERT (dn >= 1);
-  ASSERT (dp[dn-1] != 0);
-  ASSERT_MPN (ap, an);
-  ASSERT_MPN (dp, dn);
+  ASSERT (asize >= 0);
+  ASSERT (asize == 0 || ap[asize-1] != 0);
+  ASSERT (dsize >= 1);
+  ASSERT (dp[dsize-1] != 0);
+  ASSERT_MPN (ap, asize);
+  ASSERT_MPN (dp, dsize);
 
   /* When a<d only a==0 is divisible.
-     Notice this test covers all cases of an==0. */
-  if (an < dn)
-    return (an == 0);
+     Notice this test covers all cases of asize==0. */
+  if (asize < dsize)
+    return (asize == 0);
 
   /* Strip low zero limbs from d, requiring a==0 on those. */
   for (;;)
@@ -82,9 +97,9 @@
       if (alow != 0)
 	return 0;  /* a has fewer low zero limbs than d, so not divisible */
 
-      /* a!=0 and d!=0 so won't get to n==0 */
-      an--; ASSERT (an >= 1);
-      dn--; ASSERT (dn >= 1);
+      /* a!=0 and d!=0 so won't get to size==0 */
+      asize--; ASSERT (asize >= 1);
+      dsize--; ASSERT (dsize >= 1);
       ap++;
       dp++;
     }
@@ -94,88 +109,41 @@
   if ((alow & dmask) != 0)
     return 0;
 
-  if (dn == 1)
+  if (dsize == 1)
     {
-      if (BELOW_THRESHOLD (an, MODEXACT_1_ODD_THRESHOLD))
-	return mpn_mod_1 (ap, an, dlow) == 0;
+      if (BELOW_THRESHOLD (asize, MODEXACT_1_ODD_THRESHOLD))
+	return mpn_mod_1 (ap, asize, dlow) == 0;
 
-      count_trailing_zeros (twos, dlow);
-      dlow >>= twos;
-      return mpn_modexact_1_odd (ap, an, dlow) == 0;
+      if ((dlow & 1) == 0)
+	{
+	  unsigned  twos;
+	  count_trailing_zeros (twos, dlow);
+	  dlow >>= twos;
+	}
+      return mpn_modexact_1_odd (ap, asize, dlow) == 0;
     }
 
-  if (dn == 2)
+  if (dsize == 2)
     {
       mp_limb_t  dsecond = dp[1];
       if (dsecond <= dmask)
 	{
+	  unsigned  twos;
 	  count_trailing_zeros (twos, dlow);
 	  dlow = (dlow >> twos) | (dsecond << (GMP_NUMB_BITS-twos));
 	  ASSERT_LIMB (dlow);
-	  return MPN_MOD_OR_MODEXACT_1_ODD (ap, an, dlow) == 0;
+	  return MPN_MOD_OR_MODEXACT_1_ODD (ap, asize, dlow) == 0;
 	}
     }
 
-  /* Should we compute Q = A * D^(-1) mod B^k,
-                       R = A - Q * D  mod B^k
-     here, for some small values of k?  Then check if R = 0 (mod B^k).  */
-
-  /* We could also compute A' = A mod T and D' = D mod P, for some
-     P = 3 * 5 * 7 * 11 ..., and then check if any prime factor from P
-     dividing D' also divides A'.  */
-
   TMP_MARK;
 
-  rp = TMP_ALLOC_LIMBS (an + 1);
-  qp = TMP_ALLOC_LIMBS (an - dn + 1); /* FIXME: Could we avoid this */
+  rp = TMP_ALLOC_LIMBS (asize+1);
+  qp = rp + dsize;
 
-  count_trailing_zeros (twos, dp[0]);
-
-  if (twos != 0)
-    {
-      tp = TMP_ALLOC_LIMBS (dn);
-      ASSERT_NOCARRY (mpn_rshift (tp, dp, dn, twos));
-      dp = tp;
-
-      ASSERT_NOCARRY (mpn_rshift (rp, ap, an, twos));
-    }
-  else
-    {
-      MPN_COPY (rp, ap, an);
-    }
-  if (rp[an - 1] >= dp[dn - 1])
-    {
-      rp[an] = 0;
-      an++;
-    }
-  else if (an == dn)
-    {
-      TMP_FREE;
-      return 0;
-    }
-
-  ASSERT (an > dn);		/* requirement of functions below */
-
-  if (BELOW_THRESHOLD (dn, DC_BDIV_QR_THRESHOLD) ||
-      BELOW_THRESHOLD (an - dn, DC_BDIV_QR_THRESHOLD))
-    {
-      binvert_limb (di, dp[0]);
-      mpn_sbpi1_bdiv_qr (qp, rp, an, dp, dn, -di);
-      rp += an - dn;
-    }
-  else if (BELOW_THRESHOLD (dn, MU_BDIV_QR_THRESHOLD))
-    {
-      binvert_limb (di, dp[0]);
-      mpn_dcpi1_bdiv_qr (qp, rp, an, dp, dn, -di);
-      rp += an - dn;
-    }
-  else
-    {
-      tp = TMP_ALLOC_LIMBS (mpn_mu_bdiv_qr_itch (an, dn));
-      mpn_mu_bdiv_qr (qp, rp, rp, an, dp, dn, tp);
-    }
+  mpn_tdiv_qr (qp, rp, (mp_size_t) 0, ap, asize, dp, dsize);
 
-  /* test for {rp,dn} zero or non-zero */
+  /* test for {rp,dsize} zero or non-zero */
   i = 0;
   do
     {
@@ -185,7 +153,7 @@
 	  return 0;
 	}
     }
-  while (++i < dn);
+  while (++i < dsize);
 
   TMP_FREE;
   return 1;
--- 1/mpn/generic/divrem.c
+++ 2/mpn/generic/divrem.c
@@ -47,7 +47,7 @@
       TMP_DECL;
 
       TMP_MARK;
-      q2p = TMP_ALLOC_LIMBS (nn + qxn);
+      q2p = (mp_ptr) TMP_ALLOC ((nn + qxn) * BYTES_PER_MP_LIMB);
 
       np[0] = mpn_divrem_1 (q2p, qxn, np, nn, dp[0]);
       qn = nn + qxn - 1;
@@ -72,11 +72,11 @@
       if (UNLIKELY (qxn != 0))
 	{
 	  mp_ptr n2p;
-	  n2p = TMP_ALLOC_LIMBS (nn + qxn);
+	  n2p = (mp_ptr) TMP_ALLOC ((nn + qxn) * BYTES_PER_MP_LIMB);
 	  MPN_ZERO (n2p, qxn);
 	  MPN_COPY (n2p + qxn, np, nn);
-	  q2p = TMP_ALLOC_LIMBS (nn - dn + qxn + 1);
-	  rp = TMP_ALLOC_LIMBS (dn);
+	  q2p = (mp_ptr) TMP_ALLOC ((nn - dn + qxn + 1) * BYTES_PER_MP_LIMB);
+	  rp = (mp_ptr) TMP_ALLOC (dn * BYTES_PER_MP_LIMB);
 	  mpn_tdiv_qr (q2p, rp, 0L, n2p, nn + qxn, dp, dn);
 	  MPN_COPY (np, rp, dn);
 	  qn = nn - dn + qxn;
@@ -85,8 +85,8 @@
 	}
       else
 	{
-	  q2p = TMP_ALLOC_LIMBS (nn - dn + 1);
-	  rp = TMP_ALLOC_LIMBS (dn);
+	  q2p = (mp_ptr) TMP_ALLOC ((nn - dn + 1) * BYTES_PER_MP_LIMB);
+	  rp = (mp_ptr) TMP_ALLOC (dn * BYTES_PER_MP_LIMB);
 	  mpn_tdiv_qr (q2p, rp, 0L, np, nn, dp, dn);
 	  MPN_COPY (np, rp, dn);	/* overwrite np area with remainder */
 	  qn = nn - dn;
--- 1/mpn/generic/fib2_ui.c
+++ 2/mpn/generic/fib2_ui.c
@@ -4,7 +4,7 @@
    CERTAIN TO BE SUBJECT TO INCOMPATIBLE CHANGES OR DISAPPEAR COMPLETELY IN
    FUTURE GNU MP RELEASES.
 
-Copyright 2001, 2002, 2005, 2009 Free Software Foundation, Inc.
+Copyright 2001, 2002, 2005 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -24,6 +24,8 @@
 #include <stdio.h>
 #include "gmp.h"
 #include "gmp-impl.h"
+#include "longlong.h"
+
 
 /* change this to "#define TRACE(x) x" for diagnostics */
 #define TRACE(x)
@@ -50,13 +52,20 @@
    This property of F[4m+3] can be verified by induction on F[4m+3] =
    7*F[4m-1] - F[4m-5], that formula being a standard lucas sequence
    identity U[i+j] = U[i]*V[j] - U[i-j]*Q^j.
-*/
+
+   Enhancements:
+
+   If there was an mpn_addlshift, it'd be possible to eliminate the yp
+   temporary, using xp=F[k]^2, fp=F[k-1]^2, f1p=xp+fp, fp+=4*fp, fp-=f1p,
+   fp+=2*(-1)^n, etc.  */
 
 mp_size_t
 mpn_fib2_ui (mp_ptr fp, mp_ptr f1p, unsigned long int n)
 {
+  mp_ptr         xp, yp;
   mp_size_t      size;
   unsigned long  nfirst, mask;
+  TMP_DECL;
 
   TRACE (printf ("mpn_fib2_ui n=%lu\n", n));
 
@@ -76,15 +85,15 @@
   if (mask != 1)
     {
       mp_size_t  alloc;
-      mp_ptr        xp;
-      TMP_DECL;
 
       TMP_MARK;
       alloc = MPN_FIB2_SIZE (n);
-      xp = TMP_ALLOC_LIMBS (alloc);
+      TMP_ALLOC_LIMBS_2 (xp,alloc, yp,alloc);
 
       do
 	{
+	  mp_limb_t  c;
+
 	  /* Here fp==F[k] and f1p==F[k-1], with k being the bits of n from
 	     n&mask upwards.
 
@@ -106,64 +115,44 @@
 	     worth bothering checking for it */
 	  ASSERT (alloc >= 2*size);
 	  mpn_sqr_n (xp, fp,  size);
-	  mpn_sqr_n (fp, f1p, size);
+	  mpn_sqr_n (yp, f1p, size);
 	  size *= 2;
 
 	  /* Shrink if possible.  Since fp was normalized there'll be at
 	     most one high zero on xp (and if there is then there's one on
 	     yp too).  */
-	  ASSERT (xp[size-1] != 0 || fp[size-1] == 0);
+	  ASSERT (xp[size-1] != 0 || yp[size-1] == 0);
 	  size -= (xp[size-1] == 0);
 	  ASSERT (xp[size-1] != 0);  /* only one xp high zero */
 
-	  /* Calculate F[2k-1] = F[k]^2 + F[k-1]^2. */
-	  f1p[size] = mpn_add_n (f1p, xp, fp, size);
-
 	  /* Calculate F[2k+1] = 4*F[k]^2 - F[k-1]^2 + 2*(-1)^k.
 	     n&mask is the low bit of our implied k.  */
-#if HAVE_NATIVE_mpn_rsblsh2_n || HAVE_NATIVE_mpn_rsblsh_n
-#if HAVE_NATIVE_mpn_rsblsh2_n
-	  fp[size] = mpn_rsblsh2_n (fp, fp, xp, size);
-#else /* HAVE_NATIVE_mpn_rsblsh_n */
-	  fp[size] = mpn_rsblsh_n (fp, fp, xp, size, 2);
-#endif
-	  if ((n & mask) == 0)
-	    MPN_INCR_U(fp, size + 1, 2);	/* possible +2 */
-	  else
-	  {
-	    ASSERT (fp[0] >= 2);
-	    fp[0] -= 2;				/* possible -2 */
-	  }
-#else
-	  {
-	    mp_limb_t  c;
-
-	    c = mpn_lshift (xp, xp, size, 2);
-	    xp[0] |= (n & mask ? 0 : 2);	/* possible +2 */
-	    c -= mpn_sub_n (fp, xp, fp, size);
-	    ASSERT (n & mask ? fp[0] != 0 && fp[0] != 1 : 1);
-	    fp[0] -= (n & mask ? 2 : 0);	/* possible -2 */
-	    fp[size] = c;
-	  }
-#endif
+	  c = mpn_lshift (fp, xp, size, 2);
+	  fp[0] |= (n & mask ? 0 : 2);	 /* possible +2 */
+	  c -= mpn_sub_n (fp, fp, yp, size);
+	  ASSERT (n & (mask << 1) ? fp[0] != 0 && fp[0] != 1 : 1);
+	  fp[0] -= (n & mask ? 2 : 0);	 /* possible -2 */
 	  ASSERT (alloc >= size+1);
-	  size += (fp[size] != 0);
+	  xp[size] = 0;
+	  yp[size] = 0;
+	  fp[size] = c;
+	  size += (c != 0);
+
+	  /* Calculate F[2k-1] = F[k]^2 + F[k-1]^2.
+	     F[2k-1]<F[2k+1] so no carry out of "size" limbs. */
+	  ASSERT_NOCARRY (mpn_add_n (f1p, xp, yp, size));
 
 	  /* now n&mask is the new bit of n being considered */
 	  mask >>= 1;
 
 	  /* Calculate F[2k] = F[2k+1] - F[2k-1], replacing the unwanted one of
 	     F[2k+1] and F[2k-1].  */
-	  if (n & mask)
-	    ASSERT_NOCARRY (mpn_sub_n (f1p, fp, f1p, size));
-	  else {
-	    ASSERT_NOCARRY (mpn_sub_n ( fp, fp, f1p, size));
-
-	    /* Can have a high zero after replacing F[2k+1] with F[2k].
-	       f1p will have a high zero if fp does. */
-	    ASSERT (fp[size-1] != 0 || f1p[size-1] == 0);
-	    size -= (fp[size-1] == 0);
-	  }
+	  ASSERT_NOCARRY (mpn_sub_n ((n & mask ? f1p : fp), fp, f1p, size));
+
+	  /* Can have a high zero after replacing F[2k+1] with F[2k].
+	     f1p will have a high zero if fp does. */
+	  ASSERT (fp[size-1] != 0 || f1p[size-1] == 0);
+	  size -= (fp[size-1] == 0);
 	}
       while (mask != 1);
 
--- 1/mpn/generic/gcd_1.c
+++ 2/mpn/generic/gcd_1.c
@@ -21,17 +21,6 @@
 #include "gmp-impl.h"
 #include "longlong.h"
 
-#ifndef GCD_1_METHOD
-#define GCD_1_METHOD 2
-#endif
-
-#define USE_ZEROTAB 0
-
-#if USE_ZEROTAB
-static const unsigned char zerotab[16] = {
-  4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0
-};
-#endif
 
 /* Does not work for U == 0 or V == 0.  It would be tough to make it work for
    V == 0 since gcd(x,0) = x, and U does not generally fit in an mp_limb_t.
@@ -92,10 +81,6 @@
       goto strip_u_maybe;
     }
 
-  ASSERT (ulimb & 1);
-  ASSERT (vlimb & 1);
-
-#if GCD_1_METHOD == 1
   while (ulimb != vlimb)
     {
       ASSERT (ulimb & 1);
@@ -124,55 +109,6 @@
 	  while ((vlimb & 1) == 0);
 	}
     }
-#else
-# if GCD_1_METHOD  == 2
-
-  ulimb >>= 1;
-  vlimb >>= 1;
-
-  while (ulimb != vlimb)
-    {
-      int c;
-      mp_limb_t t = ulimb - vlimb;
-      mp_limb_t vgtu = LIMB_HIGHBIT_TO_MASK (t);
-
-      /* v <-- min (u, v) */
-      vlimb += (vgtu & t);
-
-      /* u <-- |u - v| */
-      ulimb = (t ^ vgtu) - vgtu;
-
-#if USE_ZEROTAB
-      /* Number of trailing zeros is the same no matter if we look at
-       * t or ulimb, but using t gives more parallelism. */
-      c = zerotab[t & 15];
-
-      while (UNLIKELY (c == 4))
-	{
-	  ulimb >>= 4;
-	  if (0)
-	  strip_u_maybe:
-	    vlimb >>= 1;
-
-	  c = zerotab[ulimb & 15];
-	}
-#else
-      if (0)
-	{
-	strip_u_maybe:
-	  vlimb >>= 1;
-	  t = ulimb;
-	}
-      count_trailing_zeros (c, t);
-#endif
-      ulimb >>= (c + 1);
-    }
-
-  vlimb = (vlimb << 1) | 1;
-# else
-#  error Unknown GCD_1_METHOD
-# endif
-#endif
 
  done:
   return vlimb << zero_bits;
--- 1/mpn/generic/gcdext_1.c
+++ 2/mpn/generic/gcdext_1.c
@@ -22,242 +22,6 @@
 #include "gmp-impl.h"
 #include "longlong.h"
 
-#ifndef GCDEXT_1_USE_BINARY
-#define GCDEXT_1_USE_BINARY 0
-#endif
-
-#ifndef GCDEXT_1_BINARY_METHOD
-#define GCDEXT_1_BINARY_METHOD 2
-#endif
-
-#ifndef USE_ZEROTAB
-#define USE_ZEROTAB 1
-#endif
-
-#if GCDEXT_1_USE_BINARY
-
-#if USE_ZEROTAB
-static unsigned char zerotab[0x40] = {
-  6, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
-  4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
-  5, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0,
-  4, 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0
-};
-#endif
-
-mp_limb_t
-mpn_gcdext_1 (mp_limb_signed_t *sp, mp_limb_signed_t *tp,
-	      mp_limb_t u, mp_limb_t v)
-{
-  /* Maintain
-
-     U = t1 u + t0 v
-     V = s1 u + s0 v
-
-     where U, V are the inputs (without any shared power of two),
-     and the matris has determinant ± 2^{shift}.
-  */
-  mp_limb_t s0 = 1;
-  mp_limb_t t0 = 0;
-  mp_limb_t s1 = 0;
-  mp_limb_t t1 = 1;
-  mp_limb_t ug;
-  mp_limb_t vg;
-  mp_limb_t ugh;
-  mp_limb_t vgh;
-  unsigned zero_bits;
-  unsigned shift;
-  unsigned i;
-#if GCDEXT_1_BINARY_METHOD == 2
-  mp_limb_t det_sign;
-#endif
-
-  ASSERT (u > 0);
-  ASSERT (v > 0);
-
-  count_trailing_zeros (zero_bits, u | v);
-  u >>= zero_bits;
-  v >>= zero_bits;
-
-  if ((u & 1) == 0)
-    {
-      count_trailing_zeros (shift, u);
-      u >>= shift;
-      t1 <<= shift;
-    }
-  else if ((v & 1) == 0)
-    {
-      count_trailing_zeros (shift, v);
-      v >>= shift;
-      s0 <<= shift;
-    }
-  else
-    shift = 0;
-
-#if GCDEXT_1_BINARY_METHOD == 1
-  while (u != v)
-    {
-      unsigned count;
-      if (u > v)
-	{
-	  u -= v;
-#if USE_ZEROTAB
-	  count = zerotab [u & 0x3f];
-	  u >>= count;
-	  if (UNLIKELY (count == 6))
-	    {
-	      unsigned c;
-	      do
-		{
-		  c = zerotab[u & 0x3f];
-		  u >>= c;
-		  count += c;
-		}
-	      while (c == 6);
-	    }
-#else
-	  count_trailing_zeros (count, u);
-	  u >>= count;
-#endif
-	  t0 += t1; t1 <<= count;
-	  s0 += s1; s1 <<= count;
-	}
-      else
-	{
-	  v -= u;
-#if USE_ZEROTAB
-	  count = zerotab [v & 0x3f];
-	  v >>= count;
-	  if (UNLIKELY (count == 6))
-	    {
-	      unsigned c;
-	      do
-		{
-		  c = zerotab[v & 0x3f];
-		  v >>= c;
-		  count += c;
-		}
-	      while (c == 6);
-	    }
-#else
-	  count_trailing_zeros (count, v);
-	  v >>= count;
-#endif
-	  t1 += t0; t0 <<= count;
-	  s1 += s0; s0 <<= count;
-	}
-      shift += count;
-    }
-#else
-# if GCDEXT_1_BINARY_METHOD == 2
-  u >>= 1;
-  v >>= 1;
-
-  det_sign = 0;
-
-  while (u != v)
-    {
-      unsigned count;
-      mp_limb_t d =  u - v;
-      mp_limb_t vgtu = LIMB_HIGHBIT_TO_MASK (d);
-      mp_limb_t sx;
-      mp_limb_t tx;
-
-      /* When v <= u (vgtu == 0), the updates are:
-
-	   (u; v)   <-- ( (u - v) >> count; v)    (det = +(1<<count) for corr. M factor)
-	   (t1, t0) <-- (t1 << count, t0 + t1)
-
-	 and when v > 0, the updates are
-
-	   (u; v)   <-- ( (v - u) >> count; u)    (det = -(1<<count))
-	   (t1, t0) <-- (t0 << count, t0 + t1)
-
-	 and similarly for s1, s0
-      */
-
-      /* v <-- min (u, v) */
-      v += (vgtu & d);
-
-      /* u <-- |u - v| */
-      u = (d ^ vgtu) - vgtu;
-
-      /* Number of trailing zeros is the same no matter if we look at
-       * d or u, but using d gives more parallelism. */
-#if USE_ZEROTAB
-      count = zerotab[d & 0x3f];
-      if (UNLIKELY (count == 6))
-	{
-	  unsigned c = 6;
-	  do
-	    {
-	      d >>= c;
-	      c = zerotab[d & 0x3f];
-	      count += c;
-	    }
-	  while (c == 6);
-	}
-#else
-      count_trailing_zeros (count, d);
-#endif
-      det_sign ^= vgtu;
-
-      tx = vgtu & (t0 - t1);
-      sx = vgtu & (s0 - s1);
-      t0 += t1;
-      s0 += s1;
-      t1 += tx;
-      s1 += sx;
-
-      count++;
-      u >>= count;
-      t1 <<= count;
-      s1 <<= count;
-      shift += count;
-    }
-  u = (u << 1) + 1;
-# else /* GCDEXT_1_BINARY_METHOD == 2 */
-#  error Unknown GCDEXT_1_BINARY_METHOD
-# endif
-#endif
-
-  /* Now u = v = g = gcd (u,v). Compute U/g and V/g */
-  ug = t0 + t1;
-  vg = s0 + s1;
-
-  ugh = ug/2 + (ug & 1);
-  vgh = vg/2 + (vg & 1);
-
-  /* Now ±2^{shift} g = s0 U - t0 V. Get rid of the power of two, using
-     s0 U - t0 V = (s0 + V/g) U - (t0 + U/g) V. */
-  for (i = 0; i < shift; i++)
-    {
-      mp_limb_t mask = - ( (s0 | t0) & 1);
-
-      s0 /= 2;
-      t0 /= 2;
-      s0 += mask & vgh;
-      t0 += mask & ugh;
-    }
-  /* FIXME: Try simplifying this condition. */
-  if ( (s0 > 1 && 2*s0 >= vg) || (t0 > 1 && 2*t0 >= ug) )
-    {
-      s0 -= vg;
-      t0 -= ug;
-    }
-#if GCDEXT_1_BINARY_METHOD == 2
-  /* Conditional negation. */
-  s0 = (s0 ^ det_sign) - det_sign;
-  t0 = (t0 ^ det_sign) - det_sign;
-#endif
-  *sp = s0;
-  *tp = -t0;
-
-  return u << zero_bits;
-}
-
-#else /* !GCDEXT_1_USE_BINARY */
-
 
 /* FIXME: Takes two single-word limbs. It could be extended to a
  * function that accepts a bignum for the first input, and only
@@ -315,4 +79,3 @@
       v1 -= q * v0;
     }
 }
-#endif /* !GCDEXT_1_USE_BINARY */
--- 1/mpn/generic/gcdext.c
+++ 2/mpn/generic/gcdext.c
@@ -118,7 +118,6 @@
     mpn_mul (tp, up, size, ap, an);
 
   size += an;
-  size -= tp[size - 1] == 0;
 
   ASSERT (gn <= size);
 
@@ -147,9 +146,21 @@
   vn = size + 1 - bn;
   ASSERT (vn <= n + 1);
 
-  mpn_divexact (vp, tp, size, bp, bn);
+  /* FIXME: Use divexact. Or do the entire calculation mod 2^{n *
+     GMP_NUMB_BITS}. */
+  mpn_tdiv_qr (vp, tp, 0, tp, size, bp, bn);
   vn -= (vp[vn-1] == 0);
 
+  /* Remainder must be zero */
+#if WANT_ASSERT
+  {
+    mp_size_t i;
+    for (i = 0; i < bn; i++)
+      {
+	ASSERT (tp[i] == 0);
+      }
+  }
+#endif
   return vn;
 }
 
@@ -170,8 +181,7 @@
    For the lehmer call after the loop, Let T denote
    GCDEXT_DC_THRESHOLD. For the gcdext_lehmer call, we need T each for
    u, a and b, and 4T+3 scratch space. Next, for compute_v, we need T
-   for u, T+1 for v and 2T + 1 scratch space. In all, 7T + 3 is
-   sufficient for both operations.
+   + 1 for v and 2T + 1 scratch space. In all, 7T + 3 is sufficient.
 
 */
 
--- 1/mpn/generic/get_d.c
+++ 2/mpn/generic/get_d.c
@@ -4,7 +4,7 @@
    CERTAIN TO BE SUBJECT TO INCOMPATIBLE CHANGES OR DISAPPEAR COMPLETELY IN
    FUTURE GNU MP RELEASES.
 
-Copyright 2003, 2004, 2007, 2009 Free Software Foundation, Inc.
+Copyright 2003, 2004 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -124,10 +124,10 @@
    Other:
 
    For reference, note that HPPA 8000, 8200, 8500 and 8600 trap FCNV,UDW,DBL
-   to the kernel for values >= 2^63.  This makes it slow, and worse the kernel
-   Linux (what versions?) apparently uses untested code in its trap handling
-   routines, and gets the sign wrong.  We don't use such a limb-to-double
-   cast, neither in the IEEE or generic code.  */
+   to the kernel for values >= 2^63.  This makes it slow, and worse the
+   Linux kernel (what versions?) apparently uses untested code in its trap
+   handling routines, and gets the sign wrong.  We don't use such a limb to
+   double cast, neither in the IEEE or generic code.  */
 
 
 double
--- 1/mpn/generic/gmp-mparam.h
+++ 2/mpn/generic/gmp-mparam.h
@@ -18,5 +18,5 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-/* Values for GMP_LIMB_BITS etc will be determined by ./configure and put
+/* Values for BITS_PER_MP_LIMB etc will be determined by ./configure and put
    in config.h. */
--- 1/mpn/generic/hgcd.c
+++ 2/mpn/generic/hgcd.c
@@ -383,7 +383,7 @@
     }
 }
 
-/* Multiply M by M1 from the right. Needs 3*(M->n + M1->n) + 5 limbs
+/* Multiply M by M1 from the right. Needs 4*(M->n + M1->n) + 5 limbs
    of temporary storage (see mpn_matrix22_mul_itch). */
 void
 mpn_hgcd_matrix_mul (struct hgcd_matrix *M, const struct hgcd_matrix *M1,
@@ -395,13 +395,7 @@
      are > 0, no element can decrease. The new elements are of size
      M->n + M1->n, one limb more or less. The computation of the
      matrix product produces elements of size M->n + M1->n + 1. But
-     the true size, after normalization, may be three limbs smaller.
-
-     The reason that the product has normalized size >= M->n + M1->n -
-     2 is subtle. It depends on the fact that M and M1 can be factored
-     as products of (1,1; 0,1) and (1,0; 1,1), and that we can't have
-     M ending with a large power and M1 starting with a large power of
-     the same matrix. */
+     the true size, after normalization, may be three limbs smaller. */
 
   /* FIXME: Strassen multiplication gives only a small speedup. In FFT
      multiplication range, this function could be sped up quite a lot
@@ -511,15 +505,16 @@
 
    Let S(r) denote the required storage. For M1 we need 4 * (ceil(n1/2) + 1)
    = 4 * (ceil(n/4) + 1), for the hgcd_matrix_adjust call, we need n + 2,
-   and for the hgcd_matrix_mul, we may need 3 ceil(n/2) + 8. In total,
-   4 * ceil(n/4) + 3 ceil(n/2) + 12 <= 10 ceil(n/4) + 12.
+   and for the hgcd_matrix_mul, we may need 4 ceil(n/2) + 1. In total,
+   4 * ceil(n/4) + 4 ceil(n/2) + 5 <= 12 ceil(n/4) + 5.
 
    For the recursive call, we need S(n1) = S(ceil(n/2)).
 
-   S(n) <= 10*ceil(n/4) + 12 + S(ceil(n/2))
-	<= 10*(ceil(n/4) + ... + ceil(n/2^(1+k))) + 12k + S(ceil(n/2^k))
-	<= 10*(2 ceil(n/4) + k) + 12k + S(ceil(n/2^k))
-	<= 20 ceil(n/4) + 22k + S(ceil(n/2^k))
+   S(n) <= 12*ceil(n/4) + 5 + S(ceil(n/2))
+	<= 12*(ceil(n/4) + ... + ceil(n/2^(1+k))) + 5k + S(ceil(n/2^k))
+	<= 12*(2 ceil(n/4) + k) + 5k + S(n/2^k)
+	<= 24 ceil(n/4) + 17k + S(n/2^k)
+
 */
 
 mp_size_t
@@ -537,7 +532,7 @@
   count_leading_zeros (count, nscaled);
   k = GMP_LIMB_BITS - count;
 
-  return 20 * ((n+3) / 4) + 22 * k
+  return 24 * ((n+3) / 4) + 17 * k
     + MPN_HGCD_LEHMER_ITCH (HGCD_THRESHOLD);
 }
 
@@ -612,18 +607,7 @@
 	  /* Needs 2 (p + M->n) <= 2 (2*s - n2 + 1 + n2 - s - 1)
 	     = 2*s <= 2*(floor(n/2) + 1) <= n + 2. */
 	  n = mpn_hgcd_matrix_adjust (&M1, p + nn, ap, bp, p, tp + scratch);
-
-	  /* We need a bound for of M->n + M1.n. Let n be the original
-	     input size. Then
-
-	       ceil(n/2) - 1 >= size of product >= M.n + M1.n - 2
-
-	     and it follows that
-
-	       M.n + M1.n <= ceil(n/2) + 1
-
-	     Then 3*(M.n + M1.n) + 5 <= 3 * ceil(n/2) + 8 is the
-	     amount of needed scratch space. */
+	  /* Needs 4 ceil(n/2) + 1 */
 	  mpn_hgcd_matrix_mul (M, &M1, tp + scratch);
 	  success = 1;
 	}
--- 1/mpn/generic/invert.c
+++ 2/mpn/generic/invert.c
@@ -1,12 +1,6 @@
-/* invert.c -- Compute floor((B^{2n}-1)/U).
+/* Compute {up,n}^(-1).
 
-   Contributed to the GNU project by Marco Bodrato.
-
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
-
-Copyright (C) 2007, 2009 Free Software Foundation, Inc.
+Copyright (C) 2007 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -23,61 +17,44 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-/* FIXME: Remove NULL and TMP_*, as soon as all the callers properly
-   allocate and pass the scratch to the function. */
-#include <stdlib.h>		/* for NULL */
-
+#include <stdlib.h>
 #include "gmp.h"
 #include "gmp-impl.h"
-#include "longlong.h"
 
-#ifndef INV_APPR_THRESHOLD
-#define INV_APPR_THRESHOLD (INV_NEWTON_THRESHOLD)
-#endif
+/* Formulas:
+	z = 2z-(zz)d
+	z = 2z-(zd)z
+	z = z(2-zd)
+	z = z-z*(zd-1)
+	z = z+z*(1-zd)
+*/
+
+mp_size_t
+mpn_invert_itch (mp_size_t n)
+{
+  return 3 * n + 2;
+}
 
 void
 mpn_invert (mp_ptr ip, mp_srcptr dp, mp_size_t n, mp_ptr scratch)
 {
-  ASSERT (n > 0);
-  ASSERT (dp[n-1] & GMP_LIMB_HIGHBIT);
-  ASSERT (! MPN_OVERLAP_P (ip, n, dp, n));
-  ASSERT (! MPN_OVERLAP_P (ip, n, scratch, mpn_invertappr_itch(n)));
-  ASSERT (! MPN_OVERLAP_P (dp, n, scratch, mpn_invertappr_itch(n)));
-
-  if (n == 1)
-    invert_limb (*ip, *dp);
-  else {
-    TMP_DECL;
-
-    TMP_MARK;
-    if (scratch == NULL)
+  mp_ptr np, rp;
+  mp_size_t i;
+  TMP_DECL;
+
+  TMP_MARK;
+  if (scratch == NULL)
+    {
       scratch = TMP_ALLOC_LIMBS (mpn_invert_itch (n));
-
-    if (BELOW_THRESHOLD (n, INV_APPR_THRESHOLD)) {
-      /* Maximum scratch needed by this branch: 3*n + 2 */
-      mp_size_t i;
-      mp_ptr xp;
-
-      xp = scratch + n + 2;				/* 2 * n limbs */
-      for (i = n - 1; i >= 0; i--)
-	xp[i] = ~CNST_LIMB(0);
-      mpn_com_n (xp + n, dp, n);
-      mpn_tdiv_qr (scratch, ip, 0, xp, 2 * n, dp, n);
-      MPN_COPY (ip, scratch, n);
-    } else { /* Use approximated inverse; correct the result if needed. */
-      mp_limb_t e; /* The possible error in the approximate inverse */
-
-      ASSERT ( mpn_invert_itch (n) >= mpn_invertappr_itch (n) );
-      e = mpn_ni_invertappr (ip, dp, n, scratch);
-
-      if (e) { /* Assume the error can only be "0" (no error) or "1". */
-	/* Code to detect and correct the "off by one" approximation. */
-	mpn_mul_n (scratch, ip, dp, n);
-	ASSERT_NOCARRY (mpn_add_n (scratch + n, scratch + n, dp, n));
-	if (! mpn_add (scratch, scratch, 2*n, dp, n))
-	  MPN_INCR_U (ip, n, 1); /* The value was wrong, correct it.  */
-      }
     }
-    TMP_FREE;
-  }
+
+  np = scratch;					/* 2 * n limbs */
+  rp = scratch + 2 * n;				/* n + 2 limbs */
+  for (i = n - 1; i >= 0; i--)
+    np[i] = ~CNST_LIMB(0);
+  mpn_com_n (np + n, dp, n);
+  mpn_tdiv_qr (rp, ip, 0L, np, 2 * n, dp, n);
+  MPN_COPY (ip, rp, n);
+
+  TMP_FREE;
 }
--- 1/mpn/generic/matrix22_mul.c
+++ 2/mpn/generic/matrix22_mul.c
@@ -1,12 +1,10 @@
 /* matrix22_mul.c.
 
-   Contributed by Niels Möller and Marco Bodrato.
-
    THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
    SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
    GUARANTEED THAT THEY'LL CHANGE OR DISAPPEAR IN A FUTURE GNU MP RELEASE.
 
-Copyright 2003, 2004, 2005, 2008, 2009 Free Software Foundation, Inc.
+Copyright 2003, 2004, 2005, 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -72,198 +70,143 @@
       || BELOW_THRESHOLD (mn, MATRIX22_STRASSEN_THRESHOLD))
     return 3*rn + 2*mn;
   else
-    return 3*(rn + mn) + 5;
+    return 4*(rn + mn) + 5;
 }
 
 /* Algorithm:
 
     / s0 \   /  1  0  0  0 \ / r0 \
-    | s1 |   |  0  1  0  1 | | r1 |
-    | s2 |   |  0  0 -1  1 | | r2 |
-    | s3 | = |  0  1 -1  1 | \ r3 /
-    | s4 |   | -1  1 -1  1 |
-    | s5 |   |  0  1  0  0 |
-    \ s6 /   \  0  0  1  0 /
+    | s1 |   |  0  1  0  0 | | r1 |
+    | s2 |   |  0  0  1  1 | | r2 |
+    | s3 | = | -1  0  1  1 | \ r3 /
+    | s4 |   |  1  0 -1  0 |
+    | s5 |   |  1  1 -1 -1 |
+    \ s6 /   \  0  0  0  1 /
 
     / t0 \   /  1  0  0  0 \ / m0 \
-    | t1 |   |  0  1  0  1 | | m1 |
-    | t2 |   |  0  0 -1  1 | | m2 |
-    | t3 | = |  0  1 -1  1 | \ m3 /
-    | t4 |   | -1  1 -1  1 |
-    | t5 |   |  0  1  0  0 |
-    \ t6 /   \  0  0  1  0 /
-
-  Note: the two matrices above are the same, but s_i and t_i are used
-  in the same product, only for i<4, see "A Strassen-like Matrix
-  Multiplication suited for squaring and highest power computation" by
-  M. Bodrato (2008).
-
-    / r0 \   / 1 0  0  0  0  1  0 \ / s0*t0 \
-    | r1 | = | 0 0 -1  1 -1  1  0 | | s1*t1 |
-    | r2 |   | 0 1  0 -1  0 -1 -1 | | s2*t2 |
-    \ r3 /   \ 0 1  1 -1  0 -1  0 / | s3*t3 |
-				    | s4*t5 |
-				    | s5*t6 |
-				    \ s6*t4 /
-
-  The scheduling uses two temporaries U0 and U1 to store products, and
-  two, S0 and T0, to store combinations of entries of the two
-  operands.
+    | t1 |   |  0  0  1  0 | | m1 |
+    | t2 |   | -1  1  0  0 | | m2 |
+    | t3 | = |  1 -1  0  1 | \ m3 /
+    | t4 |   |  0 -1  0  1 |
+    | t5 |   |  0  0  0  1 |
+    \ t6 /   \ -1  1  1 -1 /
+
+    / r0 \   / 1 1 0 0 0 0 0 \ / s0 * t0 \
+    | r1 | = | 1 0 1 1 0 1 0 | | s1 * t1 |
+    | r2 |   | 1 0 0 1 1 0 1 | | s2 * t2 |
+    \ r3 /   \ 1 0 1 1 1 0 0 / | s3 * t3 |
+			       | s4 * t4 |
+			       | s5 * t5 |
+			       \ s6 * t6 /
 */
 
 /* Computes R = R * M. Elements are numbers R = (r0, r1; r2, r3).
  *
  * Resulting elements are of size up to rn + mn + 1.
  *
- * Temporary storage: 3 rn + 3 mn + 5. */
+ * Temporary storage: 4 rn + 4 mn + 5. */
 void
 mpn_matrix22_mul_strassen (mp_ptr r0, mp_ptr r1, mp_ptr r2, mp_ptr r3, mp_size_t rn,
 			   mp_srcptr m0, mp_srcptr m1, mp_srcptr m2, mp_srcptr m3, mp_size_t mn,
 			   mp_ptr tp)
 {
-  mp_ptr s0, t0, u0, u1;
-  int r1s, r3s, s0s, t0s, u1s;
-  s0 = tp; tp += rn + 1;
-  t0 = tp; tp += mn + 1;
+  mp_ptr s2, s3, t2, t3, u0, u1;
+  int r2s, r3s, s3s, t2s, t3s, u0s, u1s;
+  s2 = tp; tp += rn;
+  s3 = tp; tp += rn + 1;
+  t2 = tp; tp += mn;
+  t3 = tp; tp += mn + 1;
   u0 = tp; tp += rn + mn + 1;
   u1 = tp; /* rn + mn + 2 */
 
-  MUL (u0, r1, rn, m2, mn);		/* u5 = s5 * t6 */
-  r3s = abs_sub_n (r3, r3, r2, rn);	/* r3 - r2 */
-  if (r3s)
-    {
-      r1s = abs_sub_n (r1, r1, r3, rn);
-      r1[rn] = 0;
-    }
-  else
-    {
-      r1[rn] = mpn_add_n (r1, r1, r3, rn);
-      r1s = 0;				/* r1 - r2 + r3  */
-    }
-  if (r1s)
-    {
-      s0[rn] = mpn_add_n (s0, r1, r0, rn);
-      s0s = 0;
-    }
-  else if (r1[rn] != 0)
-    {
-      s0[rn] = r1[rn] - mpn_sub_n (s0, r1, r0, rn);
-      s0s = 1;				/* s4 = -r0 + r1 - r2 + r3 */
-      					/* Reverse sign! */
-    }
-  else
-    {
-      s0s = abs_sub_n (s0, r0, r1, rn);
-      s0[rn] = 0;
-    }
-  MUL (u1, r0, rn, m0, mn);		/* u0 = s0 * t0 */
-  r0[rn+mn] = mpn_add_n (r0, u0, u1, rn + mn);
-  ASSERT (r0[rn+mn] < 2);		/* u0 + u5 */
+  MUL (u0, r0, rn, m0, mn); /* 0 */
+  MUL (u1, r1, rn, m2, mn); /* 1 */
 
-  t0s = abs_sub_n (t0, m3, m2, mn);
-  u1s = r3s^t0s^1;			/* Reverse sign! */
-  MUL (u1, r3, rn, t0, mn);		/* u2 = s2 * t2 */
-  u1[rn+mn] = 0;
-  if (t0s)
+  MPN_COPY (s2, r3, rn);
+
+  r3[rn] = mpn_add_n (r3, r3, r2, rn);
+  r0[rn] = 0;
+  s3s = abs_sub_n (s3, r3, r0, rn + 1);
+  t2s = abs_sub_n (t2, m1, m0, mn);
+  if (t2s)
     {
-      t0s = abs_sub_n (t0, m1, t0, mn);
-      t0[mn] = 0;
+      t3[mn] = mpn_add_n (t3, m3, t2, mn);
+      t3s = 0;
     }
   else
     {
-      t0[mn] = mpn_add_n (t0, t0, m1, mn);
+      t3s = abs_sub_n (t3, m3, t2, mn);
+      t3[mn] = 0;
     }
 
-  /* FIXME: Could be simplified if we had space for rn + mn + 2 limbs
-     at r3. I'd expect that for matrices of random size, the high
-     words t0[mn] and r1[rn] are non-zero with a pretty small
-     probability. If that can be confirmed this should be done as an
-     unconditional rn x (mn+1) followed by an if (UNLIKELY (r1[rn]))
-     add_n. */
-  if (t0[mn] != 0)
-    {
-      MUL (r3, r1, rn, t0, mn + 1);	/* u3 = s3 * t3 */
-      ASSERT (r1[rn] < 2);
-      if (r1[rn] != 0)
-	mpn_add_n (r3 + rn, r3 + rn, t0, mn + 1);
-    }
-  else
-    {
-      MUL (r3, r1, rn + 1, t0, mn);
-    }
+  r2s = abs_sub_n (r2, r0, r2, rn);
+  r0[rn+mn] = mpn_add_n (r0, u0, u1, rn + mn);
 
-  ASSERT (r3[rn+mn] < 4);
+  MUL(u1, s3, rn+1, t3, mn+1); /* 3 */
+  u1s = s3s ^ t3s;
+  ASSERT (u1[rn+mn+1] == 0);
+  ASSERT (u1[rn+mn] < 4);
 
-  u0[rn+mn] = 0;
-  if (r1s^t0s)
+  if (u1s)
     {
-      r3s = abs_sub_n (r3, u0, r3, rn + mn + 1);
+      u0[rn+mn] = 0;
+      u0s = abs_sub_n (u0, u0, u1, rn + mn + 1);
     }
   else
     {
-      ASSERT_NOCARRY (mpn_add_n (r3, r3, u0, rn + mn + 1));
-      r3s = 0;				/* u3 + u5 */
+      u0[rn+mn] = u1[rn+mn] + mpn_add_n (u0, u0, u1, rn + mn);
+      u0s = 0;
     }
+  MUL(u1, r3, rn + 1, t2, mn); /* 2 */
+  u1s = t2s;
+  ASSERT (u1[rn+mn] < 2);
 
-  if (t0s)
-    {
-      t0[mn] = mpn_add_n (t0, t0, m0, mn);
-    }
-  else if (t0[mn] != 0)
-    {
-      t0[mn] -= mpn_sub_n (t0, t0, m0, mn);
-    }
-  else
+  u1s = add_signed_n (u1, u0, u0s, u1, u1s, rn + mn + 1);
+
+  t2s = abs_sub_n (t2, m3, m1, mn);
+  if (s3s)
     {
-      t0s = abs_sub_n (t0, t0, m0, mn);
+      s3[rn] += mpn_add_n (s3, s3, r1, rn);
+      s3s = 0;
     }
-  MUL (u0, r2, rn, t0, mn + 1);		/* u6 = s6 * t4 */
-  ASSERT (u0[rn+mn] < 2);
-  if (r1s)
+  else if (s3[rn] > 0)
     {
-      ASSERT_NOCARRY (mpn_sub_n (r1, r2, r1, rn));
+      s3[rn] -= mpn_sub_n (s3, s3, r1, rn);
+      s3s = 1;
     }
   else
     {
-      r1[rn] += mpn_add_n (r1, r1, r2, rn);
-    }
-  rn++;
-  t0s = add_signed_n (r2, r3, r3s, u0, t0s, rn + mn);
-					/* u3 + u5 + u6 */
-  ASSERT (r2[rn+mn-1] < 4);
-  r3s = add_signed_n (r3, r3, r3s, u1, u1s, rn + mn);
-					/* -u2 + u3 + u5  */
-  ASSERT (r3[rn+mn-1] < 3);
-  MUL (u0, s0, rn, m1, mn);		/* u4 = s4 * t5 */
-  ASSERT (u0[rn+mn-1] < 2);
-  t0[mn] = mpn_add_n (t0, m3, m1, mn);
-  MUL (u1, r1, rn, t0, mn + 1);		/* u1 = s1 * t1 */
-  mn += rn;
-  ASSERT (u1[mn-1] < 4);
-  ASSERT (u1[mn] == 0);
-  ASSERT_NOCARRY (add_signed_n (r1, r3, r3s, u0, s0s, mn));
-					/* -u2 + u3 - u4 + u5  */
-  ASSERT (r1[mn-1] < 2);
-  if (r3s)
-    {
-      ASSERT_NOCARRY (mpn_add_n (r3, u1, r3, mn));
+      s3s = abs_sub_n (s3, r1, s3, rn);
     }
-  else
+  MUL (r1, s3, rn+1, m3, mn); /* 5 */
+  ASSERT_NOCARRY(add_signed_n (r1, r1, s3s, u1, u1s, rn + mn + 1));
+  ASSERT (r1[rn + mn] < 2);
+
+  MUL (r3, r2, rn, t2, mn); /* 4 */
+  r3s = r2s ^ t2s;
+  r3[rn + mn] = 0;
+  u0s = add_signed_n (u0, u0, u0s, r3, r3s, rn + mn + 1);
+  ASSERT_NOCARRY (add_signed_n (r3, r3, r3s, u1, u1s, rn + mn + 1));
+  ASSERT (r3[rn + mn] < 2);
+
+  if (t3s)
     {
-      ASSERT_NOCARRY (mpn_sub_n (r3, u1, r3, mn));
-					/* u1 + u2 - u3 - u5  */
+      t3[mn] += mpn_add_n (t3, m2, t3, mn);
+      t3s = 0;
     }
-  ASSERT (r3[mn-1] < 2);
-  if (t0s)
+  else if (t3[mn] > 0)
     {
-      ASSERT_NOCARRY (mpn_add_n (r2, u1, r2, mn));
+      t3[mn] -= mpn_sub_n (t3, t3, m2, mn);
+      t3s = 1;
     }
   else
     {
-      ASSERT_NOCARRY (mpn_sub_n (r2, u1, r2, mn));
-					/* u1 - u3 - u5 - u6  */
+      t3s = abs_sub_n (t3, m2, t3, mn);
     }
-  ASSERT (r2[mn-1] < 2);
+  MUL (r2, s2, rn, t3, mn + 1); /* 6 */
+
+  ASSERT_NOCARRY (add_signed_n (r2, r2, t3s, u0, u0s, rn + mn + 1));
+  ASSERT (r2[rn + mn] < 2);
 }
 
 void
--- 1/mpn/generic/mod_1_1.c
+++ 2/mpn/generic/mod_1_1.c
@@ -1,8 +1,8 @@
-/* mpn_mod_1_1p (ap, n, b, cps)
+/* mpn_mod_1s_1p (ap, n, b, cps)
    Divide (ap,,n) by b.  Return the single-limb remainder.
+   Requires that b < B / 2.
 
    Contributed to the GNU project by Torbjorn Granlund.
-   Based on a suggestion by Peter L. Montgomery.
 
    THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
    SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
@@ -30,40 +30,39 @@
 #include "longlong.h"
 
 void
-mpn_mod_1_1p_cps (mp_limb_t cps[4], mp_limb_t b)
+mpn_mod_1s_1p_cps (mp_limb_t cps[4], mp_limb_t b)
 {
   mp_limb_t bi;
   mp_limb_t B1modb, B2modb;
   int cnt;
 
+  ASSERT (b <= GMP_NUMB_MAX / 2);
+
   count_leading_zeros (cnt, b);
 
   b <<= cnt;
   invert_limb (bi, b);
 
-  if (UNLIKELY (cnt == 0))
-    B1modb = -b;
-  else
-    B1modb = -b * ((bi >> (GMP_LIMB_BITS-cnt)) | (CNST_LIMB(1) << cnt));
+  B1modb = -b * ((bi >> (GMP_LIMB_BITS-cnt)) | (CNST_LIMB(1) << cnt));
   ASSERT (B1modb <= b);		/* NB: not fully reduced mod b */
   udiv_rnd_preinv (B2modb, B1modb, b, bi);
 
+  B1modb >>= cnt;
+  B2modb >>= cnt;
+
   cps[0] = bi;
   cps[1] = cnt;
-  cps[2] = B1modb >> cnt;
-  cps[3] = B2modb >> cnt;
+  cps[2] = B1modb;
+  cps[3] = B2modb;
 }
 
 mp_limb_t
-mpn_mod_1_1p (mp_srcptr ap, mp_size_t n, mp_limb_t b, mp_limb_t bmodb[4])
+mpn_mod_1s_1p (mp_srcptr ap, mp_size_t n, mp_limb_t b, mp_limb_t bmodb[4])
 {
   mp_limb_t rh, rl, bi, q, ph, pl, r;
   mp_limb_t B1modb, B2modb;
   mp_size_t i;
   int cnt;
-  mp_limb_t mask;
-
-  ASSERT (n >= 2);		/* fix tuneup.c if this is changed */
 
   B1modb = bmodb[2];
   B2modb = bmodb[3];
@@ -86,14 +85,20 @@
 
   bi = bmodb[0];
   cnt = bmodb[1];
+#if 1
+  {
+    mp_limb_t mask;
+    r = (rh << cnt) | (rl >> (GMP_LIMB_BITS - cnt));
+    mask = -(mp_limb_t) (r >= b);
+    r -= mask & b;
+  }
+#else
+  udiv_qrnnd_preinv (q, r, rh >> (GMP_LIMB_BITS - cnt),
+		     (rh << cnt) | (rl >> (GMP_LIMB_BITS - cnt)), b, bi);
+  ASSERT (q <= 1);	/* optimize for small quotient? */
+#endif
 
-  if (LIKELY (cnt != 0))
-    rh = (rh << cnt) | (rl >> (GMP_LIMB_BITS - cnt));
-
-  mask = -(mp_limb_t) (rh >= b);
-  rh -= mask & b;
-
-  udiv_qrnnd_preinv (q, r, rh, rl << cnt, b, bi);
+  udiv_qrnnd_preinv (q, r, r, rl << cnt, b, bi);
 
   return r >> cnt;
 }
--- 1/mpn/generic/mod_1_2.c
+++ 2/mpn/generic/mod_1_2.c
@@ -36,7 +36,7 @@
   mp_limb_t B1modb, B2modb, B3modb;
   int cnt;
 
-  ASSERT ((b & GMP_LIMB_HIGHBIT) == 0);
+  ASSERT (b <= GMP_NUMB_MAX / 2);
 
   count_leading_zeros (cnt, b);
 
@@ -53,18 +53,6 @@
   cps[2] = B1modb >> cnt;
   cps[3] = B2modb >> cnt;
   cps[4] = B3modb >> cnt;
-
-#if WANT_ASSERT
-  {
-    int i;
-    b = cps[2];
-    for (i = 3; i <= 4; i++)
-      {
-	b += cps[i];
-	ASSERT (b >= cps[i]);
-      }
-  }
-#endif
 }
 
 mp_limb_t
@@ -75,27 +63,15 @@
   mp_size_t i;
   int cnt;
 
-  ASSERT (n >= 1);
-
   B1modb = cps[2];
   B2modb = cps[3];
   B3modb = cps[4];
 
   if ((n & 1) != 0)
     {
-      if (n == 1)
-	{
-	  rl = ap[n - 1];
-	  bi = cps[0];
-	  cnt = cps[1];
-	  udiv_qrnnd_preinv (q, r, rl >> (GMP_LIMB_BITS - cnt),
-			     rl << cnt, b, bi);
-	  return r >> cnt;
-	}
-
+      umul_ppmm (rh, rl, ap[n - 1], B2modb);
       umul_ppmm (ph, pl, ap[n - 2], B1modb);
       add_ssaaaa (ph, pl, ph, pl, 0, ap[n - 3]);
-      umul_ppmm (rh, rl, ap[n - 1], B2modb);
       add_ssaaaa (rh, rl, rh, rl, ph, pl);
       n--;
     }
--- 1/mpn/generic/mod_1_3.c
+++ 2/mpn/generic/mod_1_3.c
@@ -36,7 +36,7 @@
   mp_limb_t B1modb, B2modb, B3modb, B4modb;
   int cnt;
 
-  ASSERT ((b & GMP_LIMB_HIGHBIT) == 0);
+  ASSERT (b <= GMP_NUMB_MAX / 3);
 
   count_leading_zeros (cnt, b);
 
@@ -55,18 +55,6 @@
   cps[3] = B2modb >> cnt;
   cps[4] = B3modb >> cnt;
   cps[5] = B4modb >> cnt;
-
-#if WANT_ASSERT
-  {
-    int i;
-    b = cps[2];
-    for (i = 3; i <= 5; i++)
-      {
-	b += cps[i];
-	ASSERT (b >= cps[i]);
-      }
-  }
-#endif
 }
 
 mp_limb_t
@@ -77,37 +65,17 @@
   mp_size_t i;
   int cnt;
 
-  ASSERT (n >= 1);
-
   B1modb = cps[2];
   B2modb = cps[3];
   B3modb = cps[4];
   B4modb = cps[5];
 
-  /* We compute n mod 3 in a tricky way, which works except for when n is so
-     close to the maximum size that we don't need to support it.  */
-  switch ((mp_limb_t) n * MODLIMB_INVERSE_3 >> (GMP_NUMB_BITS - 2))
-    {
-    case 0:
-      umul_ppmm (ph, pl, ap[n - 2], B1modb);
-      add_ssaaaa (ph, pl, ph, pl, 0, ap[n - 3]);
-      umul_ppmm (rh, rl, ap[n - 1], B2modb);
-      add_ssaaaa (rh, rl, rh, rl, ph, pl);
-      n -= 3;
-      break;
-    case 2:	/* n mod 3 = 1 */
-      rh = 0;
-      rl = ap[n - 1];
-      n -= 1;
-      break;
-    case 1:	/* n mod 3 = 2 */
-      umul_ppmm (ph, pl, ap[n - 1], B1modb);
-      add_ssaaaa (rh, rl, ph, pl, 0, ap[n - 2]);
-      n -= 2;
-      break;
-    }
+  umul_ppmm (ph, pl, ap[n - 2], B1modb);
+  add_ssaaaa (ph, pl, ph, pl, 0, ap[n - 3]);
+  umul_ppmm (ch, cl, ap[n - 1], B2modb);
+  add_ssaaaa (rh, rl, ph, pl, ch, cl);
 
-  for (i = n - 3; i >= 0; i -= 3)
+  for (i = n - 6; i >= 0; i -= 3)
     {
       /* rr = ap[i]				< B
 	    + ap[i+1] * (B mod b)		<= (B-1)(b-1)
@@ -128,6 +96,21 @@
       add_ssaaaa (rh, rl, rh, rl, ph, pl);
     }
 
+  if (i >= -2)
+    {
+      umul_ppmm (ph, pl, rl, B1modb);
+      add_ssaaaa (ph, pl, ph, pl, 0, ap[i + 2]);
+      umul_ppmm (rh, rl, rh, B2modb);
+      add_ssaaaa (rh, rl, rh, rl, ph, pl);
+      if (i >= -1)
+	{
+	  umul_ppmm (ph, pl, rl, B1modb);
+	  add_ssaaaa (ph, pl, ph, pl, 0, ap[0]);
+	  umul_ppmm (rh, rl, rh, B2modb);
+	  add_ssaaaa (rh, rl, rh, rl, ph, pl);
+	}
+    }
+
   bi = cps[0];
   cnt = cps[1];
 
--- 1/mpn/generic/mod_1_4.c
+++ 2/mpn/generic/mod_1_4.c
@@ -36,7 +36,7 @@
   mp_limb_t B1modb, B2modb, B3modb, B4modb, B5modb;
   int cnt;
 
-  ASSERT ((b & GMP_LIMB_HIGHBIT) == 0);
+  ASSERT (b <= GMP_NUMB_MAX / 4);
 
   count_leading_zeros (cnt, b);
 
@@ -57,18 +57,6 @@
   cps[4] = B3modb >> cnt;
   cps[5] = B4modb >> cnt;
   cps[6] = B5modb >> cnt;
-
-#if WANT_ASSERT
-  {
-    int i;
-    b = cps[2];
-    for (i = 3; i <= 6; i++)
-      {
-	b += cps[i];
-	ASSERT (b >= cps[i]);
-      }
-  }
-#endif
 }
 
 mp_limb_t
@@ -79,45 +67,22 @@
   mp_size_t i;
   int cnt;
 
-  ASSERT (n >= 1);
-
   B1modb = cps[2];
   B2modb = cps[3];
   B3modb = cps[4];
   B4modb = cps[5];
   B5modb = cps[6];
 
-  switch (n & 3)
-    {
-    case 0:
-      umul_ppmm (ph, pl, ap[n - 3], B1modb);
-      add_ssaaaa (ph, pl, ph, pl, 0, ap[n - 4]);
-      umul_ppmm (ch, cl, ap[n - 2], B2modb);
-      add_ssaaaa (ph, pl, ph, pl, ch, cl);
-      umul_ppmm (rh, rl, ap[n - 1], B3modb);
-      add_ssaaaa (rh, rl, rh, rl, ph, pl);
-      n -= 4;
-      break;
-    case 1:
-      rh = 0;
-      rl = ap[n - 1];
-      n -= 1;
-      break;
-    case 2:
-      umul_ppmm (ph, pl, ap[n - 1], B1modb);
-      add_ssaaaa (rh, rl, ph, pl, 0, ap[n - 2]);
-      n -= 2;
-      break;
-    case 3:
-      umul_ppmm (ph, pl, ap[n - 2], B1modb);
-      add_ssaaaa (ph, pl, ph, pl, 0, ap[n - 3]);
-      umul_ppmm (rh, rl, ap[n - 1], B2modb);
-      add_ssaaaa (rh, rl, rh, rl, ph, pl);
-      n -= 3;
-      break;
-    }
+  umul_ppmm (ph, pl, ap[n - 3], B1modb);
+  add_ssaaaa (ph, pl, ph, pl, 0, ap[n - 4]);
 
-  for (i = n - 4; i >= 0; i -= 4)
+  umul_ppmm (ch, cl, ap[n - 2], B2modb);
+  add_ssaaaa (ph, pl, ph, pl, ch, cl);
+
+  umul_ppmm (ch, cl, ap[n - 1], B3modb);
+  add_ssaaaa (rh, rl, ph, pl, ch, cl);
+
+  for (i = n - 8; i >= 0; i -= 4)
     {
       /* rr = ap[i]				< B
 	    + ap[i+1] * (B mod b)		<= (B-1)(b-1)
@@ -142,6 +107,28 @@
       add_ssaaaa (rh, rl, rh, rl, ph, pl);
     }
 
+  if (i >= -3)
+    {
+      umul_ppmm (ph, pl, rl, B1modb);
+      add_ssaaaa (ph, pl, ph, pl, 0, ap[i + 3]);
+      umul_ppmm (rh, rl, rh, B2modb);
+      add_ssaaaa (rh, rl, rh, rl, ph, pl);
+      if (i >= -2)
+	{
+	  umul_ppmm (ph, pl, rl, B1modb);
+	  add_ssaaaa (ph, pl, ph, pl, 0, ap[i + 2]);
+	  umul_ppmm (rh, rl, rh, B2modb);
+	  add_ssaaaa (rh, rl, rh, rl, ph, pl);
+	  if (i >= -1)
+	    {
+	      umul_ppmm (ph, pl, rl, B1modb);
+	      add_ssaaaa (ph, pl, ph, pl, 0, ap[0]);
+	      umul_ppmm (rh, rl, rh, B2modb);
+	      add_ssaaaa (rh, rl, rh, rl, ph, pl);
+	    }
+	}
+    }
+
   bi = cps[0];
   cnt = cps[1];
 
--- 1/mpn/generic/mod_1.c
+++ 2/mpn/generic/mod_1.c
@@ -207,40 +207,29 @@
 
   if (UNLIKELY ((b & GMP_NUMB_HIGHBIT) != 0))
     {
-      if (BELOW_THRESHOLD (n, MOD_1_1_THRESHOLD))
-	{
-	  return mpn_mod_1_norm (ap, n, b);
-	}
-      else
-	{
-	  mp_limb_t pre[4];
-	  mpn_mod_1_1p_cps (pre, b);
-	  return mpn_mod_1_1p (ap, n, b, pre);
-	}
+      /* The functions below do not handle this large divisor.  */
+      return mpn_mod_1_norm (ap, n, b);
+    }
+  else if (BELOW_THRESHOLD (n, MOD_1_1_THRESHOLD))
+    {
+      return mpn_mod_1_unnorm (ap, n, b);
+    }
+  else if (BELOW_THRESHOLD (n, MOD_1_2_THRESHOLD))
+    {
+      mp_limb_t pre[4];
+      mpn_mod_1s_1p_cps (pre, b);
+      return mpn_mod_1s_1p (ap, n, b << pre[1], pre);
+    }
+  else if (BELOW_THRESHOLD (n, MOD_1_4_THRESHOLD) || UNLIKELY (b > GMP_NUMB_MASK / 4))
+    {
+      mp_limb_t pre[5];
+      mpn_mod_1s_2p_cps (pre, b);
+      return mpn_mod_1s_2p (ap, n, b << pre[1], pre);
     }
   else
     {
-      if (BELOW_THRESHOLD (n, MOD_1_1_THRESHOLD))
-	{
-	  return mpn_mod_1_unnorm (ap, n, b);
-	}
-      else if (BELOW_THRESHOLD (n, MOD_1_2_THRESHOLD))
-	{
-	  mp_limb_t pre[4];
-	  mpn_mod_1_1p_cps (pre, b);
-	  return mpn_mod_1_1p (ap, n, b << pre[1], pre);
-	}
-      else if (BELOW_THRESHOLD (n, MOD_1_4_THRESHOLD) || UNLIKELY (b > GMP_NUMB_MASK / 4))
-	{
-	  mp_limb_t pre[5];
-	  mpn_mod_1s_2p_cps (pre, b);
-	  return mpn_mod_1s_2p (ap, n, b << pre[1], pre);
-	}
-      else
-	{
-	  mp_limb_t pre[7];
-	  mpn_mod_1s_4p_cps (pre, b);
-	  return mpn_mod_1s_4p (ap, n, b << pre[1], pre);
-	}
+      mp_limb_t pre[7];
+      mpn_mod_1s_4p_cps (pre, b);
+      return mpn_mod_1s_4p (ap, n, b << pre[1], pre);
     }
 }
--- 1/mpn/generic/mode1o.c
+++ 2/mpn/generic/mode1o.c
@@ -30,7 +30,7 @@
 
            r*B^k + a - c == q*d
 
-   where B=2^GMP_LIMB_BITS, a is {src,size}, k is either size or size-1
+   where B=2^BITS_PER_MP_LIMB, a is {src,size}, k is either size or size-1
    (the caller won't know which), and q is the quotient (discarded).  d must
    be odd, c can be any limb value.
 
--- 1/mpn/generic/mu_bdiv_q.c
+++ 2/mpn/generic/mu_bdiv_q.c
@@ -4,11 +4,12 @@
 
    Contributed to the GNU project by Torbjorn Granlund.
 
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
+   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH A MUTABLE INTERFACE.  IT IS
+   ONLY SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS
+   ALMOST GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP
+   RELEASE.
 
-Copyright 2005, 2006, 2007, 2009 Free Software Foundation, Inc.
+Copyright 2005, 2006, 2007 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -48,10 +49,11 @@
 
    Requirements: N >= D
 		 D >= 1
+		 N mod D = 0
 		 D odd
 		 dn >= 2
 		 nn >= 2
-		 scratch space as determined by mpn_mu_bdiv_q_itch(nn,dn).
+		 scratch space as determined by mpn_divexact_itch(nn,dn).
 
    Write quotient to Q = {qp,nn}.
 
@@ -95,7 +97,7 @@
 
       /* Some notes on allocation:
 
-	 When in = dn, R dies when mpn_mullo returns, if in < dn the low in
+	 When in = dn, R dies when mpn_mullow returns, if in < dn the low in
 	 limbs of R dies at that point.  We could save memory by letting T live
 	 just under R, and let the upper part of T expand into R. These changes
 	 should reduce itch to perhaps 3dn.
@@ -112,10 +114,9 @@
 
       MPN_COPY (rp, np, dn);
       np += dn;
-      mpn_mullo_n (qp, rp, ip, in);
+      mpn_mullow_n (qp, rp, ip, in);
       qn -= in;
 
-#if WANT_FFT
       if (ABOVE_THRESHOLD (dn, MUL_FFT_MODF_THRESHOLD))
 	{
 	  k = mpn_fft_best_k (dn, 0);
@@ -123,7 +124,6 @@
 	  wn = dn + in - m;			/* number of wrapped limbs */
 	  ASSERT_ALWAYS (wn >= 0);		/* could handle this below */
 	}
-#endif
 
       while (qn > in)
 	{
@@ -164,7 +164,7 @@
 	  /* Subtract tp[dn+in-1...dn] from dividend.  */
 	  cy = mpn_sub_nc (rp + dn - in, np, tp + dn, in, cy);
 	  np += in;
-	  mpn_mullo_n (qp, rp, ip, in);
+	  mpn_mullow_n (qp, rp, ip, in);
 	  qn -= in;
 	}
 
@@ -199,7 +199,7 @@
 	}
 
       mpn_sub_nc (rp + dn - in, np, tp + dn, qn - (dn - in), cy);
-      mpn_mullo_n (qp, rp, ip, qn);
+      mpn_mullow_n (qp, rp, ip, qn);
     }
   else
     {
@@ -215,7 +215,7 @@
 
       mpn_binvert (ip, dp, in, scratch);
 
-      mpn_mullo_n (qp, np, ip, in);		/* low `in' quotient limbs */
+      mpn_mullow_n (qp, np, ip, in);		/* low `in' quotient limbs */
 #if WANT_FFT
       if (ABOVE_THRESHOLD (qn, MUL_FFT_MODF_THRESHOLD))
 	{
@@ -233,7 +233,7 @@
 	mpn_mul (rp, dp, qn, qp, in);		/* mulhigh */
 
       mpn_sub_n (rp, np + in, rp + in, qn - in);
-      mpn_mullo_n (qp + in, rp, ip, qn - in);	/* high qn-in quotient limbs */
+      mpn_mullow_n (qp + in, rp, ip, qn - in);	/* high qn-in quotient limbs */
     }
 }
 
--- 1/mpn/generic/mu_bdiv_qr.c
+++ 2/mpn/generic/mu_bdiv_qr.c
@@ -1,14 +1,12 @@
-/* mpn_mu_bdiv_qr(qp,rp,np,nn,dp,dn,tp) -- Compute {np,nn} / {dp,dn} mod B^qn,
-   where qn = nn-dn, storing the result in {qp,qn}.  Overlap allowed between Q
-   and N; all other overlap disallowed.
+/* mpn_mu_bdiv_qr -- divide-and-conquer Hensel division using a variant of
+   Barrett's algorithm, returning quotient and remainder.
 
-   Contributed to the GNU project by Torbjorn Granlund.
+   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH A MUTABLE INTERFACE.  IT IS
+   ONLY SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS
+   ALMOST GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP
+   RELEASE.
 
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
-
-Copyright 2005, 2006, 2007, 2009 Free Software Foundation, Inc.
+Copyright 2005, 2006, 2007 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -25,275 +23,29 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-
-/* We use the "misunderstanding algorithm" (MU), discovered by Paul Zimmermann
-   and Torbjorn Granlund when Torbjorn misunderstood Paul's explanation of
-   Jebelean's bidirectional exact division algorithm.
-
-   The idea of this algorithm is to compute a smaller inverted value than used
-   in the standard Barrett algorithm, and thus save time in the Newton
-   iterations, and pay just a small price when using the inverted value for
-   developing quotient bits.
-
-   Written by Torbjorn Granlund.  Paul Zimmermann suggested the use of the
-   "wrap around" trick.
-*/
-
 #include "gmp.h"
 #include "gmp-impl.h"
 
 
-/* N = {np,nn}
-   D = {dp,dn}
+/* Computes Hensel binary division of {np, 2*n} by {dp, n}.
+
+   Output:
+
+      q = n * d^{-1} mod 2^{qn * GMP_NUMB_BITS},
+
+      r = (n - q * d) * 2^{-qn * GMP_NUMB_BITS}
+
+   Stores q at qp. Stores the n least significant limbs of r at the high half
+   of np, and returns the borrow from the subtraction n - q*d.
 
-   Requirements: N >= D
-		 D >= 1
-		 D odd
-		 dn >= 2
-		 nn >= 2
-		 scratch space as determined by mpn_mu_bdiv_qr_itch(nn,dn).
-
-   Write quotient to Q = {qp,nn}.
-
-   FIXME: When iterating, perhaps do the small step before loop, not after.
-   FIXME: Try to avoid the scalar divisions when computing inverse size.
-   FIXME: Trim allocation for (qn > dn) case, 3*dn might be possible.  In
-	  particular, when dn==in, tp and rp could use the same space.
-*/
-mp_limb_t
+   d must be odd. dinv is (-d)^-1 mod 2^GMP_NUMB_BITS. */
+
+void
 mpn_mu_bdiv_qr (mp_ptr qp,
 		mp_ptr rp,
 		mp_srcptr np, mp_size_t nn,
 		mp_srcptr dp, mp_size_t dn,
 		mp_ptr scratch)
 {
-  mp_ptr ip;
-  mp_size_t qn;
-  mp_size_t in;
-
-  qn = nn - dn;
-
-  ASSERT (dn >= 2);
-  ASSERT (qn >= 2);
-
-  if (qn > dn)
-    {
-      mp_size_t b;
-      mp_ptr tp;
-      mp_limb_t cy;
-      int k;
-      mp_size_t m, wn;
-      mp_size_t i;
-
-      /* |_______________________|   dividend
-			|________|   divisor  */
-
-      /* Compute an inverse size that is a nice partition of the quotient.  */
-      b = (qn - 1) / dn + 1;	/* ceil(qn/dn), number of blocks */
-      in = (qn - 1) / b + 1;	/* ceil(qn/b) = ceil(qn / ceil(qn/dn)) */
-
-      /* Some notes on allocation:
-
-	 When in = dn, R dies when mpn_mullo returns, if in < dn the low in
-	 limbs of R dies at that point.  We could save memory by letting T live
-	 just under R, and let the upper part of T expand into R. These changes
-	 should reduce itch to perhaps 3dn.
-       */
-
-      ip = scratch;			/* in limbs */
-      tp = scratch + in;		/* dn + in limbs FIXME: mpn_fft_next_size */
-      scratch += in;			/* Roughly 2in+1 limbs */
-
-      mpn_binvert (ip, dp, in, scratch);
-
-      MPN_COPY (rp, np, dn);
-      np += dn;
-      cy = 0;
-
-      while (qn > in)
-	{
-	  mpn_mullo_n (qp, rp, ip, in);
-
-#if WANT_FFT
-	  if (ABOVE_THRESHOLD (dn, MUL_FFT_MODF_THRESHOLD))
-	    {
-	      /* The two multiplicands are dn and 'in' limbs, with dn >= in.
-		 The relevant part of the result will typically partially wrap,
-		 and that part will come out as subtracted to the right.  The
-		 unwrapped part, m-in limbs at the high end of tp, is the lower
-		 part of the sought product.  The wrapped part, at the low end
-		 of tp, will be subtracted from the low part of the partial
-		 remainder; we undo that operation with another subtraction. */
-
-	      k = mpn_fft_best_k (dn, 0);
-	      m = mpn_fft_next_size (dn, k);
-	      mpn_mul_fft (tp, m, dp, dn, qp, in, k);
-	      wn = dn + in - m;			/* number of wrapped limbs */
-	      if (wn > 0)
-		{
-		  int c0 = mpn_sub_n (tp + m, rp, tp, wn);
-		  for (i = wn; c0 != 0 && i < in; i++)
-		    c0 = tp[i] == GMP_NUMB_MASK;
-		  mpn_incr_u (tp + in, c0);
-		}
-	    }
-	  else
-#endif
-	    mpn_mul (tp, dp, dn, qp, in);	/* mulhi, need tp[dn+in-1...in] */
-	  qp += in;
-	  qn -= in;
-
-	  if (dn != in)
-	    {
-	      /* Subtract tp[dn-1...in] from partial remainder.  */
-	      cy += mpn_sub_n (rp, rp + in, tp + in, dn - in);
-	      if (cy == 2)
-		{
-		  mpn_incr_u (tp + dn, 1);
-		  cy = 1;
-		}
-	    }
-	  /* Subtract tp[dn+in-1...dn] from dividend.  */
-	  cy = mpn_sub_nc (rp + dn - in, np, tp + dn, in, cy);
-	  np += in;
-	}
-
-      /* Generate last qn limbs.  */
-      mpn_mullo_n (qp, rp, ip, qn);
-
-#if WANT_FFT
-      if (ABOVE_THRESHOLD (dn, MUL_FFT_MODF_THRESHOLD))
-	{
-	  k = mpn_fft_best_k (dn, 0);
-	  m = mpn_fft_next_size (dn, k);
-	  mpn_mul_fft (tp, m, dp, dn, qp, qn, k);
-	  wn = dn + qn - m;			/* number of wrapped limbs */
-	  if (wn > 0)
-	    {
-	      int c0 = mpn_sub_n (tp + m, rp, tp, wn);
-	      for (i = wn; c0 != 0 && i < qn; i++)
-		c0 = tp[i] == GMP_NUMB_MASK;
-	      mpn_incr_u (tp + qn, c0);
-	    }
-	}
-      else
-#endif
-	mpn_mul (tp, dp, dn, qp, qn);		/* mulhi, need tp[qn+in-1...in] */
-
-      if (dn != qn)
-	{
-	  cy += mpn_sub_n (rp, rp + qn, tp + qn, dn - qn);
-	  if (cy == 2)
-	    {
-	      mpn_incr_u (tp + dn, 1);
-	      cy = 1;
-	    }
-	}
-      return mpn_sub_nc (rp + dn - qn, np, tp + dn, qn, cy);
-    }
-  else
-    {
-      mp_ptr tp;
-      mp_limb_t cy;
-      int k;
-      mp_size_t m, wn;
-      mp_size_t i;
-
-      /* |_______________________|   dividend
-		|________________|   divisor  */
-
-      /* Compute half-sized inverse.  */
-      in = qn - (qn >> 1);
-
-      ip = scratch;			/* ceil(qn/2) limbs */
-      tp = scratch + in;		/* dn + in limbs FIXME: mpn_fft_next_size */
-      scratch += in;			/* 2*ceil(qn/2)+2 */
-
-      mpn_binvert (ip, dp, in, scratch);
-
-      mpn_mullo_n (qp, np, ip, in);		/* low `in' quotient limbs */
-#if WANT_FFT
-      /* FIXME: We might perform extremely unbalanced multiplies here, were FFT
-	 is not efficient.  */
-      if (ABOVE_THRESHOLD (dn, MUL_FFT_MODF_THRESHOLD))
-	{
-	  k = mpn_fft_best_k (dn, 0);
-	  m = mpn_fft_next_size (dn, k);
-	  mpn_mul_fft (tp, m, dp, dn, qp, in, k);
-	  wn = dn + in - m;
-	  if (wn > 0)
-	    {
-	      int c0 = mpn_sub_n (tp + m, np, tp, wn);
-	      for (i = wn; c0 != 0 && i < in; i++)
-		c0 = tp[i] == GMP_NUMB_MASK;
-	      mpn_incr_u (tp + in, c0);
-	    }
-	}
-      else
-#endif
-	mpn_mul (tp, dp, dn, qp, in);		/* mulhigh */
-      qp += in;
-      qn -= in;
-
-      cy = mpn_sub_n (rp, np + in, tp + in, dn);
-      mpn_mullo_n (qp, rp, ip, qn);		/* high qn quotient limbs */
-#if WANT_FFT
-      if (ABOVE_THRESHOLD (dn, MUL_FFT_MODF_THRESHOLD))
-	{
-	  k = mpn_fft_best_k (dn, 0);
-	  m = mpn_fft_next_size (dn, k);
-	  mpn_mul_fft (tp, m, dp, dn, qp, qn, k);
-	  wn = dn + qn - m;
-	  if (wn > 0)
-	    {
-	      int c0 = mpn_sub_n (tp + m, rp, tp, wn);
-	      for (i = wn; c0 != 0 && i < qn; i++)
-		c0 = tp[i] == GMP_NUMB_MASK;
-	      mpn_incr_u (tp + qn, c0);
-	    }
-	}
-      else
-#endif
-	mpn_mul (tp, dp, dn, qp, qn);		/* mulhigh */
-
-      cy += mpn_sub_n (rp, rp + qn, tp + qn, dn - qn);
-      if (cy == 2)
-	{
-	  mpn_incr_u (tp + dn, 1);
-	  cy = 1;
-	}
-      return mpn_sub_nc (rp + dn - qn, np + dn + in, tp + dn, qn, cy);
-    }
-}
-
-mp_size_t
-mpn_mu_bdiv_qr_itch (mp_size_t nn, mp_size_t dn)
-{
-  mp_size_t qn, in, m, itch_invert, itch;
-  mp_size_t b;
-
-  qn = nn - dn;
-
-  if (qn > dn)
-    {
-      b = (qn - 1) / dn + 1;	/* ceil(qn/dn), number of blocks */
-      in = (qn - 1) / b + 1;	/* ceil(qn/b) = ceil(qn / ceil(qn/dn)) */
-      return in + mpn_binvert_itch (in);
-    }
-  else
-    {
-      in = qn - (qn >> 1);
-#if WANT_FFT
-      if (ABOVE_THRESHOLD (dn, MUL_FFT_MODF_THRESHOLD))
-	{
-	  m = mpn_fft_next_size (dn, mpn_fft_best_k (dn, 0));
-	  m = MAX (dn + in, m);
-	}
-      else
-#endif
-	m = dn + in;
-      itch_invert = mpn_binvert_itch (in);
-      itch = MAX (itch_invert, m);
-      return in + itch;
-    }
+  ASSERT_ALWAYS (0);
 }
--- 1/mpn/generic/mu_divappr_q.c
+++ 2/mpn/generic/mu_divappr_q.c
@@ -7,9 +7,10 @@
 
    Contributed to the GNU project by Torbjorn Granlund.
 
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
+   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH A MUTABLE INTERFACE.  IT IS
+   ONLY SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS
+   ALMOST GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP
+   RELEASE.
 
 Copyright 2005, 2006, 2007 Free Software Foundation, Inc.
 
@@ -67,7 +68,7 @@
     to save with special code.
 
   * The itch/scratch scheme isn't perhaps such a good idea as it once seemed,
-    demonstrated by the fact that the mpn_invert function's scratch needs mean
+    demonstrated by the fact that the mpn_inv function's scratch needs means
     that we need to keep a large allocation long after it is needed.  Things
     are worse as mpn_mul_fft does not accept any scratch parameter, which means
     we'll have a large memory hole while in mpn_mul_fft.  In general, a peak
--- 1/mpn/generic/mu_div_q.c
+++ 2/mpn/generic/mu_div_q.c
@@ -1,10 +1,11 @@
 /* mpn_mu_div_q, mpn_preinv_mu_div_q.
 
-   Contributed to the GNU project by Torbjorn Granlund.
+   Contributed to the GNU project by Torbjörn Granlund.
 
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
+   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH A MUTABLE INTERFACE.  IT IS
+   ONLY SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS
+   ALMOST GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP
+   RELEASE.
 
 Copyright 2005, 2006, 2007 Free Software Foundation, Inc.
 
--- 1/mpn/generic/mu_div_qr.c
+++ 2/mpn/generic/mu_div_qr.c
@@ -7,9 +7,10 @@
 
    Contributed to the GNU project by Torbjorn Granlund.
 
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP RELEASE.
+   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH A MUTABLE INTERFACE.  IT IS
+   ONLY SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS
+   ALMOST GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GMP
+   RELEASE.
 
 Copyright 2005, 2006, 2007 Free Software Foundation, Inc.
 
--- 1/mpn/generic/mul_basecase.c
+++ 2/mpn/generic/mul_basecase.c
@@ -33,7 +33,7 @@
    Note that prodp gets usize+vsize limbs stored, even if the actual result
    only needs usize+vsize-1.
 
-   There's no good reason to call here with vsize>=MUL_TOOM22_THRESHOLD.
+   There's no good reason to call here with vsize>=MUL_KARATSUBA_THRESHOLD.
    Currently this is allowed, but it might not be in the future.
 
    This is the most critical code for multiplication.  All multiplies rely
--- 1/mpn/generic/mul.c
+++ 2/mpn/generic/mul.c
@@ -3,7 +3,7 @@
    Contributed to the GNU project by Torbjorn Granlund.
 
 Copyright 1991, 1993, 1994, 1996, 1997, 1999, 2000, 2001, 2002, 2003, 2005,
-2006, 2007, 2009 Free Software Foundation, Inc.
+2006, 2007 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -28,9 +28,6 @@
 #define MUL_BASECASE_MAX_UN 500
 #endif
 
-#define TOOM33_OK(an,bn) (6 + 2 * an < 3 * bn)
-#define TOOM44_OK(an,bn) (12 + 3 * an < 4 * bn)
-
 /* Multiply the natural numbers u (pointed to by UP, with UN limbs) and v
    (pointed to by VP, with VN limbs), and store the result at PRODP.  The
    result is UN + VN limbs.  Return the most significant limb of the result.
@@ -53,11 +50,16 @@
   ASSERT (! MPN_OVERLAP_P (prodp, un+vn, up, un));
   ASSERT (! MPN_OVERLAP_P (prodp, un+vn, vp, vn));
 
-  if (up == vp && un == vn)
+  if (un == vn)
     {
-      mpn_sqr_n (prodp, up, un);
+      if (up == vp)
+	mpn_sqr_n (prodp, up, un);
+      else
+	mpn_mul_n (prodp, up, vp, un);
+      return prodp[2 * un - 1];
     }
-  else if (vn < MUL_TOOM22_THRESHOLD)
+
+  if (vn < MUL_KARATSUBA_THRESHOLD)
     { /* plain schoolbook multiplication */
 
       /* Unless un is very large, or else if have an applicable mpn_mul_N,
@@ -96,9 +98,9 @@
 	    The parts marked with X are the parts whose sums are copied into
 	    the temporary buffer.  */
 
-	  mp_limb_t tp[MUL_TOOM22_THRESHOLD_LIMIT];
+	  mp_limb_t tp[MUL_KARATSUBA_THRESHOLD_LIMIT];
 	  mp_limb_t cy;
-          ASSERT (MUL_TOOM22_THRESHOLD <= MUL_TOOM22_THRESHOLD_LIMIT);
+          ASSERT (MUL_KARATSUBA_THRESHOLD <= MUL_KARATSUBA_THRESHOLD_LIMIT);
 
 	  mpn_mul_basecase (prodp, up, MUL_BASECASE_MAX_UN, vp, vn);
 	  prodp += MUL_BASECASE_MAX_UN;
@@ -109,7 +111,7 @@
 	    {
 	      mpn_mul_basecase (prodp, up, MUL_BASECASE_MAX_UN, vp, vn);
 	      cy = mpn_add_n (prodp, prodp, tp, vn); /* add back preserved triangle */
-	      mpn_incr_u (prodp + vn, cy);
+	      mpn_incr_u (prodp + vn, cy);		/* safe? */
 	      prodp += MUL_BASECASE_MAX_UN;
 	      MPN_COPY (tp, prodp, vn);		/* preserve high triangle */
 	      up += MUL_BASECASE_MAX_UN;
@@ -125,168 +127,96 @@
 	      mpn_mul_basecase (prodp, vp, vn, up, un);
 	    }
 	  cy = mpn_add_n (prodp, prodp, tp, vn); /* add back preserved triangle */
-	  mpn_incr_u (prodp + vn, cy);
+	  mpn_incr_u (prodp + vn, cy);		/* safe? */
 	}
+      return prodp[un + vn - 1];
     }
-  else if (BELOW_THRESHOLD (vn, MUL_TOOM33_THRESHOLD) ||
-	   /* Also do larger unbalanced here, up to a (somewhat arbitrarily)
-	      larger vn limit, unless toom33 can do this product directly.  */
-	   (!TOOM33_OK (un, vn) && BELOW_THRESHOLD (vn, MUL_TOOM33_THRESHOLD * 3 / 2)))
+
+  if (ABOVE_THRESHOLD ((un + vn) >> 1, MUL_FFT_THRESHOLD) &&
+      ABOVE_THRESHOLD (vn, MUL_FFT_THRESHOLD / 3)) /* FIXME */
     {
-      /* Loop over toom42, then choose toom42, toom32, or toom22 */
-      mp_ptr ws;
-      mp_ptr scratch;
-      TMP_DECL; TMP_MARK;
+      mpn_mul_fft_full (prodp, up, un, vp, vn);
+      return prodp[un + vn - 1];
+    }
+
+  {
+    mp_ptr ws;
+    mp_ptr scratch;
+#if WANT_ASSERT
+    mp_ptr ssssp;
+#endif
+    TMP_DECL;
+    TMP_MARK;
 
 #define WSALL (4 * vn)
-      ws = TMP_SALLOC_LIMBS (WSALL + 1);
+    ws = TMP_SALLOC_LIMBS (WSALL + 1);
 
 #define ITCH ((un + vn) * 4 + 100)
-      scratch = TMP_ALLOC_LIMBS (ITCH + 1);
-
-      if (un >= 3 * vn)
-	{
-	  mp_limb_t cy;
-
-	  mpn_toom42_mul (prodp, up, 2 * vn, vp, vn, scratch);
-	  un -= 2 * vn;
-	  up += 2 * vn;
-	  prodp += 2 * vn;
-
-	  while (un >= 3 * vn)
-	    {
-	      mpn_toom42_mul (ws, up, 2 * vn, vp, vn, scratch);
-	      un -= 2 * vn;
-	      up += 2 * vn;
-	      cy = mpn_add_n (prodp, prodp, ws, vn);
-	      MPN_COPY (prodp + vn, ws + vn, 2 * vn);
-	      mpn_incr_u (prodp + vn, cy);
-	      prodp += 2 * vn;
-	    }
+    scratch = TMP_ALLOC_LIMBS (ITCH + 1);
+#if WANT_ASSERT
+    ssssp = scratch + ITCH;
+    ws[WSALL] = 0xbabecafe;
+    ssssp[0] = 0xbeef;
+#endif
 
-	  /* FIXME: Test these in opposite order, following the philosophy of
-	     minimizing the relative overhead.  */
-	  if (5 * un > 9 * vn)
-	    {
-	      mpn_toom42_mul (ws, up, un, vp, vn, scratch);
-	      cy = mpn_add_n (prodp, prodp, ws, vn);
-	      MPN_COPY (prodp + vn, ws + vn, un);
-	      mpn_incr_u (prodp + vn, cy);
-	    }
-	  else if (9 * un > 10 * vn)
-	    {
-	      mpn_toom32_mul (ws, up, un, vp, vn, scratch);
-	      cy = mpn_add_n (prodp, prodp, ws, vn);
-	      MPN_COPY (prodp + vn, ws + vn, un);
-	      mpn_incr_u (prodp + vn, cy);
-	    }
-	  else
-	    {
-	      mpn_toom22_mul (ws, up, un, vp, vn, scratch);
-	      cy = mpn_add_n (prodp, prodp, ws, vn);
-	      MPN_COPY (prodp + vn, ws + vn, un);
-	      mpn_incr_u (prodp + vn, cy);
-	    }
-	}
-      else
-	{
-	  /* FIXME: Test these in opposite order, following the philosophy of
-	     minimizing the relative overhead.  */
-	  if (5 * un > 9 * vn)
-	    mpn_toom42_mul (prodp, up, un, vp, vn, scratch);
-	  else if (9 * un > 10 * vn)
-	    mpn_toom32_mul (prodp, up, un, vp, vn, scratch);
-	  else
-	    mpn_toom22_mul (prodp, up, un, vp, vn, scratch);
-	}
-      TMP_FREE;
-    }
-  else if (BELOW_THRESHOLD (vn, MUL_TOOM44_THRESHOLD))
-    {
-      TMP_DECL; TMP_MARK;
-      if (TOOM33_OK (un, vn))
-	{
-	  /* Apply toom33 directly, since operands are balanced enough.  */
-	  mp_ptr scratch;
-	  scratch = TMP_SALLOC_LIMBS (mpn_toom33_mul_itch (un, vn));
-	  mpn_toom33_mul (prodp, up, un, vp, vn, scratch);
-	}
-      else
-	{
-	  /* Apply toom33, recurse.  FUTURE: toom63, toom53, toom43, toom33 */
-	  mp_ptr scratch, pp;		/* FIXME: use same area for these */
-	  mp_limb_t cy;
-	  scratch = TMP_SALLOC_LIMBS (mpn_toom33_mul_itch (vn, vn));
-	  mpn_toom33_mul (prodp, up, vn, vp, vn, scratch);
-	  prodp += vn;
-	  up += vn;
-	  un -= vn;
-	  pp = TMP_SALLOC_LIMBS (2 * vn);
-	  while (un >= vn)
-	    {
-	      mpn_toom33_mul (pp, up, vn, vp, vn, scratch);
-	      cy = mpn_add_n (prodp, prodp, pp, vn);
-	      MPN_COPY (prodp + vn, pp + vn, vn);
-	      mpn_incr_u (prodp + vn, cy);
-	      prodp += vn;
-	      up += vn;
-	      un -= vn;
-	    }
-	  if (un != 0)
-	    {
-	      mpn_mul (pp, vp, vn, up, un);
-	      cy = mpn_add_n (prodp, prodp, pp, vn);
-	      MPN_COPY (prodp + vn, pp + vn, un);
-	      mpn_incr_u (prodp + vn, cy);
-	    }
-	}
-      TMP_FREE;
-    }
-  else if (BELOW_THRESHOLD ((un + vn) >> 1, MUL_FFT_THRESHOLD) ||
-	   BELOW_THRESHOLD (vn, MUL_FFT_THRESHOLD / 3)) /* FIXME */
-    {
-      TMP_DECL; TMP_MARK;
-      if (TOOM44_OK (un, vn))
-	{
-	  /* Apply toom44 directly, since operands are balanced enough.  */
-	  mp_ptr scratch;
-	  scratch = TMP_ALLOC_LIMBS (mpn_toom44_mul_itch (un, vn));
-	  mpn_toom44_mul (prodp, up, un, vp, vn, scratch);
-	}
-      else
-	{
-	  /* Apply toom44, recurse.  FUTURE: toom84, toom74, toom64, toom54, toom44 */
-	  mp_ptr scratch, pp;		/* FIXME: use same area for these */
-	  mp_limb_t cy;
-	  scratch = TMP_ALLOC_LIMBS (mpn_toom44_mul_itch (vn, vn));
-	  mpn_toom44_mul (prodp, up, vn, vp, vn, scratch);
-	  prodp += vn;
-	  up += vn;
-	  un -= vn;
-	  pp = TMP_SALLOC_LIMBS (2 * vn);
-	  while (un >= vn)
-	    {
-	      mpn_toom44_mul (pp, up, vn, vp, vn, scratch);
-	      cy = mpn_add_n (prodp, prodp, pp, vn);
-	      MPN_COPY (prodp + vn, pp + vn, vn);
-	      mpn_incr_u (prodp + vn, cy);
-	      prodp += vn;
-	      up += vn;
-	      un -= vn;
-	    }
-	  if (un != 0)
-	    {
-	      mpn_mul (pp, vp, vn, up, un);
-	      cy = mpn_add_n (prodp, prodp, pp, vn);
-	      MPN_COPY (prodp + vn, pp + vn, un);
-	      mpn_incr_u (prodp + vn, cy);
-	    }
-	}
-      TMP_FREE;
-    }
-  else
-    {
-      mpn_mul_fft_full (prodp, up, un, vp, vn);
-    }
-  return prodp[un + vn - 1];
+    if (un >= 3 * vn)
+      {
+	mp_limb_t cy;
+
+	mpn_toom42_mul (prodp, up, 2 * vn, vp, vn, scratch);
+	un -= 2 * vn;
+	up += 2 * vn;
+	prodp += 2 * vn;
+
+	while (un >= 3 * vn)
+	  {
+	    mpn_toom42_mul (ws, up, 2 * vn, vp, vn, scratch);
+	    un -= 2 * vn;
+	    up += 2 * vn;
+	    cy = mpn_add_n (prodp, prodp, ws, vn);
+	    MPN_COPY (prodp + vn, ws + vn, 2 * vn);
+	    mpn_incr_u (prodp + vn, cy);
+	    prodp += 2 * vn;
+	  }
+
+	if (5 * un > 9 * vn)
+	  {
+	    mpn_toom42_mul (ws, up, un, vp, vn, scratch);
+	    cy = mpn_add_n (prodp, prodp, ws, vn);
+	    MPN_COPY (prodp + vn, ws + vn, un);
+	    mpn_incr_u (prodp + vn, cy);
+	  }
+	else if (9 * un > 10 * vn)
+	  {
+	    mpn_toom32_mul (ws, up, un, vp, vn, scratch);
+	    cy = mpn_add_n (prodp, prodp, ws, vn);
+	    MPN_COPY (prodp + vn, ws + vn, un);
+	    mpn_incr_u (prodp + vn, cy);
+	  }
+	else
+	  {
+	    mpn_toom22_mul (ws, up, un, vp, vn, scratch);
+	    cy = mpn_add_n (prodp, prodp, ws, vn);
+	    MPN_COPY (prodp + vn, ws + vn, un);
+	    mpn_incr_u (prodp + vn, cy);
+	  }
+
+	ASSERT (ws[WSALL] == 0xbabecafe);
+	ASSERT (ssssp[0] == 0xbeef);
+	TMP_FREE;
+	return prodp[un + vn - 1];
+      }
+
+    if (un * 5 > vn * 9)
+      mpn_toom42_mul (prodp, up, un, vp, vn, scratch);
+    else if (9 * un > 10 * vn)
+      mpn_toom32_mul (prodp, up, un, vp, vn, scratch);
+    else
+      mpn_toom22_mul (prodp, up, un, vp, vn, scratch);
+
+    ASSERT (ws[WSALL] == 0xbabecafe);
+    ASSERT (ssssp[0] == 0xbeef);
+    TMP_FREE;
+    return prodp[un + vn - 1];
+  }
 }
--- 1/mpn/generic/mul_fft.c
+++ 2/mpn/generic/mul_fft.c
@@ -60,8 +60,8 @@
 #include "gmp-impl.h"
 
 #ifdef WANT_ADDSUB
-#include "generic/add_n_sub_n.c"
-#define HAVE_NATIVE_mpn_add_n_sub_n 1
+#include "generic/addsub_n.c"
+#define HAVE_NATIVE_mpn_addsub_n 1
 #endif
 
 static mp_limb_t mpn_mul_fft_internal
@@ -387,8 +387,8 @@
   if (K == 2)
     {
       mp_limb_t cy;
-#if HAVE_NATIVE_mpn_add_n_sub_n
-      cy = mpn_add_n_sub_n (Ap[0], Ap[inc], Ap[0], Ap[inc], n + 1) & 1;
+#if HAVE_NATIVE_mpn_addsub_n
+      cy = mpn_addsub_n (Ap[0], Ap[inc], Ap[0], Ap[inc], n + 1) & 1;
 #else
       MPN_COPY (tp, Ap[0], n + 1);
       mpn_add_n (Ap[0], Ap[0], Ap[inc], n + 1);
@@ -562,8 +562,8 @@
   if (K == 2)
     {
       mp_limb_t cy;
-#if HAVE_NATIVE_mpn_add_n_sub_n
-      cy = mpn_add_n_sub_n (Ap[0], Ap[1], Ap[0], Ap[1], n + 1) & 1;
+#if HAVE_NATIVE_mpn_addsub_n
+      cy = mpn_addsub_n (Ap[0], Ap[1], Ap[0], Ap[1], n + 1) & 1;
 #else
       MPN_COPY (tp, Ap[0], n + 1);
       mpn_add_n (Ap[0], Ap[0], Ap[1], n + 1);
@@ -974,23 +974,23 @@
 
   ASSERT_ALWAYS(pl3 <= pl);
   cc = mpn_mul_fft (op, pl3, n, nl, m, ml, k3);     /* mu */
-  ASSERT(cc == 0);
+  ASSERT_ALWAYS(cc == 0);
   pad_op = __GMP_ALLOCATE_FUNC_LIMBS (pl2);
   cc = mpn_mul_fft (pad_op, pl2, n, nl, m, ml, k2); /* lambda */
   cc = -cc + mpn_sub_n (pad_op, pad_op, op, pl2);    /* lambda - low(mu) */
   /* 0 <= cc <= 1 */
-  ASSERT(0 <= cc && cc <= 1);
+  ASSERT_ALWAYS(0 <= cc && cc <= 1);
   l = pl3 - pl2; /* l = pl2 / 2 since pl3 = 3/2 * pl2 */
   c2 = mpn_add_n (pad_op, pad_op, op + pl2, l);
   cc = mpn_add_1 (pad_op + l, pad_op + l, l, (mp_limb_t) c2) - cc;
-  ASSERT(-1 <= cc && cc <= 1);
+  ASSERT_ALWAYS(-1 <= cc && cc <= 1);
   if (cc < 0)
     cc = mpn_add_1 (pad_op, pad_op, pl2, (mp_limb_t) -cc);
-  ASSERT(0 <= cc && cc <= 1);
+  ASSERT_ALWAYS(0 <= cc && cc <= 1);
   /* now lambda-mu = {pad_op, pl2} - cc mod 2^(pl2*GMP_NUMB_BITS)+1 */
   oldcc = cc;
-#if HAVE_NATIVE_mpn_add_n_sub_n
-  c2 = mpn_add_n_sub_n (pad_op + l, pad_op, pad_op, pad_op + l, l);
+#if HAVE_NATIVE_mpn_addsub_n
+  c2 = mpn_addsub_n (pad_op + l, pad_op, pad_op, pad_op + l, l);
   /* c2 & 1 is the borrow, c2 & 2 is the carry */
   cc += c2 >> 1; /* carry out from high <- low + high */
   c2 = c2 & 1; /* borrow out from low <- low - high */
--- 1/mpn/generic/mul_n.c
+++ 2/mpn/generic/mul_n.c
@@ -1,7 +1,12 @@
-/* mpn_mul_n -- multiply natural numbers.
+/* mpn_mul_n and helper function -- Multiply/square natural numbers.
+
+   THE HELPER FUNCTIONS IN THIS FILE (meaning everything except mpn_mul_n) ARE
+   INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY SAFE TO REACH THEM THROUGH
+   DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST GUARANTEED THAT THEY'LL CHANGE
+   OR DISAPPEAR IN A FUTURE GNU MP RELEASE.
 
 Copyright 1991, 1993, 1994, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
-2005, 2008, 2009 Free Software Foundation, Inc.
+2005 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -22,6 +27,669 @@
 #include "gmp-impl.h"
 #include "longlong.h"
 
+
+/* Multiplies using 3 half-sized mults and so on recursively.
+ * p[0..2*n-1] := product of a[0..n-1] and b[0..n-1].
+ * No overlap of p[...] with a[...] or b[...].
+ * ws is workspace.
+ */
+
+void
+mpn_kara_mul_n (mp_ptr p, mp_srcptr a, mp_srcptr b, mp_size_t n, mp_ptr ws)
+{
+  mp_limb_t w, w0, w1;
+  mp_size_t n2;
+  mp_srcptr x, y;
+  mp_size_t i;
+  int sign;
+
+  n2 = n >> 1;
+  ASSERT (n2 > 0);
+
+  if ((n & 1) != 0)
+    {
+      /* Odd length. */
+      mp_size_t n1, n3, nm1;
+
+      n3 = n - n2;
+
+      sign = 0;
+      w = a[n2];
+      if (w != 0)
+	w -= mpn_sub_n (p, a, a + n3, n2);
+      else
+	{
+	  i = n2;
+	  do
+	    {
+	      --i;
+	      w0 = a[i];
+	      w1 = a[n3 + i];
+	    }
+	  while (w0 == w1 && i != 0);
+	  if (w0 < w1)
+	    {
+	      x = a + n3;
+	      y = a;
+	      sign = ~0;
+	    }
+	  else
+	    {
+	      x = a;
+	      y = a + n3;
+	    }
+	  mpn_sub_n (p, x, y, n2);
+	}
+      p[n2] = w;
+
+      w = b[n2];
+      if (w != 0)
+	w -= mpn_sub_n (p + n3, b, b + n3, n2);
+      else
+	{
+	  i = n2;
+	  do
+	    {
+	      --i;
+	      w0 = b[i];
+	      w1 = b[n3 + i];
+	    }
+	  while (w0 == w1 && i != 0);
+	  if (w0 < w1)
+	    {
+	      x = b + n3;
+	      y = b;
+	      sign = ~sign;
+	    }
+	  else
+	    {
+	      x = b;
+	      y = b + n3;
+	    }
+	  mpn_sub_n (p + n3, x, y, n2);
+	}
+      p[n] = w;
+
+      n1 = n + 1;
+      if (n2 < MUL_KARATSUBA_THRESHOLD)
+	{
+	  if (n3 < MUL_KARATSUBA_THRESHOLD)
+	    {
+	      mpn_mul_basecase (ws, p, n3, p + n3, n3);
+	      mpn_mul_basecase (p, a, n3, b, n3);
+	    }
+	  else
+	    {
+	      mpn_kara_mul_n (ws, p, p + n3, n3, ws + n1);
+	      mpn_kara_mul_n (p, a, b, n3, ws + n1);
+	    }
+	  mpn_mul_basecase (p + n1, a + n3, n2, b + n3, n2);
+	}
+      else
+	{
+	  mpn_kara_mul_n (ws, p, p + n3, n3, ws + n1);
+	  mpn_kara_mul_n (p, a, b, n3, ws + n1);
+	  mpn_kara_mul_n (p + n1, a + n3, b + n3, n2, ws + n1);
+	}
+
+      if (sign)
+	mpn_add_n (ws, p, ws, n1);
+      else
+	mpn_sub_n (ws, p, ws, n1);
+
+      nm1 = n - 1;
+      if (mpn_add_n (ws, p + n1, ws, nm1))
+	{
+	  mp_limb_t x = (ws[nm1] + 1) & GMP_NUMB_MASK;
+	  ws[nm1] = x;
+	  if (x == 0)
+	    ws[n] = (ws[n] + 1) & GMP_NUMB_MASK;
+	}
+      if (mpn_add_n (p + n3, p + n3, ws, n1))
+	{
+	  mpn_incr_u (p + n1 + n3, 1);
+	}
+    }
+  else
+    {
+      /* Even length. */
+      i = n2;
+      do
+	{
+	  --i;
+	  w0 = a[i];
+	  w1 = a[n2 + i];
+	}
+      while (w0 == w1 && i != 0);
+      sign = 0;
+      if (w0 < w1)
+	{
+	  x = a + n2;
+	  y = a;
+	  sign = ~0;
+	}
+      else
+	{
+	  x = a;
+	  y = a + n2;
+	}
+      mpn_sub_n (p, x, y, n2);
+
+      i = n2;
+      do
+	{
+	  --i;
+	  w0 = b[i];
+	  w1 = b[n2 + i];
+	}
+      while (w0 == w1 && i != 0);
+      if (w0 < w1)
+	{
+	  x = b + n2;
+	  y = b;
+	  sign = ~sign;
+	}
+      else
+	{
+	  x = b;
+	  y = b + n2;
+	}
+      mpn_sub_n (p + n2, x, y, n2);
+
+      /* Pointwise products. */
+      if (n2 < MUL_KARATSUBA_THRESHOLD)
+	{
+	  mpn_mul_basecase (ws, p, n2, p + n2, n2);
+	  mpn_mul_basecase (p, a, n2, b, n2);
+	  mpn_mul_basecase (p + n, a + n2, n2, b + n2, n2);
+	}
+      else
+	{
+	  mpn_kara_mul_n (ws, p, p + n2, n2, ws + n);
+	  mpn_kara_mul_n (p, a, b, n2, ws + n);
+	  mpn_kara_mul_n (p + n, a + n2, b + n2, n2, ws + n);
+	}
+
+      /* Interpolate. */
+      if (sign)
+	w = mpn_add_n (ws, p, ws, n);
+      else
+	w = -mpn_sub_n (ws, p, ws, n);
+      w += mpn_add_n (ws, p + n, ws, n);
+      w += mpn_add_n (p + n2, p + n2, ws, n);
+      MPN_INCR_U (p + n2 + n, 2 * n - (n2 + n), w);
+    }
+}
+
+void
+mpn_kara_sqr_n (mp_ptr p, mp_srcptr a, mp_size_t n, mp_ptr ws)
+{
+  mp_limb_t w, w0, w1;
+  mp_size_t n2;
+  mp_srcptr x, y;
+  mp_size_t i;
+
+  n2 = n >> 1;
+  ASSERT (n2 > 0);
+
+  if ((n & 1) != 0)
+    {
+      /* Odd length. */
+      mp_size_t n1, n3, nm1;
+
+      n3 = n - n2;
+
+      w = a[n2];
+      if (w != 0)
+	w -= mpn_sub_n (p, a, a + n3, n2);
+      else
+	{
+	  i = n2;
+	  do
+	    {
+	      --i;
+	      w0 = a[i];
+	      w1 = a[n3 + i];
+	    }
+	  while (w0 == w1 && i != 0);
+	  if (w0 < w1)
+	    {
+	      x = a + n3;
+	      y = a;
+	    }
+	  else
+	    {
+	      x = a;
+	      y = a + n3;
+	    }
+	  mpn_sub_n (p, x, y, n2);
+	}
+      p[n2] = w;
+
+      n1 = n + 1;
+
+      /* n2 is always either n3 or n3-1 so maybe the two sets of tests here
+	 could be combined.  But that's not important, since the tests will
+	 take a miniscule amount of time compared to the function calls.  */
+      if (BELOW_THRESHOLD (n3, SQR_BASECASE_THRESHOLD))
+	{
+	  mpn_mul_basecase (ws, p, n3, p, n3);
+	  mpn_mul_basecase (p,  a, n3, a, n3);
+	}
+      else if (BELOW_THRESHOLD (n3, SQR_KARATSUBA_THRESHOLD))
+	{
+	  mpn_sqr_basecase (ws, p, n3);
+	  mpn_sqr_basecase (p,  a, n3);
+	}
+      else
+	{
+	  mpn_kara_sqr_n   (ws, p, n3, ws + n1);	 /* (x-y)^2 */
+	  mpn_kara_sqr_n   (p,  a, n3, ws + n1);	 /* x^2	    */
+	}
+      if (BELOW_THRESHOLD (n2, SQR_BASECASE_THRESHOLD))
+	mpn_mul_basecase (p + n1, a + n3, n2, a + n3, n2);
+      else if (BELOW_THRESHOLD (n2, SQR_KARATSUBA_THRESHOLD))
+	mpn_sqr_basecase (p + n1, a + n3, n2);
+      else
+	mpn_kara_sqr_n   (p + n1, a + n3, n2, ws + n1);	 /* y^2	    */
+
+      /* Since x^2+y^2-(x-y)^2 = 2xy >= 0 there's no need to track the
+	 borrow from mpn_sub_n.	 If it occurs then it'll be cancelled by a
+	 carry from ws[n].  Further, since 2xy fits in n1 limbs there won't
+	 be any carry out of ws[n] other than cancelling that borrow. */
+
+      mpn_sub_n (ws, p, ws, n1);	     /* x^2-(x-y)^2 */
+
+      nm1 = n - 1;
+      if (mpn_add_n (ws, p + n1, ws, nm1))   /* x^2+y^2-(x-y)^2 = 2xy */
+	{
+	  mp_limb_t x = (ws[nm1] + 1) & GMP_NUMB_MASK;
+	  ws[nm1] = x;
+	  if (x == 0)
+	    ws[n] = (ws[n] + 1) & GMP_NUMB_MASK;
+	}
+      if (mpn_add_n (p + n3, p + n3, ws, n1))
+	{
+	  mpn_incr_u (p + n1 + n3, 1);
+	}
+    }
+  else
+    {
+      /* Even length. */
+      i = n2;
+      do
+	{
+	  --i;
+	  w0 = a[i];
+	  w1 = a[n2 + i];
+	}
+      while (w0 == w1 && i != 0);
+      if (w0 < w1)
+	{
+	  x = a + n2;
+	  y = a;
+	}
+      else
+	{
+	  x = a;
+	  y = a + n2;
+	}
+      mpn_sub_n (p, x, y, n2);
+
+      /* Pointwise products. */
+      if (BELOW_THRESHOLD (n2, SQR_BASECASE_THRESHOLD))
+	{
+	  mpn_mul_basecase (ws,    p,      n2, p,      n2);
+	  mpn_mul_basecase (p,     a,      n2, a,      n2);
+	  mpn_mul_basecase (p + n, a + n2, n2, a + n2, n2);
+	}
+      else if (BELOW_THRESHOLD (n2, SQR_KARATSUBA_THRESHOLD))
+	{
+	  mpn_sqr_basecase (ws,    p,      n2);
+	  mpn_sqr_basecase (p,     a,      n2);
+	  mpn_sqr_basecase (p + n, a + n2, n2);
+	}
+      else
+	{
+	  mpn_kara_sqr_n (ws,    p,      n2, ws + n);
+	  mpn_kara_sqr_n (p,     a,      n2, ws + n);
+	  mpn_kara_sqr_n (p + n, a + n2, n2, ws + n);
+	}
+
+      /* Interpolate. */
+      w = -mpn_sub_n (ws, p, ws, n);
+      w += mpn_add_n (ws, p + n, ws, n);
+      w += mpn_add_n (p + n2, p + n2, ws, n);
+      MPN_INCR_U (p + n2 + n, 2 * n - (n2 + n), w);
+    }
+}
+
+/******************************************************************************
+ *                                                                            *
+ *              Toom 3-way multiplication and squaring                        *
+ *                                                                            *
+ *****************************************************************************/
+
+/* Starts from:
+   {v0,2k}    (stored in {c,2k})
+   {vm1,2k+1} (which sign is sa, and absolute value is stored in {vm1,2k+1})
+   {v1,2k+1}  (stored in {c+2k,2k+1})
+   {v2,2k+1}
+   {vinf,twor}  (stored in {c+4k,twor}, except the first limb, saved in vinf0)
+
+   ws is temporary space, and should have at least twor limbs.
+
+   put in {c, 2n} where n = 2k+twor the value of {v0,2k} (already in place)
+   + B^k * {tm1, 2k+1}
+   + B^(2k) * {t1, 2k+1}
+   + B^(3k) * {t2, 2k+1}
+   + B^(4k) * {vinf,twor} (high twor-1 limbs already in place)
+   where {t1, 2k+1} = ({v1, 2k+1} + sa * {vm1, 2k+1}- 2*{v0,2k})/2-*{vinf,twor}
+	 {t2, 2k+1} = (3*({v1,2k+1}-{v0,2k})-sa*{vm1,2k+1}+{v2,2k+1})/6-2*{vinf,twor}
+	 {tm1,2k+1} = ({v1,2k+1}-sa*{vm1,2k+1}/2-{t2,2k+1}
+
+   Exact sequence described in a comment in mpn_toom3_mul_n.
+   mpn_toom3_mul_n() and mpn_toom3_sqr_n() implement steps 1-2.
+   mpn_toom_interpolate_5pts() implements steps 3-4.
+
+   Reference: What About Toom-Cook Matrices Optimality? Marco Bodrato
+   and Alberto Zanoni, October 19, 2006, http://bodrato.it/papers/#CIVV2006
+
+   ************* saved note ****************
+   Think about:
+
+   The evaluated point a-b+c stands a good chance of having a zero carry
+   limb, a+b+c would have a 1/4 chance, and 4*a+2*b+c a 1/8 chance, roughly.
+   Perhaps this could be tested and stripped.  Doing so before recursing
+   would be better than stripping at the start of mpn_toom3_mul_n/sqr_n,
+   since then the recursion could be based on the new size.  Although in
+   truth the kara vs toom3 crossover is never so exact that one limb either
+   way makes a difference.
+
+   A small value like 1 or 2 for the carry could perhaps also be handled
+   with an add_n or addlsh1_n.  Would that be faster than an extra limb on a
+   (recursed) multiply/square?
+*/
+
+#define TOOM3_MUL_REC(p, a, b, n, ws) \
+  do {								\
+    if (MUL_TOOM3_THRESHOLD / 3 < MUL_KARATSUBA_THRESHOLD	\
+	&& BELOW_THRESHOLD (n, MUL_KARATSUBA_THRESHOLD))	\
+      mpn_mul_basecase (p, a, n, b, n);				\
+    else if (BELOW_THRESHOLD (n, MUL_TOOM3_THRESHOLD))		\
+      mpn_kara_mul_n (p, a, b, n, ws);				\
+    else							\
+      mpn_toom3_mul_n (p, a, b, n, ws);				\
+  } while (0)
+
+#define TOOM3_SQR_REC(p, a, n, ws)				\
+  do {								\
+    if (SQR_TOOM3_THRESHOLD / 3 < SQR_BASECASE_THRESHOLD	\
+	&& BELOW_THRESHOLD (n, SQR_BASECASE_THRESHOLD))		\
+      mpn_mul_basecase (p, a, n, a, n);				\
+    else if (SQR_TOOM3_THRESHOLD / 3 < SQR_KARATSUBA_THRESHOLD	\
+	&& BELOW_THRESHOLD (n, SQR_KARATSUBA_THRESHOLD))	\
+      mpn_sqr_basecase (p, a, n);				\
+    else if (BELOW_THRESHOLD (n, SQR_TOOM3_THRESHOLD))		\
+      mpn_kara_sqr_n (p, a, n, ws);				\
+    else							\
+      mpn_toom3_sqr_n (p, a, n, ws);				\
+  } while (0)
+
+/* The necessary temporary space T(n) satisfies T(n)=0 for n < THRESHOLD,
+   and T(n) <= max(2n+2, 6k+3, 4k+3+T(k+1)) otherwise, where k = ceil(n/3).
+
+   Assuming T(n) >= 2n, 6k+3 <= 4k+3+T(k+1).
+   Similarly, 2n+2 <= 6k+2 <= 4k+3+T(k+1).
+
+   With T(n) = 2n+S(n), this simplifies to S(n) <= 9 + S(k+1).
+   Since THRESHOLD >= 17, we have n/(k+1) >= 19/8
+   thus S(n) <= S(n/(19/8)) + 9 thus S(n) <= 9*log(n)/log(19/8) <= 8*log2(n).
+*/
+
+void
+mpn_toom3_mul_n (mp_ptr c, mp_srcptr a, mp_srcptr b, mp_size_t n, mp_ptr t)
+{
+  mp_size_t k, k1, kk1, r, twok, twor;
+  mp_limb_t cy, cc, saved, vinf0;
+  mp_ptr trec;
+  int sa, sb;
+  mp_ptr c1, c2, c3, c4, c5;
+
+  ASSERT(GMP_NUMB_BITS >= 6);
+  ASSERT(n >= 17); /* so that r <> 0 and 5k+3 <= 2n */
+
+  /*
+  The algorithm is the following:
+
+  0. k = ceil(n/3), r = n - 2k, B = 2^(GMP_NUMB_BITS), t = B^k
+  1. split a and b in three parts each a0, a1, a2 and b0, b1, b2
+     with a0, a1, b0, b1 of k limbs, and a2, b2 of r limbs
+  2. Evaluation: vm1 may be negative, the other can not.
+     v0   <- a0*b0
+     v1   <- (a0+a1+a2)*(b0+b1+b2)
+     v2   <- (a0+2*a1+4*a2)*(b0+2*b1+4*b2)
+     vm1  <- (a0-a1+a2)*(b0-b1+b2)
+     vinf <- a2*b2
+  3. Interpolation: every result is positive, all divisions are exact
+     t2   <- (v2 - vm1)/3
+     tm1  <- (v1 - vm1)/2
+     t1   <- (v1 - v0)
+     t2   <- (t2 - t1)/2
+     t1   <- (t1 - tm1 - vinf)
+     t2   <- (t2 - 2*vinf)
+     tm1  <- (tm1 - t2)
+  4. result is c0+c1*t+c2*t^2+c3*t^3+c4*t^4 where
+     c0   <- v0
+     c1   <- tm1
+     c2   <- t1
+     c3   <- t2
+     c4   <- vinf
+  */
+
+  k = (n + 2) / 3; /* ceil(n/3) */
+  twok = 2 * k;
+  k1 = k + 1;
+  kk1 = k + k1;
+  r = n - twok;   /* last chunk */
+  twor = 2 * r;
+
+  c1 = c + k;
+  c2 = c1 + k;
+  c3 = c2 + k;
+  c4 = c3 + k;
+  c5 = c4 + k;
+
+  trec = t + 4 * k + 3; /* trec = v2 + (2k+2) */
+
+  /* put a0+a2 in {c, k+1}, and b0+b2 in {c+4k+2, k+1};
+     put a0+a1+a2 in {t, k+1} and b0+b1+b2 in {t+k+1,k+1}
+     [????requires 5k+3 <= 2n, ie. n >= 9] */
+  cy = mpn_add_n (c,      a, a + twok, r);
+  cc = mpn_add_n (c4 + 2, b, b + twok, r);
+  if (r < k)
+    {
+      __GMPN_ADD_1 (cy, c + r,      a + r, k - r, cy);
+      __GMPN_ADD_1 (cc, c4 + 2 + r, b + r, k - r, cc);
+    }
+
+  /* Put in {t, k+1} the sum
+   * (a_0+a_2) - stored in {c, k+1} -
+   * +
+   * a_1       - stored in {a+k, k} */
+  t[k] = (c1[0] = cy) + mpn_add_n (t, c, a + k, k);
+  /*          ^              ^
+   * carry of a_0 + a_2    carry of (a_0+a_2) + a_1
+
+   */
+
+  /* Put in {t+k+1, k+1} the sum of the two values
+   * (b_0+b_2) - stored in {c1+1, k+1} -
+   * +
+   * b_1       - stored in {b+k, k} */
+  t[kk1] = (c5[3] = cc) + mpn_add_n (t + k1, c4 + 2, b + k, k);
+  /*          ^              ^
+   * carry of b_0 + b_2    carry of (b_0+b_2) + b_1 */
+
+#define v2 (t+2*k+1)
+
+  /* compute v1 := (a0+a1+a2)*(b0+b1+b2) in {t, 2k+1};
+     since v1 < 9*B^(2k), v1 uses only 2k+1 words if GMP_NUMB_BITS >= 4 */
+  TOOM3_MUL_REC (c2, t, t + k1, k1, trec);
+
+  /*   c         c2    c4                 t
+     {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+		 v1                                            */
+
+  /* put |a0-a1+a2| in {c, k+1} and |b0-b1+b2| in {c+4k+2,k+1} */
+  /* (They're already there, actually)                         */
+
+  /* sa = sign(a0-a1+a2) */
+  sa   = (cy != 0) ? 1 : mpn_cmp (c, a + k, k);
+  c[k] = (sa >= 0) ? cy - mpn_sub_n (c, c, a + k, k)
+		   : mpn_sub_n (c, a + k, c, k);
+
+  sb    = (cc != 0) ? 1 : mpn_cmp (c4 + 2, b + k, k);
+  c5[2] = (sb >= 0) ? cc - mpn_sub_n (c4 + 2, c4 + 2, b + k, k)
+		    : mpn_sub_n (c4 + 2, b + k, c4 + 2, k);
+  sa *= sb; /* sign of vm1 */
+
+  /* compute vm1 := (a0-a1+a2)*(b0-b1+b2) in {t, 2k+1};
+     since |vm1| < 4*B^(2k), vm1 uses only 2k+1 limbs */
+  TOOM3_MUL_REC (t, c, c4 + 2, k1, trec);
+
+  /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+		v1                      vm1
+  */
+
+  /* compute a0+2a1+4a2 in {c, k+1} and b0+2b1+4b2 in {c+4k+2, k+1}
+     [requires 5k+3 <= 2n, i.e. n >= 17] */
+#ifdef HAVE_NATIVE_mpn_addlsh1_n
+  c1[0] = mpn_addlsh1_n (c, a + k, a + twok, r);
+  c5[2] = mpn_addlsh1_n (c4 + 2, b + k, b + twok, r);
+  if (r < k)
+    {
+      __GMPN_ADD_1 (c1[0], c + r, a + k + r, k - r, c1[0]);
+      __GMPN_ADD_1 (c5[2], c4 + 2 + r, b + k + r, k - r, c5[2]);
+    }
+  c1[0] = 2 * c1[0] + mpn_addlsh1_n (c, a, c, k);
+  c5[2] = 2 * c5[2] + mpn_addlsh1_n (c4 + 2, b, c4 + 2, k);
+#else
+  c[r] = mpn_lshift (c, a + twok, r, 1);
+  c4[r + 2] = mpn_lshift (c4 + 2, b + twok, r, 1);
+  if (r < k)
+    {
+      MPN_ZERO(c + r + 1, k - r);
+      MPN_ZERO(c4 + r + 3, k - r);
+    }
+  c1[0] += mpn_add_n (c, c, a + k, k);
+  c5[2] += mpn_add_n (c4 + 2, c4 + 2, b + k, k);
+  mpn_lshift (c, c, k1, 1);
+  mpn_lshift (c4 + 2, c4 + 2, k1, 1);
+  c1[0] += mpn_add_n (c, c, a, k);
+  c5[2] += mpn_add_n (c4 + 2, c4 + 2, b, k);
+#endif
+
+  /* compute v2 := (a0+2a1+4a2)*(b0+2b1+4b2) in {t+2k+1, 2k+1}
+     v2 < 49*B^k so v2 uses at most 2k+1 limbs if GMP_NUMB_BITS >= 6 */
+  TOOM3_MUL_REC (v2, c, c4 + 2, k1, trec);
+
+  /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+		v1                      vm1         v2
+  */
+
+  /* compute v0 := a0*b0 in {c, 2k} */
+  TOOM3_MUL_REC (c, a, b, k, trec);
+
+  /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+       v0       v1                      vm1       v2                   */
+
+  /* compute vinf := a2*b2 in {t+4k+2, 2r}: in {c4, 2r} */
+
+  saved = c4[0];              /* Remember v1's highest byte (will be overwritten). */
+  TOOM3_MUL_REC (c4, a + twok, b + twok, r, trec);           /* Overwrites c4[0].  */
+  vinf0 = c4[0];              /* Remember vinf's lowest byte (will be overwritten).*/
+  c4[0] = saved;              /* Overwriting. Now v1 value is correct.             */
+
+  /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+       v0       v1       vinf[1..]      vm1       v2               */
+
+  mpn_toom_interpolate_5pts (c, v2, t, k, 2*r, sa, vinf0, trec);
+
+#undef v2
+}
+
+void
+mpn_toom3_sqr_n (mp_ptr c, mp_srcptr a, mp_size_t n, mp_ptr t)
+{
+  mp_size_t k, k1, kk1, r, twok, twor;
+  mp_limb_t cy, saved, vinf0;
+  mp_ptr trec;
+  int sa;
+  mp_ptr c1, c2, c3, c4;
+
+  ASSERT(GMP_NUMB_BITS >= 6);
+  ASSERT(n >= 17); /* so that r <> 0 and 5k+3 <= 2n */
+
+  /* the algorithm is the same as mpn_toom3_mul_n, with b=a */
+
+  k = (n + 2) / 3; /* ceil(n/3) */
+  twok = 2 * k;
+  k1 = k + 1;
+  kk1 = k + k1;
+  r = n - twok;   /* last chunk */
+  twor = 2 * r;
+
+  c1 = c + k;
+  c2 = c1 + k;
+  c3 = c2 + k;
+  c4 = c3 + k;
+
+  trec = t + 4 * k + 3; /* trec = v2 + (2k+2) */
+
+  cy = mpn_add_n (c, a, a + twok, r);
+  if (r < k)
+    __GMPN_ADD_1 (cy, c + r, a + r, k - r, cy);
+  t[k] = (c1[0] = cy) + mpn_add_n (t, c, a + k, k);
+
+#define v2 (t+2*k+1)
+
+  TOOM3_SQR_REC (c2, t, k1, trec);
+
+  sa = (cy != 0) ? 1 : mpn_cmp (c, a + k, k);
+  c[k] = (sa >= 0) ? cy - mpn_sub_n (c, c, a + k, k)
+    : mpn_sub_n (c, a + k, c, k);
+
+  TOOM3_SQR_REC (t, c, k1, trec);
+
+#ifdef HAVE_NATIVE_mpn_addlsh1_n
+  c1[0] = mpn_addlsh1_n (c, a + k, a + twok, r);
+  if (r < k)
+    __GMPN_ADD_1 (c1[0], c + r, a + k + r, k - r, c1[0]);
+  c1[0] = 2 * c1[0] + mpn_addlsh1_n (c, a, c, k);
+#else
+  c[r] = mpn_lshift (c, a + twok, r, 1);
+  if (r < k)
+    MPN_ZERO(c + r + 1, k - r);
+  c1[0] += mpn_add_n (c, c, a + k, k);
+  mpn_lshift (c, c, k1, 1);
+  c1[0] += mpn_add_n (c, c, a, k);
+#endif
+
+  TOOM3_SQR_REC (v2, c, k1, trec);
+
+  TOOM3_SQR_REC (c, a, k, trec);
+
+  saved = c4[0];
+  TOOM3_SQR_REC (c4, a + twok, r, trec);
+  vinf0 = c4[0];
+  c4[0] = saved;
+
+  mpn_toom_interpolate_5pts (c, v2, t, k, 2*r,  1, vinf0, trec);
+
+#undef v2
+}
+
 void
 mpn_mul_n (mp_ptr p, mp_srcptr a, mp_srcptr b, mp_size_t n)
 {
@@ -29,25 +697,24 @@
   ASSERT (! MPN_OVERLAP_P (p, 2 * n, a, n));
   ASSERT (! MPN_OVERLAP_P (p, 2 * n, b, n));
 
-  if (BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))
+  if (BELOW_THRESHOLD (n, MUL_KARATSUBA_THRESHOLD))
     {
       mpn_mul_basecase (p, a, n, b, n);
     }
-  else if (BELOW_THRESHOLD (n, MUL_TOOM33_THRESHOLD))
+  else if (BELOW_THRESHOLD (n, MUL_TOOM3_THRESHOLD))
     {
       /* Allocate workspace of fixed size on stack: fast! */
-      mp_limb_t ws[mpn_toom22_mul_itch (MUL_TOOM33_THRESHOLD_LIMIT-1,
-					MUL_TOOM33_THRESHOLD_LIMIT-1)];
-      ASSERT (MUL_TOOM33_THRESHOLD <= MUL_TOOM33_THRESHOLD_LIMIT);
-      mpn_toom22_mul (p, a, n, b, n, ws);
+      mp_limb_t ws[MPN_KARA_MUL_N_TSIZE (MUL_TOOM3_THRESHOLD_LIMIT-1)];
+      ASSERT (MUL_TOOM3_THRESHOLD <= MUL_TOOM3_THRESHOLD_LIMIT);
+      mpn_kara_mul_n (p, a, b, n, ws);
     }
   else if (BELOW_THRESHOLD (n, MUL_TOOM44_THRESHOLD))
     {
       mp_ptr ws;
       TMP_SDECL;
       TMP_SMARK;
-      ws = TMP_SALLOC_LIMBS (mpn_toom33_mul_itch (n, n));
-      mpn_toom33_mul (p, a, n, b, n, ws);
+      ws = TMP_SALLOC_LIMBS (MPN_TOOM3_MUL_N_TSIZE (n));
+      mpn_toom3_mul_n (p, a, b, n, ws);
       TMP_SFREE;
     }
 #if WANT_FFT || TUNE_PROGRAM_BUILD
@@ -81,4 +748,73 @@
       TMP_FREE;
     }
 #endif
+}
+
+void
+mpn_sqr (mp_ptr p, mp_srcptr a, mp_size_t n)
+{
+  ASSERT (n >= 1);
+  ASSERT (! MPN_OVERLAP_P (p, 2 * n, a, n));
+
+#if 0
+  /* FIXME: Can this be removed? */
+  if (n == 0)
+    return;
+#endif
+
+  if (BELOW_THRESHOLD (n, SQR_BASECASE_THRESHOLD))
+    { /* mul_basecase is faster than sqr_basecase on small sizes sometimes */
+      mpn_mul_basecase (p, a, n, a, n);
+    }
+  else if (BELOW_THRESHOLD (n, SQR_KARATSUBA_THRESHOLD))
+    {
+      mpn_sqr_basecase (p, a, n);
+    }
+  else if (BELOW_THRESHOLD (n, SQR_TOOM3_THRESHOLD))
+    {
+      /* Allocate workspace of fixed size on stack: fast! */
+      mp_limb_t ws[MPN_KARA_SQR_N_TSIZE (SQR_TOOM3_THRESHOLD_LIMIT-1)];
+      ASSERT (SQR_TOOM3_THRESHOLD <= SQR_TOOM3_THRESHOLD_LIMIT);
+      mpn_kara_sqr_n (p, a, n, ws);
+    }
+  else if (BELOW_THRESHOLD (n, SQR_TOOM4_THRESHOLD))
+    {
+      mp_ptr ws;
+      TMP_SDECL;
+      TMP_SMARK;
+      ws = TMP_SALLOC_LIMBS (MPN_TOOM3_SQR_N_TSIZE (n));
+      mpn_toom3_sqr_n (p, a, n, ws);
+      TMP_SFREE;
+    }
+#if WANT_FFT || TUNE_PROGRAM_BUILD
+  else if (BELOW_THRESHOLD (n, SQR_FFT_THRESHOLD))
+#else
+  else if (BELOW_THRESHOLD (n, MPN_TOOM44_MAX_N))
+#endif
+    {
+      mp_ptr ws;
+      TMP_SDECL;
+      TMP_SMARK;
+      ws = TMP_SALLOC_LIMBS (mpn_toom4_sqr_itch (n));
+      mpn_toom4_sqr (p, a, n, ws);
+      TMP_SFREE;
+    }
+  else
+#if WANT_FFT || TUNE_PROGRAM_BUILD
+    {
+      /* The current FFT code allocates its own space.  That should probably
+	 change.  */
+      mpn_mul_fft_full (p, a, n, a, n);
+    }
+#else
+    {
+      /* Toom4 for large operands.  */
+      mp_ptr ws;
+      TMP_DECL;
+      TMP_MARK;
+      ws = TMP_BALLOC_LIMBS (mpn_toom4_sqr_itch (n));
+      mpn_toom4_sqr (p, a, n, ws);
+      TMP_FREE;
+    }
+#endif
 }
--- 1/mpn/generic/perfsqr.c
+++ 2/mpn/generic/perfsqr.c
@@ -218,7 +218,7 @@
     TMP_DECL;
 
     TMP_MARK;
-    root_ptr = TMP_ALLOC_LIMBS ((usize + 1) / 2);
+    root_ptr = (mp_ptr) TMP_ALLOC ((usize + 1) / 2 * BYTES_PER_MP_LIMB);
 
     /* Iff mpn_sqrtrem returns zero, the square is perfect.  */
     res = ! mpn_sqrtrem (root_ptr, NULL, up, usize);
--- 1/mpn/generic/pow_1.c
+++ 2/mpn/generic/pow_1.c
@@ -34,9 +34,6 @@
   mp_size_t rn;
   int par;
 
-  ASSERT (bn >= 1);
-  /* FIXME: Add operand overlap criteria */
-
   if (exp <= 1)
     {
       if (exp == 0)
--- 1/mpn/generic/powlo.c
+++ 2/mpn/generic/powlo.c
@@ -1,4 +1,4 @@
-/* mpn_powlo -- Compute R = U^E mod B^n, where B is the limb base.
+/* mpn_powlo -- Compute R = U^E mod R^n, where R is the limb base.
 
 Copyright 2007, 2008, 2009 Free Software Foundation, Inc.
 
@@ -60,7 +60,7 @@
   return k;
 }
 
-/* rp[n-1..0] = bp[n-1..0] ^ ep[en-1..0] mod B^n, B is the limb base.
+/* rp[n-1..0] = bp[n-1..0] ^ ep[en-1..0] mod R^n, R is the limb base.
    Requires that ep[en-1] is non-zero.
    Uses scratch space tp[3n-1..0], i.e., 3n words.  */
 void
@@ -86,7 +86,7 @@
 
   windowsize = win_size (ebi);
 
-  pp = TMP_ALLOC_LIMBS ((n << (windowsize - 1)) + n); /* + n is for mullo ign part */
+  pp = TMP_ALLOC_LIMBS ((n << (windowsize - 1)) + n); /* + n is for mullow ign part */
 
   this_pp = pp;
 
@@ -103,7 +103,7 @@
     {
       last_pp = this_pp;
       this_pp += n;
-      mpn_mullo_n (this_pp, last_pp, b2p, n);
+      mpn_mullow_n (this_pp, last_pp, b2p, n);
     }
 
   expbits = getbits (ep, ebi, windowsize);
@@ -153,7 +153,7 @@
 	}
       while (this_windowsize != 0);
 
-      mpn_mullo_n (tp, rp, pp + n * (expbits >> 1), n);
+      mpn_mullow_n (tp, rp, pp + n * (expbits >> 1), n);
       MPN_COPY (rp, tp, n);
     }
 
--- 1/mpn/generic/powm.c
+++ 2/mpn/generic/powm.c
@@ -1,11 +1,5 @@
 /* mpn_powm -- Compute R = U^E mod M.
 
-   Contributed to the GNU project by Torbjorn Granlund.
-
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GNU MP RELEASE.
-
 Copyright 2007, 2008, 2009 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
@@ -25,16 +19,19 @@
 
 
 /*
-  BASIC ALGORITHM, Compute U^E mod M, where M < B^n is odd.
+  BASIC ALGORITHM, Compute b^e mod n, where n is odd.
+
+  1. w <- b
 
-  1. W <- U
+  2. While w^2 < n (and there are more bits in e)
+       w <- power left-to-right base-2 without reduction
 
-  2. T <- (B^n * U) mod M                Convert to REDC form
+  3. t <- (B^n * b) / n                Convert to REDC form
 
-  3. Compute table U^1, U^3, U^5... of E-dependent size
+  4. Compute power table of e-dependent size
 
-  4. While there are more bits in E
-       W <- power left-to-right base-k
+  5. While there are more bits in e
+       w <- power left-to-right base-k with reduction
 
 
   TODO:
@@ -47,41 +44,47 @@
 
    * Choose window size without looping.  (Superoptimize or think(tm).)
 
-   * Handle small bases with initial, reduction-free exponentiation.
+   * How do we handle small bases?
+
+   * This is slower than old mpz code, in particular if we base it on redc_1
+     (use: #undef HAVE_NATIVE_mpn_addmul_2).  Why?
+
+   * Make it sub-quadratic.
 
    * Call new division functions, not mpn_tdiv_qr.
 
+   * Is redc obsolete with improved SB division?
+
    * Consider special code for one-limb M.
 
-   * How should we handle the redc1/redc2/redc_n choice?
-     - redc1:  T(binvert_1limb)  + e * (n)   * (T(mullo-1x1) + n*T(addmul_1))
-     - redc2:  T(binvert_2limbs) + e * (n/2) * (T(mullo-2x2) + n*T(addmul_2))
-     - redc_n: T(binvert_nlimbs) + e * (T(mullo-nxn) + T(M(n)))
+   * CRT for N = odd*2^t:
+      Using Newton's method and 2-adic arithmetic:
+        m1_inv_m2 = 1/odd mod 2^t
+      Plain 2-adic (REDC) modexp:
+        r1 = a ^ b mod odd
+      Mullo+sqrlo-based modexp:
+        r2 = a ^ b mod 2^t
+      mullo, mul, add:
+        r = ((r2 - r1) * m1_i_m2 mod 2^t) * odd + r1
+
+   * How should we handle the redc1/redc2/redc2/redc4/redc_subquad choice?
+     - redc1: T(binvert_1limb)  + e * (n)   * (T(mullo1x1) + n*T(addmul_1))
+     - redc2: T(binvert_2limbs) + e * (n/2) * (T(mullo2x2) + n*T(addmul_2))
+     - redc3: T(binvert_3limbs) + e * (n/3) * (T(mullo3x3) + n*T(addmul_3))
      This disregards the addmul_N constant term, but we could think of
-     that as part of the respective mullo.
-
-   * When U (the base) is small, we should start the exponentiation with plain
-     operations, then convert that partial result to REDC form.
-
-   * When U is just one limb, should it be handled without the k-ary tricks?
-     We could keep a factor of B^n in W, but use U' = BU as base.  After
-     multiplying by this (pseudo two-limb) number, we need to multiply by 1/B
-     mod M.
+     that as part of the respective mulloNxN.
 */
 
 #include "gmp.h"
 #include "gmp-impl.h"
 #include "longlong.h"
 
-#if HAVE_NATIVE_mpn_addmul_2 || HAVE_NATIVE_mpn_redc_2
-#define WANT_REDC_2 1
-#endif
 
 #define getbit(p,bi) \
   ((p[(bi - 1) / GMP_LIMB_BITS] >> (bi - 1) % GMP_LIMB_BITS) & 1)
 
 static inline mp_limb_t
-getbits (const mp_limb_t *p, mp_bitcnt_t bi, int nbits)
+getbits (const mp_limb_t *p, unsigned long bi, int nbits)
 {
   int nbits_in_r;
   mp_limb_t r;
@@ -94,27 +97,49 @@
   else
     {
       bi -= nbits;			/* bit index of low bit to extract */
-      i = bi / GMP_NUMB_BITS;		/* word index of low bit to extract */
-      bi %= GMP_NUMB_BITS;		/* bit index in low word */
+      i = bi / GMP_LIMB_BITS;		/* word index of low bit to extract */
+      bi %= GMP_LIMB_BITS;		/* bit index in low word */
       r = p[i] >> bi;			/* extract (low) bits */
-      nbits_in_r = GMP_NUMB_BITS - bi;	/* number of bits now in r */
+      nbits_in_r = GMP_LIMB_BITS - bi;	/* number of bits now in r */
       if (nbits_in_r < nbits)		/* did we get enough bits? */
 	r += p[i + 1] << nbits_in_r;	/* prepend bits from higher word */
       return r & (((mp_limb_t ) 1 << nbits) - 1);
     }
 }
 
+#undef HAVE_NATIVE_mpn_addmul_2
+
+#ifndef HAVE_NATIVE_mpn_addmul_2
+#define REDC_2_THRESHOLD		MP_SIZE_T_MAX
+#endif
+
+#ifndef REDC_2_THRESHOLD
+#define REDC_2_THRESHOLD		4
+#endif
+
+static void mpn_redc_n () {ASSERT_ALWAYS(0);}
+
 static inline int
-win_size (mp_bitcnt_t eb)
+win_size (unsigned long eb)
 {
   int k;
-  static mp_bitcnt_t x[] = {1,7,25,81,241,673,1793,4609,11521,28161,~0ul};
+  static unsigned long x[] = {1,7,25,81,241,673,1793,4609,11521,28161,~0ul};
   for (k = 0; eb > x[k]; k++)
     ;
   return k;
 }
 
-/* Convert U to REDC form, U_r = B^n * U mod M */
+#define MPN_REDC_X(rp, tp, mp, n, mip)					\
+  do {									\
+    if (redc_x == 1)							\
+      mpn_redc_1 (rp, tp, mp, n, mip[0]);				\
+    else if (redc_x == 2)						\
+      mpn_redc_2 (rp, tp, mp, n, mip);					\
+    else								\
+      mpn_redc_n (rp, tp, mp, n, mip);					\
+  } while (0)
+
+  /* Convert U to REDC form, U_r = B^n * U mod M */
 static void
 redcify (mp_ptr rp, mp_srcptr up, mp_size_t un, mp_srcptr mp, mp_size_t n)
 {
@@ -140,14 +165,15 @@
 	  mp_srcptr ep, mp_size_t en,
 	  mp_srcptr mp, mp_size_t n, mp_ptr tp)
 {
-  mp_limb_t ip[2], *mip;
+  mp_limb_t mip[2];
   int cnt;
-  mp_bitcnt_t ebi;
+  long ebi;
   int windowsize, this_windowsize;
   mp_limb_t expbits;
-  mp_ptr pp, this_pp;
+  mp_ptr pp, this_pp, last_pp;
   mp_ptr b2p;
   long i;
+  int redc_x;
   TMP_DECL;
 
   ASSERT (en > 1 || (en == 1 && ep[0] > 1));
@@ -156,7 +182,7 @@
   TMP_MARK;
 
   count_leading_zeros (cnt, ep[en - 1]);
-  ebi = (mp_bitcnt_t) en * GMP_LIMB_BITS - cnt;
+  ebi = en * GMP_LIMB_BITS - cnt;
 
 #if 0
   if (bn < n)
@@ -176,32 +202,24 @@
 
   windowsize = win_size (ebi);
 
-#if WANT_REDC_2
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_2_THRESHOLD))
+  if (BELOW_THRESHOLD (n, REDC_2_THRESHOLD))
     {
-      mip = ip;
       binvert_limb (mip[0], mp[0]);
       mip[0] = -mip[0];
+      redc_x = 1;
     }
-  else if (BELOW_THRESHOLD (n, REDC_2_TO_REDC_N_THRESHOLD))
+#if defined (HAVE_NATIVE_mpn_addmul_2)
+  else
     {
-      mip = ip;
       mpn_binvert (mip, mp, 2, tp);
       mip[0] = -mip[0]; mip[1] = ~mip[1];
-    }
-#else
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_N_THRESHOLD))
-    {
-      mip = ip;
-      binvert_limb (mip[0], mp[0]);
-      mip[0] = -mip[0];
+      redc_x = 2;
     }
 #endif
-  else
-    {
-      mip = TMP_ALLOC_LIMBS (n);
-      mpn_binvert (mip, mp, n, tp);
-    }
+#if 0
+  mpn_binvert (mip, mp, n, tp);
+  redc_x = 0;
+#endif
 
   pp = TMP_ALLOC_LIMBS (n << (windowsize - 1));
 
@@ -212,41 +230,21 @@
 
   /* Store b^2 in b2.  */
   mpn_sqr_n (tp, this_pp, n);
-#if WANT_REDC_2
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_2_THRESHOLD))
-    mpn_redc_1 (b2p, tp, mp, n, mip[0]);
-  else if (BELOW_THRESHOLD (n, REDC_2_TO_REDC_N_THRESHOLD))
-    mpn_redc_2 (b2p, tp, mp, n, mip);
-#else
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_N_THRESHOLD))
-    mpn_redc_1 (b2p, tp, mp, n, mip[0]);
-#endif
-  else
-    mpn_redc_n (b2p, tp, mp, n, mip);
+  MPN_REDC_X (b2p, tp, mp, n, mip);
 
   /* Precompute odd powers of b and put them in the temporary area at pp.  */
   for (i = (1 << (windowsize - 1)) - 1; i > 0; i--)
     {
-      mpn_mul_n (tp, this_pp, b2p, n);
+      last_pp = this_pp;
       this_pp += n;
-#if WANT_REDC_2
-      if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_2_THRESHOLD))
-	mpn_redc_1 (this_pp, tp, mp, n, mip[0]);
-      else if (BELOW_THRESHOLD (n, REDC_2_TO_REDC_N_THRESHOLD))
-	mpn_redc_2 (this_pp, tp, mp, n, mip);
-#else
-      if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_N_THRESHOLD))
-	mpn_redc_1 (this_pp, tp, mp, n, mip[0]);
-#endif
-      else
-	mpn_redc_n (this_pp, tp, mp, n, mip);
+      mpn_mul_n (tp, last_pp, b2p, n);
+      MPN_REDC_X (this_pp, tp, mp, n, mip);
     }
 
   expbits = getbits (ep, ebi, windowsize);
-  if (ebi < windowsize)
+  ebi -= windowsize;
+  if (ebi < 0)
     ebi = 0;
-  else
-    ebi -= windowsize;
 
   count_trailing_zeros (cnt, expbits);
   ebi += cnt;
@@ -254,190 +252,51 @@
 
   MPN_COPY (rp, pp + n * (expbits >> 1), n);
 
-#define INNERLOOP							\
-  while (ebi != 0)							\
-    {									\
-      while (getbit (ep, ebi) == 0)					\
-	{								\
-	  MPN_SQR_N (tp, rp, n);					\
-	  MPN_REDUCE (rp, tp, mp, n, mip);				\
-	  ebi--;							\
-	  if (ebi == 0)							\
-	    goto done;							\
-	}								\
-									\
-      /* The next bit of the exponent is 1.  Now extract the largest	\
-	 block of bits <= windowsize, and such that the least		\
-	 significant bit is 1.  */					\
-									\
-      expbits = getbits (ep, ebi, windowsize);				\
-      this_windowsize = windowsize;					\
-      if (ebi < windowsize)						\
-	{								\
-	  this_windowsize -= windowsize - ebi;				\
-	  ebi = 0;							\
-	}								\
-      else								\
-        ebi -= windowsize;						\
-									\
-      count_trailing_zeros (cnt, expbits);				\
-      this_windowsize -= cnt;						\
-      ebi += cnt;							\
-      expbits >>= cnt;							\
-									\
-      do								\
-	{								\
-	  MPN_SQR_N (tp, rp, n);					\
-	  MPN_REDUCE (rp, tp, mp, n, mip);				\
-	  this_windowsize--;						\
-	}								\
-      while (this_windowsize != 0);					\
-									\
-      MPN_MUL_N (tp, rp, pp + n * (expbits >> 1), n);			\
-      MPN_REDUCE (rp, tp, mp, n, mip);					\
-    }
+  while (ebi != 0)
+    {
+      while (getbit (ep, ebi) == 0)
+	{
+	  mpn_sqr_n (tp, rp, n);
+	  MPN_REDC_X (rp, tp, mp, n, mip);
+	  ebi--;
+	  if (ebi == 0)
+	    goto done;
+	}
 
+      /* The next bit of the exponent is 1.  Now extract the largest block of
+	 bits <= windowsize, and such that the least significant bit is 1.  */
 
-#if WANT_REDC_2
-#if REDC_1_TO_REDC_2_THRESHOLD < MUL_TOOM22_THRESHOLD
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_2_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_basecase (r,a,n,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_basecase (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_1 (rp, tp, mp, n, mip[0])
-      INNERLOOP;
-    }
-  else if (BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_basecase (r,a,n,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_basecase (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_2 (rp, tp, mp, n, mip)
-      INNERLOOP;
-    }
-#else
-  if (BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_basecase (r,a,n,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_basecase (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_1 (rp, tp, mp, n, mip[0])
-      INNERLOOP;
-    }
-  else if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_2_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_n (r,a,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_n (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_1 (rp, tp, mp, n, mip[0])
-      INNERLOOP;
-    }
-#endif  /* REDC_1_TO_REDC_2_THRESHOLD < MUL_TOOM22_THRESHOLD */
-  else if (BELOW_THRESHOLD (n, REDC_2_TO_REDC_N_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_n (r,a,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_n (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_2 (rp, tp, mp, n, mip)
-      INNERLOOP;
-    }
-  else
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_n (r,a,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_n (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_n (rp, tp, mp, n, mip)
-      INNERLOOP;
-    }
+      expbits = getbits (ep, ebi, windowsize);
+      ebi -= windowsize;
+      this_windowsize = windowsize;
+      if (ebi < 0)
+	{
+	  this_windowsize += ebi;
+	  ebi = 0;
+	}
 
-#else  /* WANT_REDC_2 */
-#if REDC_1_TO_REDC_N_THRESHOLD < MUL_TOOM22_THRESHOLD
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_N_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_basecase (r,a,n,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_basecase (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_1 (rp, tp, mp, n, mip[0])
-      INNERLOOP;
-    }
-  else if (BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_basecase (r,a,n,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_basecase (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_n (rp, tp, mp, n, mip)
-      INNERLOOP;
-    }
-#else
-  if (BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_basecase (r,a,n,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_basecase (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_1 (rp, tp, mp, n, mip[0])
-      INNERLOOP;
-    }
-  else if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_N_THRESHOLD))
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_n (r,a,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_n (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_1 (rp, tp, mp, n, mip[0])
-      INNERLOOP;
-    }
-#endif
-  else
-    {
-#undef MPN_MUL_N
-#undef MPN_SQR_N
-#undef MPN_REDUCE
-#define MPN_MUL_N(r,a,b,n)		mpn_mul_n (r,a,b,n)
-#define MPN_SQR_N(r,a,n)		mpn_sqr_n (r,a,n)
-#define MPN_REDUCE(rp,tp,mp,n,mip)	mpn_redc_n (rp, tp, mp, n, mip)
-      INNERLOOP;
+      count_trailing_zeros (cnt, expbits);
+      this_windowsize -= cnt;
+      ebi += cnt;
+      expbits >>= cnt;
+
+      do
+	{
+	  mpn_sqr_n (tp, rp, n);
+	  MPN_REDC_X (rp, tp, mp, n, mip);
+	  this_windowsize--;
+	}
+      while (this_windowsize != 0);
+
+      mpn_mul_n (tp, rp, pp + n * (expbits >> 1), n);
+      MPN_REDC_X (rp, tp, mp, n, mip);
     }
-#endif  /* WANT_REDC_2 */
 
  done:
-
   MPN_COPY (tp, rp, n);
   MPN_ZERO (tp + n, n);
-
-#if WANT_REDC_2
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_2_THRESHOLD))
-    mpn_redc_1 (rp, tp, mp, n, mip[0]);
-  else if (BELOW_THRESHOLD (n, REDC_2_TO_REDC_N_THRESHOLD))
-    mpn_redc_2 (rp, tp, mp, n, mip);
-#else
-  if (BELOW_THRESHOLD (n, REDC_1_TO_REDC_N_THRESHOLD))
-    mpn_redc_1 (rp, tp, mp, n, mip[0]);
-#endif
-  else
-    mpn_redc_n (rp, tp, mp, n, mip);
-
+  MPN_REDC_X (rp, tp, mp, n, mip);
   if (mpn_cmp (rp, mp, n) >= 0)
     mpn_sub_n (rp, rp, mp, n);
-
   TMP_FREE;
 }
--- 1/mpn/generic/powm_sec.c
+++ 2/mpn/generic/powm_sec.c
@@ -1,11 +1,4 @@
-/* mpn_powm_sec -- Compute R = U^E mod M.  Secure variant, side-channel silent
-   under the assumption that the multiply instruction is side channel silent.
-
-   Contributed to the GNU project by Torbjorn Granlund.
-
-   THE FUNCTIONS IN THIS FILE ARE INTERNAL WITH MUTABLE INTERFACES.  IT IS ONLY
-   SAFE TO REACH THEM THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
-   GUARANTEED THAT THEY WILL CHANGE OR DISAPPEAR IN A FUTURE GNU MP RELEASE.
+/* mpn_powm_sec -- Compute R = U^E mod M.  Safe variant, not leaking time info.
 
 Copyright 2007, 2008, 2009 Free Software Foundation, Inc.
 
@@ -26,14 +19,19 @@
 
 
 /*
-  BASIC ALGORITHM, Compute U^E mod M, where M < B^n is odd.
+  BASIC ALGORITHM, Compute b^e mod n, where n is odd.
+
+  1. w <- b
+
+  2. While w^2 < n (and there are more bits in e)
+       w <- power left-to-right base-2 without reduction
 
-  1. T <- (B^n * U) mod M                Convert to REDC form
+  3. t <- (B^n * b) / n                Convert to REDC form
 
-  2. Compute table U^0, U^1, U^2... of E-dependent size
+  4. Compute power table of e-dependent size
 
-  3. While there are more bits in E
-       W <- power left-to-right base-k
+  5. While there are more bits in e
+       w <- power left-to-right base-k with reduction
 
 
   TODO:
@@ -46,7 +44,15 @@
 
    * Choose window size without looping.  (Superoptimize or think(tm).)
 
+   * Make it sub-quadratic.
+
    * Call new division functions, not mpn_tdiv_qr.
+
+   * Is redc obsolete with improved SB division?
+
+   * Consider special code for one-limb M.
+
+   * Handle even M (in mpz_powm_sec) with two modexps and CRT.
 */
 
 #include "gmp.h"
@@ -56,111 +62,11 @@
 #define WANT_CACHE_SECURITY 1
 
 
-/* Define our own mpn squaring function.  We do this since we cannot use a
-   native mpn_sqr_basecase over TUNE_SQR_TOOM2_MAX, or a non-native one over
-   SQR_TOOM2_THRESHOLD.  This is so because of fixed size stack allocations
-   made inside mpn_sqr_basecase.  */
-
-#if HAVE_NATIVE_mpn_sqr_diagonal
-#define MPN_SQR_DIAGONAL(rp, up, n)					\
-  mpn_sqr_diagonal (rp, up, n)
-#else
-#define MPN_SQR_DIAGONAL(rp, up, n)					\
-  do {									\
-    mp_size_t _i;							\
-    for (_i = 0; _i < (n); _i++)					\
-      {									\
-	mp_limb_t ul, lpl;						\
-	ul = (up)[_i];							\
-	umul_ppmm ((rp)[2 * _i + 1], lpl, ul, ul << GMP_NAIL_BITS);	\
-	(rp)[2 * _i] = lpl >> GMP_NAIL_BITS;				\
-      }									\
-  } while (0)
-#endif
-
-
-#if ! HAVE_NATIVE_mpn_sqr_basecase
-/* The limit of the generic code is SQR_TOOM2_THRESHOLD.  */
-#define SQR_BASECASE_MAX  SQR_TOOM2_THRESHOLD
-#endif
-
-#if HAVE_NATIVE_mpn_sqr_basecase
-#ifdef TUNE_SQR_TOOM2_MAX
-/* We slightly abuse TUNE_SQR_TOOM2_MAX here.  If it is set for an assembly
-   mpn_sqr_basecase, it comes from SQR_TOOM2_THRESHOLD_MAX in the assembly
-   file.  Assembly mpn_sqr_basecase that do not define it, should allow any
-   size.  */
-#define SQR_BASECASE_MAX  TUNE_SQR_TOOM2_MAX
-#endif
-#endif
-
-#ifndef SQR_BASECASE_MAX
-/* If SQR_BASECASE_MAX is now not defined, use mpn_sqr_basecase for any operand
-   size.  */
-#define mpn_local_sqr_n mpn_sqr_basecase
-#else
-/* Define our own squaring function, which uses mpn_sqr_basecase for its
-   allowed sizes, but its own code for larger sizes.  */
-static void
-mpn_local_sqr_n (mp_ptr rp, mp_srcptr up, mp_size_t n)
-{
-  mp_size_t i;
-
-  ASSERT (n >= 1);
-  ASSERT (! MPN_OVERLAP_P (rp, 2*n, up, n));
-
-  if (n < SQR_BASECASE_MAX)
-    {
-      mpn_sqr_basecase (rp, up, n);
-      return;
-    }
-
-  {
-    mp_limb_t ul, lpl;
-    ul = up[0];
-    umul_ppmm (rp[1], lpl, ul, ul << GMP_NAIL_BITS);
-    rp[0] = lpl >> GMP_NAIL_BITS;
-  }
-  if (n > 1)
-    {
-      mp_ptr tp;
-      mp_limb_t cy;
-      TMP_DECL;
-      TMP_MARK;
-
-      tp = TMP_ALLOC_LIMBS (2 * n);
-
-      cy = mpn_mul_1 (tp, up + 1, n - 1, up[0]);
-      tp[n - 1] = cy;
-      for (i = 2; i < n; i++)
-	{
-	  mp_limb_t cy;
-	  cy = mpn_addmul_1 (tp + 2 * i - 2, up + i, n - i, up[i - 1]);
-	  tp[n + i - 2] = cy;
-	}
-      MPN_SQR_DIAGONAL (rp + 2, up + 1, n - 1);
-
-      {
-	mp_limb_t cy;
-#if HAVE_NATIVE_mpn_addlsh1_n
-	cy = mpn_addlsh1_n (rp + 1, rp + 1, tp, 2 * n - 2);
-#else
-	cy = mpn_lshift (tp, tp, 2 * n - 2, 1);
-	cy += mpn_add_n (rp + 1, rp + 1, tp, 2 * n - 2);
-#endif
-	rp[2 * n - 1] += cy;
-      }
-
-      TMP_FREE;
-    }
-}
-#endif
-
 #define getbit(p,bi) \
   ((p[(bi - 1) / GMP_LIMB_BITS] >> (bi - 1) % GMP_LIMB_BITS) & 1)
 
 static inline mp_limb_t
-getbits (const mp_limb_t *p, mp_bitcnt_t bi, int nbits)
+getbits (const mp_limb_t *p, unsigned long bi, int nbits)
 {
   int nbits_in_r;
   mp_limb_t r;
@@ -183,17 +89,39 @@
     }
 }
 
+#undef HAVE_NATIVE_mpn_addmul_2
+
+#ifndef HAVE_NATIVE_mpn_addmul_2
+#define REDC_2_THRESHOLD		MP_SIZE_T_MAX
+#endif
+
+#ifndef REDC_2_THRESHOLD
+#define REDC_2_THRESHOLD		4
+#endif
+
+static void mpn_redc_n () {ASSERT_ALWAYS(0);}
+
 static inline int
-win_size (mp_bitcnt_t eb)
+win_size (unsigned long eb)
 {
   int k;
-  static mp_bitcnt_t x[] = {1,4,27,100,325,1026,2905,7848,20457,51670,~0ul};
+  static unsigned long x[] = {1,4,27,100,325,1026,2905,7848,20457,51670,~0ul};
   for (k = 0; eb > x[k]; k++)
     ;
   return k;
 }
 
-/* Convert U to REDC form, U_r = B^n * U mod M */
+#define MPN_REDC_X(rp, tp, mp, n, mip)					\
+  do {									\
+    if (redc_x == 1)							\
+      mpn_redc_1 (rp, tp, mp, n, mip[0]);				\
+    else if (redc_x == 2)						\
+      mpn_redc_2 (rp, tp, mp, n, mip);					\
+    else								\
+      mpn_redc_n (rp, tp, mp, n, mip);					\
+  } while (0)
+
+  /* Convert U to REDC form, U_r = B^n * U mod M */
 static void
 redcify (mp_ptr rp, mp_srcptr up, mp_size_t un, mp_srcptr mp, mp_size_t n)
 {
@@ -219,14 +147,14 @@
 	      mp_srcptr ep, mp_size_t en,
 	      mp_srcptr mp, mp_size_t n, mp_ptr tp)
 {
-  mp_limb_t minv;
+  mp_limb_t mip[2];
   int cnt;
-  mp_bitcnt_t ebi;
+  long ebi;
   int windowsize, this_windowsize;
   mp_limb_t expbits;
-  mp_ptr pp, this_pp;
+  mp_ptr pp, this_pp, last_pp;
   long i;
-  int cnd;
+  int redc_x;
   TMP_DECL;
 
   ASSERT (en > 1 || (en == 1 && ep[0] > 1));
@@ -235,12 +163,28 @@
   TMP_MARK;
 
   count_leading_zeros (cnt, ep[en - 1]);
-  ebi = (mp_bitcnt_t) en * GMP_LIMB_BITS - cnt;
+  ebi = en * GMP_LIMB_BITS - cnt;
 
   windowsize = win_size (ebi);
 
-  binvert_limb (minv, mp[0]);
-  minv = -minv;
+  if (BELOW_THRESHOLD (n, REDC_2_THRESHOLD))
+    {
+      binvert_limb (mip[0], mp[0]);
+      mip[0] = -mip[0];
+      redc_x = 1;
+    }
+#if defined (HAVE_NATIVE_mpn_addmul_2)
+  else
+    {
+      mpn_binvert (mip, mp, 2, tp);
+      mip[0] = -mip[0]; mip[1] = ~mip[1];
+      redc_x = 2;
+    }
+#endif
+#if 0
+  mpn_binvert (mip, mp, n, tp);
+  redc_x = 0;
+#endif
 
   pp = TMP_ALLOC_LIMBS (n << windowsize);
 
@@ -253,53 +197,52 @@
   /* Precompute powers of b and put them in the temporary area at pp.  */
   for (i = (1 << windowsize) - 2; i > 0; i--)
     {
-      mpn_mul_basecase (tp, this_pp, n, pp + n, n);
+      last_pp = this_pp;
       this_pp += n;
-      mpn_redc_1_sec (this_pp, tp, mp, n, minv);
+      mpn_mul_n (tp, last_pp, pp + n, n);
+      MPN_REDC_X (this_pp, tp, mp, n, mip);
     }
 
   expbits = getbits (ep, ebi, windowsize);
-  if (ebi < windowsize)
+  ebi -= windowsize;
+  if (ebi < 0)
     ebi = 0;
-  else
-    ebi -= windowsize;
 
   MPN_COPY (rp, pp + n * expbits, n);
 
   while (ebi != 0)
     {
       expbits = getbits (ep, ebi, windowsize);
+      ebi -= windowsize;
       this_windowsize = windowsize;
-      if (ebi < windowsize)
+      if (ebi < 0)
 	{
-	  this_windowsize -= windowsize - ebi;
+	  this_windowsize += ebi;
 	  ebi = 0;
 	}
-      else
-	ebi -= windowsize;
 
       do
 	{
-	  mpn_local_sqr_n (tp, rp, n);
-	  mpn_redc_1_sec (rp, tp, mp, n, minv);
+	  mpn_sqr_n (tp, rp, n);
+	  MPN_REDC_X (rp, tp, mp, n, mip);
 	  this_windowsize--;
 	}
       while (this_windowsize != 0);
 
 #if WANT_CACHE_SECURITY
       mpn_tabselect (tp + 2*n, pp, n, 1 << windowsize, expbits);
-      mpn_mul_basecase (tp, rp, n, tp + 2*n, n);
+      mpn_mul_n (tp, rp, tp + 2*n, n);
 #else
-      mpn_mul_basecase (tp, rp, n, pp + n * expbits, n);
+      mpn_mul_n (tp, rp, pp + n * expbits, n);
 #endif
-      mpn_redc_1_sec (rp, tp, mp, n, minv);
+      MPN_REDC_X (rp, tp, mp, n, mip);
     }
 
   MPN_COPY (tp, rp, n);
   MPN_ZERO (tp + n, n);
-  mpn_redc_1_sec (rp, tp, mp, n, minv);
-  cnd = mpn_sub_n (tp, rp, mp, n);	/* we need just retval */
-  mpn_subcnd_n (rp, rp, mp, n, !cnd);
+  MPN_REDC_X (rp, tp, mp, n, mip);
+  if (mpn_cmp (rp, mp, n) >= 0)
+    mpn_sub_n (rp, rp, mp, n);
   TMP_FREE;
 }
 
--- 1/mpn/generic/pre_divrem_1.c
+++ 2/mpn/generic/pre_divrem_1.c
@@ -32,8 +32,8 @@
 /* Same test here for skipping one divide step as in mpn_divrem_1.
 
    The main reason for a separate shift==0 case is that not all CPUs give
-   zero for "n0 >> GMP_LIMB_BITS" which would arise in the general case
-   code used on shift==0.  shift==0 is also reasonably common in mp_bases
+   zero for "n0 >> BITS_PER_MP_LIMB" which would arise in the general case
+   code used on shift==0.  shift==0 is also reasonably common in __mp_bases
    big_base, for instance base==10 on a 64-bit limb.
 
    Under shift!=0 it would be possible to call mpn_lshift to adjust the
@@ -106,14 +106,14 @@
 	}
 
       n1 = ap[size-1];
-      r |= n1 >> (GMP_LIMB_BITS - shift);
+      r |= n1 >> (BITS_PER_MP_LIMB - shift);
 
       for (i = size-2; i >= 0; i--)
 	{
 	  ASSERT (r < d);
 	  n0 = ap[i];
 	  udiv_qrnnd_preinv (*qp, r, r,
-			     ((n1 << shift) | (n0 >> (GMP_LIMB_BITS - shift))),
+			     ((n1 << shift) | (n0 >> (BITS_PER_MP_LIMB - shift))),
 			     d, dinv);
 	  qp--;
 	  n1 = n0;
--- 1/mpn/generic/redc_1.c
+++ 2/mpn/generic/redc_1.c
@@ -4,7 +4,7 @@
    THIS IS AN INTERNAL FUNCTION WITH A MUTABLE INTERFACE.  IT IS ONLY
    SAFE TO REACH THIS FUNCTION THROUGH DOCUMENTED INTERFACES.
 
-Copyright (C) 2000, 2001, 2002, 2004, 2008, 2009 Free Software Foundation, Inc.
+Copyright (C) 2000, 2001, 2002, 2004, 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -30,14 +30,11 @@
   mp_size_t j;
   mp_limb_t cy;
 
-  ASSERT (n > 0);
   ASSERT_MPN (up, 2*n);
 
   for (j = n - 1; j >= 0; j--)
     {
-      cy = mpn_addmul_1 (up, mp, n, (up[0] * invm) & GMP_NUMB_MASK);
-      ASSERT (up[0] == 0);
-      up[0] = cy;
+      up[0] = mpn_addmul_1 (up, mp, n, (up[0] * invm) & GMP_NUMB_MASK);
       up++;
     }
   cy = mpn_add_n (rp, up, up - n, n);
--- 1/mpn/generic/redc_2.c
+++ 2/mpn/generic/redc_2.c
@@ -74,7 +74,6 @@
   mp_limb_t upn;
   mp_limb_t cy;
 
-  ASSERT (n > 0);
   ASSERT_MPN (up, 2*n);
 
   if ((n & 1) != 0)
--- 1/mpn/generic/set_str.c
+++ 2/mpn/generic/set_str.c
@@ -69,7 +69,7 @@
       int next_bitpos;
       mp_limb_t res_digit;
       mp_size_t size;
-      int bits_per_indigit = mp_bases[base].big_base;
+      int bits_per_indigit = __mp_bases[base].big_base;
 
       size = 0;
       res_digit = 0;
@@ -107,7 +107,7 @@
 
       TMP_MARK;
 
-      chars_per_limb = mp_bases[base].chars_per_limb;
+      chars_per_limb = __mp_bases[base].chars_per_limb;
 
       un = str_len / chars_per_limb + 1;
 
@@ -139,9 +139,9 @@
 
   powtab_mem_ptr = powtab_mem;
 
-  chars_per_limb = mp_bases[base].chars_per_limb;
-  big_base = mp_bases[base].big_base;
-  big_base_inverted = mp_bases[base].big_base_inverted;
+  chars_per_limb = __mp_bases[base].chars_per_limb;
+  big_base = __mp_bases[base].big_base;
+  big_base_inverted = __mp_bases[base].big_base_inverted;
   count_leading_zeros (normalization_steps, big_base);
 
   p = powtab_mem_ptr;
@@ -278,11 +278,11 @@
   mp_limb_t res_digit;
 
   ASSERT (base >= 2);
-  ASSERT (base < numberof (mp_bases));
+  ASSERT (base < numberof (__mp_bases));
   ASSERT (str_len >= 1);
 
-  big_base = mp_bases[base].big_base;
-  chars_per_limb = mp_bases[base].chars_per_limb;
+  big_base = __mp_bases[base].big_base;
+  chars_per_limb = __mp_bases[base].chars_per_limb;
 
   size = 0;
   for (i = chars_per_limb; i < str_len; i += chars_per_limb)
--- 1/mpn/generic/sizeinbase.c
+++ 2/mpn/generic/sizeinbase.c
@@ -37,7 +37,7 @@
 
   ASSERT (xsize >= 0);
   ASSERT (base >= 2);
-  ASSERT (base < numberof (mp_bases));
+  ASSERT (base < numberof (__mp_bases));
 
   /* Special case for X == 0.  */
   if (xsize == 0)
@@ -45,14 +45,14 @@
 
   /* Calculate the total number of significant bits of X.  */
   count_leading_zeros (cnt, xp[xsize-1]);
-  totbits = xsize * GMP_LIMB_BITS - cnt;
+  totbits = xsize * BITS_PER_MP_LIMB - cnt;
 
   if (POW2_P (base))
     {
       /* Special case for powers of 2, giving exact result.  */
-      lb_base = mp_bases[base].big_base;
+      lb_base = __mp_bases[base].big_base;
       return (totbits + lb_base - 1) / lb_base;
     }
   else
-    return (size_t) (totbits * mp_bases[base].chars_per_bit_exactly) + 1;
+    return (size_t) (totbits * __mp_bases[base].chars_per_bit_exactly) + 1;
 }
--- 1/mpn/generic/sqr_basecase.c
+++ 2/mpn/generic/sqr_basecase.c
@@ -54,12 +54,12 @@
 mpn_sqr_basecase (mp_ptr rp, mp_srcptr up, mp_size_t n)
 {
   mp_size_t i;
-  mp_limb_t tarr[2 * SQR_TOOM2_THRESHOLD];
+  mp_limb_t tarr[2 * SQR_KARATSUBA_THRESHOLD];
   mp_ptr tp = tarr;
   mp_limb_t cy;
 
   /* must fit 2*n limbs in tarr */
-  ASSERT (n <= SQR_TOOM2_THRESHOLD);
+  ASSERT (n <= SQR_KARATSUBA_THRESHOLD);
 
   if ((n & 1) != 0)
     {
@@ -136,12 +136,12 @@
 mpn_sqr_basecase (mp_ptr rp, mp_srcptr up, mp_size_t n)
 {
   mp_size_t i;
-  mp_limb_t tarr[2 * SQR_TOOM2_THRESHOLD];
+  mp_limb_t tarr[2 * SQR_KARATSUBA_THRESHOLD];
   mp_ptr tp = tarr;
   mp_limb_t cy;
 
   /* must fit 2*n limbs in tarr */
-  ASSERT (n <= SQR_TOOM2_THRESHOLD);
+  ASSERT (n <= SQR_KARATSUBA_THRESHOLD);
 
   if ((n & 1) != 0)
     {
@@ -268,12 +268,12 @@
   }
   if (n > 1)
     {
-      mp_limb_t tarr[2 * SQR_TOOM2_THRESHOLD];
+      mp_limb_t tarr[2 * SQR_KARATSUBA_THRESHOLD];
       mp_ptr tp = tarr;
       mp_limb_t cy;
 
       /* must fit 2*n limbs in tarr */
-      ASSERT (n <= SQR_TOOM2_THRESHOLD);
+      ASSERT (n <= SQR_KARATSUBA_THRESHOLD);
 
       cy = mpn_mul_1 (tp, up + 1, n - 1, up[0]);
       tp[n - 1] = cy;
--- 1/mpn/generic/sub_n.c
+++ 2/mpn/generic/sub_n.c
@@ -1,7 +1,6 @@
 /* mpn_sub_n -- Subtract equal length limb vectors.
 
-Copyright 1992, 1993, 1994, 1996, 2000, 2002, 2009 Free Software Foundation,
-Inc.
+Copyright 1992, 1993, 1994, 1996, 2000, 2002 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -30,8 +29,8 @@
   mp_limb_t ul, vl, sl, rl, cy, cy1, cy2;
 
   ASSERT (n >= 1);
-  ASSERT (MPN_SAME_OR_INCR_P (rp, up, n));
-  ASSERT (MPN_SAME_OR_INCR_P (rp, vp, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, up, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, vp, n));
 
   cy = 0;
   do
@@ -60,8 +59,8 @@
   mp_limb_t ul, vl, rl, cy;
 
   ASSERT (n >= 1);
-  ASSERT (MPN_SAME_OR_INCR_P (rp, up, n));
-  ASSERT (MPN_SAME_OR_INCR_P (rp, vp, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, up, n));
+  ASSERT (MPN_SAME_OR_SEPARATE_P (rp, vp, n));
 
   cy = 0;
   do
--- 1/mpn/generic/tdiv_qr.c
+++ 2/mpn/generic/tdiv_qr.c
@@ -1,7 +1,7 @@
 /* mpn_tdiv_qr -- Divide the numerator (np,nn) by the denominator (dp,dn) and
    write the nn-dn+1 quotient limbs at qp and the dn remainder limbs at rp.  If
    qxn is non-zero, generate that many fraction limbs and append them after the
-   other quotient limbs, and update the remainder accordingly.  The input
+   other quotient limbs, and update the remainder accordningly.  The input
    operands are unaffected.
 
    Preconditions:
@@ -58,7 +58,7 @@
 
     case 1:
       {
-	rp[0] = mpn_divrem_1 (qp, (mp_size_t) 0, np, nn, dp[0]);
+	rp[0] = mpn_divmod_1 (qp, np, nn, dp[0]);
 	return;
       }
 
@@ -77,7 +77,7 @@
 	    d2p = dtmp;
 	    d2p[1] = (dp[1] << cnt) | (dp[0] >> (GMP_NUMB_BITS - cnt));
 	    d2p[0] = (dp[0] << cnt) & GMP_NUMB_MASK;
-	    n2p = TMP_ALLOC_LIMBS (nn + 1);
+	    n2p = (mp_ptr) TMP_ALLOC ((nn + 1) * BYTES_PER_MP_LIMB);
 	    cy = mpn_lshift (n2p, np, nn, cnt);
 	    n2p[nn] = cy;
 	    qhl = mpn_divrem_2 (qp, 0L, n2p, nn + (cy != 0), d2p);
@@ -90,7 +90,7 @@
 	else
 	  {
 	    d2p = (mp_ptr) dp;
-	    n2p = TMP_ALLOC_LIMBS (nn);
+	    n2p = (mp_ptr) TMP_ALLOC (nn * BYTES_PER_MP_LIMB);
 	    MPN_COPY (n2p, np, nn);
 	    qhl = mpn_divrem_2 (qp, 0L, n2p, nn, d2p);
 	    qp[nn - 2] = qhl;	/* always store nn-2+1 quotient limbs */
@@ -104,13 +104,12 @@
     default:
       {
 	int adjust;
-	gmp_pi1_t dinv;
 	TMP_DECL;
 	TMP_MARK;
 	adjust = np[nn - 1] >= dp[dn - 1];	/* conservative tests for quotient size */
 	if (nn + adjust >= 2 * dn)
 	  {
-	    mp_ptr n2p, d2p;
+	    mp_ptr n2p, d2p, q2p;
 	    mp_limb_t cy;
 	    int cnt;
 
@@ -119,9 +118,9 @@
 	      {
 		count_leading_zeros (cnt, dp[dn - 1]);
 		cnt -= GMP_NAIL_BITS;
-		d2p = TMP_ALLOC_LIMBS (dn);
+		d2p = (mp_ptr) TMP_ALLOC (dn * BYTES_PER_MP_LIMB);
 		mpn_lshift (d2p, dp, dn, cnt);
-		n2p = TMP_ALLOC_LIMBS (nn + 1);
+		n2p = (mp_ptr) TMP_ALLOC ((nn + 1) * BYTES_PER_MP_LIMB);
 		cy = mpn_lshift (n2p, np, nn, cnt);
 		n2p[nn] = cy;
 		nn += adjust;
@@ -130,17 +129,50 @@
 	      {
 		cnt = 0;
 		d2p = (mp_ptr) dp;
-		n2p = TMP_ALLOC_LIMBS (nn + 1);
+		n2p = (mp_ptr) TMP_ALLOC ((nn + 1) * BYTES_PER_MP_LIMB);
 		MPN_COPY (n2p, np, nn);
 		n2p[nn] = 0;
 		nn += adjust;
 	      }
 
-	    invert_pi1 (dinv, d2p[dn - 1], d2p[dn - 2]);
-	    if (dn < DC_DIV_QR_THRESHOLD)
-	      mpn_sbpi1_div_qr (qp, n2p, nn, d2p, dn, dinv.inv32);
+	    if (dn < DIV_DC_THRESHOLD)
+	      mpn_sb_divrem_mn (qp, n2p, nn, d2p, dn);
 	    else
-	      mpn_dcpi1_div_qr (qp, n2p, nn, d2p, dn, &dinv);
+	      {
+		/* Divide 2*dn / dn limbs as long as the limbs in np last.  */
+		q2p = qp + nn - dn;
+		n2p += nn - dn;
+		do
+		  {
+		    q2p -= dn;  n2p -= dn;
+		    mpn_dc_divrem_n (q2p, n2p, d2p, dn);
+		    nn -= dn;
+		  }
+		while (nn >= 2 * dn);
+
+		if (nn != dn)
+		  {
+		    mp_limb_t ql;
+		    n2p -= nn - dn;
+
+		    /* We have now dn < nn - dn < 2dn.  Make a recursive call,
+		       since falling out to the code below isn't pretty.
+		       Unfortunately, mpn_tdiv_qr returns nn-dn+1 quotient
+		       limbs, which would overwrite one already generated
+		       quotient limbs.  Preserve it with an ugly hack.  */
+		    /* FIXME: This suggests that we should have an
+		       mpn_tdiv_qr_internal that instead returns the most
+		       significant quotient limb and move the meat of this
+		       function there.  */
+		    /* FIXME: Perhaps call mpn_sb_divrem_mn here for certain
+		       operand ranges, to decrease overhead for small
+		       operands?  */
+		    ql = qp[nn - dn]; /* preserve quotient limb... */
+		    mpn_tdiv_qr (qp, n2p, 0L, n2p, nn, d2p, dn);
+		    qp[nn - dn] = ql; /* ...restore it again */
+		  }
+	      }
+
 
 	    if (cnt != 0)
 	      mpn_rshift (rp, n2p, dn, cnt);
@@ -214,11 +246,11 @@
 		count_leading_zeros (cnt, dp[dn - 1]);
 		cnt -= GMP_NAIL_BITS;
 
-		d2p = TMP_ALLOC_LIMBS (qn);
+		d2p = (mp_ptr) TMP_ALLOC (qn * BYTES_PER_MP_LIMB);
 		mpn_lshift (d2p, dp + in, qn, cnt);
 		d2p[0] |= dp[in - 1] >> (GMP_NUMB_BITS - cnt);
 
-		n2p = TMP_ALLOC_LIMBS (2 * qn + 1);
+		n2p = (mp_ptr) TMP_ALLOC ((2 * qn + 1) * BYTES_PER_MP_LIMB);
 		cy = mpn_lshift (n2p, np + nn - 2 * qn, 2 * qn, cnt);
 		if (adjust)
 		  {
@@ -235,7 +267,7 @@
 		cnt = 0;
 		d2p = (mp_ptr) dp + in;
 
-		n2p = TMP_ALLOC_LIMBS (2 * qn + 1);
+		n2p = (mp_ptr) TMP_ALLOC ((2 * qn + 1) * BYTES_PER_MP_LIMB);
 		MPN_COPY (n2p, np + nn - 2 * qn, 2 * qn);
 		if (adjust)
 		  {
@@ -248,21 +280,25 @@
 	    if (qn == 1)
 	      {
 		mp_limb_t q0, r0;
-		udiv_qrnnd (q0, r0, n2p[1], n2p[0] << GMP_NAIL_BITS, d2p[0] << GMP_NAIL_BITS);
-		n2p[0] = r0 >> GMP_NAIL_BITS;
+		mp_limb_t gcc272bug_n1, gcc272bug_n0, gcc272bug_d0;
+		/* Due to a gcc 2.7.2.3 reload pass bug, we have to use some
+		   temps here.  This doesn't hurt code quality on any machines
+		   so we do it unconditionally.  */
+		gcc272bug_n1 = n2p[1];
+		gcc272bug_n0 = n2p[0];
+		gcc272bug_d0 = d2p[0];
+		udiv_qrnnd (q0, r0, gcc272bug_n1, gcc272bug_n0 << GMP_NAIL_BITS,
+			    gcc272bug_d0 << GMP_NAIL_BITS);
+		r0 >>= GMP_NAIL_BITS;
+		n2p[0] = r0;
 		qp[0] = q0;
 	      }
 	    else if (qn == 2)
-	      mpn_divrem_2 (qp, 0L, n2p, 4L, d2p); /* FIXME: obsolete function */
+	      mpn_divrem_2 (qp, 0L, n2p, 4L, d2p);
+	    else if (qn < DIV_DC_THRESHOLD)
+	      mpn_sb_divrem_mn (qp, n2p, 2 * qn, d2p, qn);
 	    else
-	      {
-		gmp_pi1_t dinv;
-		invert_pi1 (dinv, d2p[qn - 1], d2p[qn - 2]);
-		if (qn < DC_DIV_QR_THRESHOLD)
-		  mpn_sbpi1_div_qr (qp, n2p, 2 * qn, d2p, qn, dinv.inv32);
-		else
-		  mpn_dcpi1_div_qr (qp, n2p, 2 * qn, d2p, qn, &dinv);
-	      }
+	      mpn_dc_divrem_n (qp, n2p, d2p, qn);
 
 	    rn = qn;
 	    /* Multiply the first ignored divisor limb by the most significant
@@ -280,7 +316,7 @@
 		dl = dp[in - 2];
 
 #if GMP_NAIL_BITS == 0
-	      x = (dp[in - 1] << cnt) | ((dl >> 1) >> ((~cnt) % GMP_LIMB_BITS));
+	      x = (dp[in - 1] << cnt) | ((dl >> 1) >> ((~cnt) % BITS_PER_MP_LIMB));
 #else
 	      x = (dp[in - 1] << cnt) & GMP_NUMB_MASK;
 	      if (cnt != 0)
@@ -330,7 +366,7 @@
 	      }
 	    /* True: partial remainder now is neutral, i.e., it is not shifted up.  */
 
-	    tp = TMP_ALLOC_LIMBS (dn);
+	    tp = (mp_ptr) TMP_ALLOC (dn * BYTES_PER_MP_LIMB);
 
 	    if (in < qn)
 	      {
--- 1/mpn/generic/toom22_mul.c
+++ 2/mpn/generic/toom22_mul.c
@@ -51,21 +51,12 @@
 #define TOOM22_MUL_N_REC(p, a, b, n, ws)				\
   do {									\
     if (! MAYBE_mul_toom22						\
-	|| BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))			\
+	|| BELOW_THRESHOLD (n, MUL_KARATSUBA_THRESHOLD))		\
       mpn_mul_basecase (p, a, n, b, n);					\
     else								\
       mpn_toom22_mul (p, a, n, b, n, ws);				\
   } while (0)
 
-#define TOOM22_MUL_MN_REC(p, a, an, b, bn, ws)				\
-  do {									\
-    if (! MAYBE_mul_toom22						\
-	|| BELOW_THRESHOLD (bn, MUL_TOOM22_THRESHOLD))			\
-      mpn_mul_basecase (p, a, an, b, bn);				\
-    else								\
-      mpn_toom22_mul (p, a, an, b, bn, ws);				\
-  } while (0)
-
 void
 mpn_toom22_mul (mp_ptr pp,
 		mp_srcptr ap, mp_size_t an,
@@ -159,8 +150,8 @@
   /* vm1, 2n limbs */
   TOOM22_MUL_N_REC (vm1, asm1, bsm1, n, scratch_out);
 
-  if (s > t)  TOOM22_MUL_MN_REC (vinf, a1, s, b1, t, scratch_out);
-  else        TOOM22_MUL_N_REC (vinf, a1, b1, s, scratch_out);
+  /* vinf, s+t limbs */
+  mpn_mul (vinf, a1, s, b1, t);
 
   /* v0, 2n limbs */
   TOOM22_MUL_N_REC (v0, ap, bp, n, scratch_out);
--- 1/mpn/generic/toom2_sqr.c
+++ 2/mpn/generic/toom2_sqr.c
@@ -32,10 +32,12 @@
   <-s--><--n-->
    ____ ______
   |_a1_|___a0_|
+   |b1_|___b0_|
+   <-t-><--n-->
 
-  v0  =  a0     ^2  #   A(0)^2
-  vm1 = (a0- a1)^2  #  A(-1)^2
-  vinf=      a1 ^2  # A(inf)^2
+  v0  =  a0     * b0       #   A(0)*B(0)
+  vm1 = (a0- a1)*(b0- b1)  #  A(-1)*B(-1)
+  vinf=      a1 *     b1   # A(inf)*B(inf)
 */
 
 #if TUNE_PROGRAM_BUILD
@@ -48,7 +50,7 @@
 #define TOOM2_SQR_N_REC(p, a, n, ws)					\
   do {									\
     if (! MAYBE_sqr_toom2						\
-	|| BELOW_THRESHOLD (n, SQR_TOOM2_THRESHOLD))			\
+	|| BELOW_THRESHOLD (n, SQR_KARATSUBA_THRESHOLD))		\
       mpn_sqr_basecase (p, a, n);					\
     else								\
       mpn_toom2_sqr (p, a, n, ws);					\
@@ -101,16 +103,15 @@
 #define v0	pp				/* 2n */
 #define vinf	(pp + 2 * n)			/* s+s */
 #define vm1	scratch				/* 2n */
-#define scratch_out	scratch + 2 * n
 
   /* vm1, 2n limbs */
-  TOOM2_SQR_N_REC (vm1, asm1, n, scratch_out);
+  TOOM2_SQR_N_REC (vm1, asm1, n, scratch);
 
   /* vinf, s+s limbs */
-  TOOM2_SQR_N_REC (vinf, a1, s, scratch_out);
+  TOOM2_SQR_N_REC (vinf, a1, s, scratch);
 
   /* v0, 2n limbs */
-  TOOM2_SQR_N_REC (v0, ap, n, scratch_out);
+  TOOM2_SQR_N_REC (v0, ap, n, scratch);
 
   /* H(v0) + L(vinf) */
   cy = mpn_add_n (pp + 2 * n, v0 + n, vinf, n);
--- 1/mpn/generic/toom32_mul.c
+++ 2/mpn/generic/toom32_mul.c
@@ -2,7 +2,6 @@
    times as large as bn.  Or more accurately, bn < an < 3bn.
 
    Contributed to the GNU project by Torbjorn Granlund.
-   Improvements by Marco Bodrato.
 
    The idea of applying toom to unbalanced multiplication is due to Marco
    Bodrato and Alberto Zanoni.
@@ -11,7 +10,7 @@
    SAFE TO REACH IT THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
    GUARANTEED THAT IT WILL CHANGE OR DISAPPEAR IN A FUTURE GNU MP RELEASE.
 
-Copyright 2006, 2007, 2008, 2009 Free Software Foundation, Inc.
+Copyright 2006, 2007, 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -29,6 +28,15 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1, asm1, bs1, and bsm1 could be
+     avoided by instead reusing the pp area and the scratch allocation.
+
+  2. Apply optimizations also to mul_toom42.c.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
@@ -46,9 +54,20 @@
   vinf=          a2 *     b1  # A(inf)*B(inf)
 */
 
-#define TOOM32_MUL_N_REC(p, a, b, n, ws)				\
+#if TUNE_PROGRAM_BUILD
+#define MAYBE_mul_toom22   1
+#else
+#define MAYBE_mul_toom22						\
+  (MUL_TOOM33_THRESHOLD >= 2 * MUL_TOOM22_THRESHOLD)
+#endif
+
+#define TOOM22_MUL_N_REC(p, a, b, n, ws)				\
   do {									\
-    mpn_mul_n (p, a, b, n);						\
+    if (! MAYBE_mul_toom22						\
+	|| BELOW_THRESHOLD (n, MUL_KARATSUBA_THRESHOLD))		\
+      mpn_mul_basecase (p, a, n, b, n);					\
+    else								\
+      mpn_toom22_mul (p, a, n, b, n, ws);				\
   } while (0)
 
 void
@@ -59,7 +78,15 @@
 {
   mp_size_t n, s, t;
   int vm1_neg;
+#if HAVE_NATIVE_mpn_add_nc
   mp_limb_t cy;
+#else
+  mp_limb_t cy, cy2;
+#endif
+  mp_ptr a0_a2;
+  mp_ptr as1, asm1;
+  mp_ptr bs1, bsm1;
+  TMP_DECL;
 
 #define a0  ap
 #define a1  (ap + n)
@@ -75,39 +102,45 @@
   ASSERT (0 < s && s <= n);
   ASSERT (0 < t && t <= n);
 
-#define as1   (pp + n + 1)			/* n+1 */
-#define asm1  (scratch + n)			/* n+1 */
-#define bs1   (pp)				/* n+1 */
-#define bsm1  (scratch)			/* n */
-#define a0_a2 (scratch)				/* n */
+  TMP_MARK;
+
+  as1 = TMP_SALLOC_LIMBS (n + 1);
+  asm1 = TMP_SALLOC_LIMBS (n + 1);
+
+  bs1 = TMP_SALLOC_LIMBS (n + 1);
+  bsm1 = TMP_SALLOC_LIMBS (n);
+
+  a0_a2 = pp;
 
   /* Compute as1 and asm1.  */
-  asm1[n] = mpn_add (a0_a2, a0, n, a2, s);
-#if HAVE_NATIVE_mpn_add_n_sub_n
-  if (asm1[n] == 0 && mpn_cmp (a0_a2, a1, n) < 0)
+  a0_a2[n] = mpn_add (a0_a2, a0, n, a2, s);
+#if HAVE_NATIVE_mpn_addsub_n
+  if (a0_a2[n] == 0 && mpn_cmp (a0_a2, a1, n) < 0)
     {
-      cy = mpn_add_n_sub_n (as1, asm1, a1, a0_a2, n);
+      cy = mpn_addsub_n (as1, asm1, a1, a0_a2, n);
       as1[n] = cy >> 1;
+      asm1[n] = 0;
       vm1_neg = 1;
     }
   else
     {
-      cy = mpn_add_n_sub_n (as1, asm1, a0_a2, a1, n);
-      as1[n] = asm1[n] + (cy >> 1);
-      asm1[n]-= cy & 1;
+      cy = mpn_addsub_n (as1, asm1, a0_a2, a1, n);
+      as1[n] = a0_a2[n] + (cy >> 1);
+      asm1[n] = a0_a2[n] - (cy & 1);
       vm1_neg = 0;
     }
 #else
-  as1[n] = asm1[n] + mpn_add_n (as1, a0_a2, a1, n);
-  if (asm1[n] == 0 && mpn_cmp (a0_a2, a1, n) < 0)
+  as1[n] = a0_a2[n] + mpn_add_n (as1, a0_a2, a1, n);
+  if (a0_a2[n] == 0 && mpn_cmp (a0_a2, a1, n) < 0)
     {
       mpn_sub_n (asm1, a1, a0_a2, n);
+      asm1[n] = 0;
       vm1_neg = 1;
     }
   else
     {
       cy = mpn_sub_n (asm1, a0_a2, a1, n);
-      asm1[n]-= cy;
+      asm1[n] = a0_a2[n] - cy;
       vm1_neg = 0;
     }
 #endif
@@ -115,15 +148,15 @@
   /* Compute bs1 and bsm1.  */
   if (t == n)
     {
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
       if (mpn_cmp (b0, b1, n) < 0)
 	{
-	  cy = mpn_add_n_sub_n (bs1, bsm1, b1, b0, n);
+	  cy = mpn_addsub_n (bs1, bsm1, b1, b0, n);
 	  vm1_neg ^= 1;
 	}
       else
 	{
-	  cy = mpn_add_n_sub_n (bs1, bsm1, b0, b1, n);
+	  cy = mpn_addsub_n (bs1, bsm1, b0, b1, n);
 	}
       bs1[n] = cy >> 1;
 #else
@@ -168,14 +201,18 @@
 #define scratch_out	scratch + 4 * n + 2
 
   /* vm1, 2n+1 limbs */
-  TOOM32_MUL_N_REC (vm1, asm1, bsm1, n, scratch_out);
+  TOOM22_MUL_N_REC (vm1, asm1, bsm1, n, scratch_out);
   cy = 0;
   if (asm1[n] != 0)
     cy = mpn_add_n (vm1 + n, vm1 + n, bsm1, n);
   vm1[2 * n] = cy;
 
+  /* vinf, s+t limbs */
+  if (s > t)  mpn_mul (vinf, a2, s, b1, t);
+  else        mpn_mul (vinf, b1, t, a2, s);
+
   /* v1, 2n+1 limbs */
-  TOOM32_MUL_N_REC (v1, as1, bs1, n, scratch_out);
+  TOOM22_MUL_N_REC (v1, as1, bs1, n, scratch_out);
   if (as1[n] == 1)
     {
       cy = bs1[n] + mpn_add_n (v1 + n, v1 + n, bs1, n);
@@ -194,12 +231,7 @@
     cy += mpn_add_n (v1 + n, v1 + n, as1, n);
   v1[2 * n] = cy;
 
-  /* vinf, s+t limbs.  Use mpn_mul for now, to handle unbalanced operands */
-  if (s > t)  mpn_mul (vinf, a2, s, b1, t);
-  else        mpn_mul (vinf, b1, t, a2, s);
-
-  /* v0, 2n limbs */
-  TOOM32_MUL_N_REC (v0, ap, bp, n, scratch_out);
+  mpn_mul_n (v0, ap, bp, n);                    /* v0, 2n limbs */
 
   /* Interpolate */
 
@@ -222,60 +254,38 @@
 #endif
     }
 
-  t += s;   /* limbs in vinf */
-  if (LIKELY(t>=n)) /* We should consider this branch only, even better t>n */
-    s = n;  /* limbs in Lvinf */
-  else {
-    s = t;  /* limbs in Lvinf */
-    v1[2 * n]=0;
-  }
-
-  mpn_sub_n (v1, v1, vm1, s + n + 1);
+  mpn_sub_n (v1, v1, vm1, 2 * n + 1);
+  v1[2 * n] -= mpn_sub_n (v1, v1, v0, 2 * n);
 
   /*
     pp[] prior to operations:
-     |_H vinf|_L vinf|_______|_H v0__|_L v0__|
+     |_H vinf|_L vinf|_______|_______|_______|
 
     summation scheme for remaining operations:
-     |______4|n_____3|n_____2|n______|n______|pp
-     |_Hvinf_|_L*vinf|       |_H*v0__|_L v0__|
-		    || H vm1 | L vm1 |
-		     |-H vinf|-L*vinf|
-	    || H v1  | L v1  |
-	     |-H*v0  |-L v0  |
-
-    We will avoid double computation of Hv0-Lvinf. Be careful! This
-    operation can give a negative result!
+     |_______|_______|_______|_______|_______|
+     |_Hvinf_|_Lvinf_|       |_H v0__|_L v0__|
+		     | H vm1 | L vm1 |
+		     |-H vinf|-L vinf|
+	     | H v1  | L v1  |
   */
 
-  cy = mpn_sub_n (v0 + n, v0 + n, vinf, s); /* Hv0-Lvinf*/
-  v1[ n + s ] += cy - mpn_sub_n(pp + 2 * n, v1, v0, n + s); /* v1-v0+Lvinf */
-  cy -= mpn_add_n (pp + n, pp + n, vm1, s); /* (Hv0-Lvinf)+Lvm1 */
-  if (cy != 0) { /* carry and borrow did not cancel one another, apply the right one */
-    if (cy == 1)
-      MPN_DECR_U (vm1 + s, 2 * n + 1 - s, 1);
-    else /* (cy == -1) */
-      MPN_INCR_U (vm1 + s, 2 * n + 1 - s, 1);
-  }
-  if (LIKELY(t>n)) { /* It works also without this "if", t<=n implies t-s==0,v1[2*n]==0 */
-    mpn_sub (vm1 + n, vm1 + n, n + 1, vinf + n, t - s); /* Hvm1-Hvinf */
-    MPN_INCR_U (vinf + s, t - s, v1[2 * n]);
-  }
-  cy = mpn_add_n (pp + n + s, pp + n + s, vm1 + s, 2 * n + 1 - s);
-  MPN_INCR_U (vinf + 1, t - 1, cy);
-
-#undef a0
-#undef a1
-#undef a2
-#undef b0
-#undef b1
-#undef v0
-#undef v1
-#undef vinf
-#undef vm1
-#undef a0_a2
-#undef bsm1
-#undef asm1
-#undef bs1
-#undef as1
+  mpn_sub (vm1, vm1, 2 * n + 1, vinf, s + t);
+#if HAVE_NATIVE_mpn_add_nc
+  cy = mpn_add_n (pp + n, pp + n, vm1, n);
+  cy = mpn_add_nc (pp + 2 * n, v1, vm1 + n, n, cy);
+  cy = mpn_add_nc (pp + 3 * n, pp + 3 * n, v1 + n, n, cy);
+  mpn_incr_u (pp + 3 * n, vm1[2 * n]);
+  if (LIKELY (n != s + t))  /* FIXME: Limit operand range to avoid condition */
+    mpn_incr_u (pp + 4 * n, cy + v1[2 * n]);
+#else
+  cy2 = mpn_add_n (pp + n, pp + n, vm1, n);
+  cy = mpn_add_n (pp + 2 * n, v1, vm1 + n, n);
+  mpn_incr_u (pp + 2 * n, cy2);
+  mpn_incr_u (pp + 3 * n, cy + vm1[2 * n]);
+  cy = mpn_add_n (pp + 3 * n, pp + 3 * n, v1 + n,  n);
+  if (LIKELY (n != s + t))  /* FIXME: Limit operand range to avoid condition */
+    mpn_incr_u (pp + 4 * n, cy + v1[2 * n]);
+#endif
+
+  TMP_FREE;
 }
--- 1/mpn/generic/toom33_mul.c
+++ 2/mpn/generic/toom33_mul.c
@@ -1,8 +1,7 @@
-/* mpn_toom33_mul -- Multiply {ap,an} and {p,bn} where an and bn are close in
+/* mpn_toom33_mul -- Multiply {ap,an} and {bp,bn} where an and bn are close in
    size.  Or more accurately, bn <= an < (3/2)bn.
 
    Contributed to the GNU project by Torbjorn Granlund.
-   Additional improvements by Marco Bodrato.
 
    THE FUNCTION IN THIS FILE IS INTERNAL WITH A MUTABLE INTERFACE.  IT IS ONLY
    SAFE TO REACH IT THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
@@ -26,16 +25,24 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1, asm1, bs1, and bsm1 could be
+     avoided by instead reusing the pp area and the scratch area.
+  2. Use new toom functions for the recursive calls.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
 /* Evaluate in: -1, 0, +1, +2, +inf
 
-  <-s--><--n--><--n--><--n-->
-   ____ ______ ______ ______
-  |_a3_|___a2_|___a1_|___a0_|
-   |b3_|___b2_|___b1_|___b0_|
-   <-t-><--n--><--n--><--n-->
+  <-s-><--n--><--n--><--n-->
+   ___ ______ ______ ______
+  |a3_|___a2_|___a1_|___a0_|
+	       |_b1_|___b0_|
+	       <-t--><--n-->
 
   v0  =  a0         * b0          #   A(0)*B(0)
   v1  = (a0+ a1+ a2)*(b0+ b1+ b2) #   A(1)*B(1)      ah  <= 2  bh <= 2
@@ -49,28 +56,21 @@
 #define MAYBE_mul_toom33   1
 #else
 #define MAYBE_mul_basecase						\
-  (MUL_TOOM33_THRESHOLD < 3 * MUL_TOOM22_THRESHOLD)
+  (MUL_TOOM33_THRESHOLD < 3 * MUL_KARATSUBA_THRESHOLD)
 #define MAYBE_mul_toom33						\
   (MUL_TOOM44_THRESHOLD >= 3 * MUL_TOOM33_THRESHOLD)
 #endif
 
-/* FIXME: TOOM33_MUL_N_REC is not quite right for a balanced
-   multiplication at the infinity point. We may have
-   MAYBE_mul_basecase == 0, and still get s just below
-   MUL_TOOM22_THRESHOLD. If MUL_TOOM33_THRESHOLD == 7, we can even get
-   s == 1 and mpn_toom22_mul will crash.
-*/
-
 #define TOOM33_MUL_N_REC(p, a, b, n, ws)				\
   do {									\
     if (MAYBE_mul_basecase						\
-	&& BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))			\
+	&& BELOW_THRESHOLD (n, MUL_KARATSUBA_THRESHOLD))		\
       mpn_mul_basecase (p, a, n, b, n);					\
     else if (! MAYBE_mul_toom33						\
 	     || BELOW_THRESHOLD (n, MUL_TOOM33_THRESHOLD))		\
-      mpn_toom22_mul (p, a, n, b, n, ws);				\
+      mpn_kara_mul_n (p, a, b, n, ws);					\
     else								\
-      mpn_toom33_mul (p, a, n, b, n, ws);				\
+      mpn_toom3_mul_n (p, a, b, n, ws);					\
   } while (0)
 
 void
@@ -85,6 +85,7 @@
   mp_ptr gp;
   mp_ptr as1, asm1, as2;
   mp_ptr bs1, bsm1, bs2;
+  TMP_DECL;
 
 #define a0  ap
 #define a1  (ap + n)
@@ -103,31 +104,33 @@
   ASSERT (0 < s && s <= n);
   ASSERT (0 < t && t <= n);
 
-  as1  = scratch + 4 * n + 4;
-  asm1 = scratch + 2 * n + 2;
-  as2 = pp + n + 1;
-
-  bs1 = pp;
-  bsm1 = scratch + 3 * n + 3; /* we need 4n+4 <= 4n+s+t */
-  bs2 = pp + 2 * n + 2;
+  TMP_MARK;
+
+  as1 = TMP_SALLOC_LIMBS (n + 1);
+  asm1 = TMP_SALLOC_LIMBS (n + 1);
+  as2 = TMP_SALLOC_LIMBS (n + 1);
 
-  gp = scratch;
+  bs1 = TMP_SALLOC_LIMBS (n + 1);
+  bsm1 = TMP_SALLOC_LIMBS (n + 1);
+  bs2 = TMP_SALLOC_LIMBS (n + 1);
+
+  gp = pp;
 
   vm1_neg = 0;
 
   /* Compute as1 and asm1.  */
   cy = mpn_add (gp, a0, n, a2, s);
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
   if (cy == 0 && mpn_cmp (gp, a1, n) < 0)
     {
-      cy = mpn_add_n_sub_n (as1, asm1, a1, gp, n);
+      cy = mpn_addsub_n (as1, asm1, a1, gp, n);
       as1[n] = 0;
       asm1[n] = 0;
       vm1_neg = 1;
     }
   else
     {
-      cy2 = mpn_add_n_sub_n (as1, asm1, gp, a1, n);
+      cy2 = mpn_addsub_n (as1, asm1, gp, a1, n);
       as1[n] = cy + (cy2 >> 1);
       asm1[n] = cy - (cy & 1);
     }
@@ -147,42 +150,34 @@
 #endif
 
   /* Compute as2.  */
-#if HAVE_NATIVE_mpn_rsblsh1_n
-  cy = mpn_add_n (as2, a2, as1, s);
-  if (s != n)
-    cy = mpn_add_1 (as2 + s, as1 + s, n - s, cy);
-  cy += as1[n];
-  cy = 2 * cy + mpn_rsblsh1_n (as2, a0, as2, n);
-#else
 #if HAVE_NATIVE_mpn_addlsh1_n
   cy  = mpn_addlsh1_n (as2, a1, a2, s);
   if (s != n)
     cy = mpn_add_1 (as2 + s, a1 + s, n - s, cy);
   cy = 2 * cy + mpn_addlsh1_n (as2, a0, as2, n);
 #else
-  cy = mpn_add_n (as2, a2, as1, s);
+  cy  = mpn_lshift (as2, a2, s, 1);
+  cy += mpn_add_n (as2, a1, as2, s);
   if (s != n)
-    cy = mpn_add_1 (as2 + s, as1 + s, n - s, cy);
-  cy += as1[n];
+    cy = mpn_add_1 (as2 + s, a1 + s, n - s, cy);
   cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
-  cy -= mpn_sub_n (as2, as2, a0, n);
-#endif
+  cy += mpn_add_n (as2, a0, as2, n);
 #endif
   as2[n] = cy;
 
   /* Compute bs1 and bsm1.  */
   cy = mpn_add (gp, b0, n, b2, t);
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
   if (cy == 0 && mpn_cmp (gp, b1, n) < 0)
     {
-      cy = mpn_add_n_sub_n (bs1, bsm1, b1, gp, n);
+      cy = mpn_addsub_n (bs1, bsm1, b1, gp, n);
       bs1[n] = 0;
       bsm1[n] = 0;
       vm1_neg ^= 1;
     }
   else
     {
-      cy2 = mpn_add_n_sub_n (bs1, bsm1, gp, b1, n);
+      cy2 = mpn_addsub_n (bs1, bsm1, gp, b1, n);
       bs1[n] = cy + (cy2 >> 1);
       bsm1[n] = cy - (cy & 1);
     }
@@ -202,26 +197,18 @@
 #endif
 
   /* Compute bs2.  */
-#if HAVE_NATIVE_mpn_rsblsh1_n
-  cy = mpn_add_n (bs2, b2, bs1, t);
-  if (t != n)
-    cy = mpn_add_1 (bs2 + t, bs1 + t, n - t, cy);
-  cy += bs1[n];
-  cy = 2 * cy + mpn_rsblsh1_n (bs2, b0, bs2, n);
-#else
 #if HAVE_NATIVE_mpn_addlsh1_n
   cy  = mpn_addlsh1_n (bs2, b1, b2, t);
   if (t != n)
     cy = mpn_add_1 (bs2 + t, b1 + t, n - t, cy);
   cy = 2 * cy + mpn_addlsh1_n (bs2, b0, bs2, n);
 #else
-  cy  = mpn_add_n (bs2, bs1, b2, t);
+  cy  = mpn_lshift (bs2, b2, t, 1);
+  cy += mpn_add_n (bs2, b1, bs2, t);
   if (t != n)
-    cy = mpn_add_1 (bs2 + t, bs1 + t, n - t, cy);
-  cy += bs1[n];
+    cy = mpn_add_1 (bs2 + t, b1 + t, n - t, cy);
   cy = 2 * cy + mpn_lshift (bs2, bs2, n, 1);
-  cy -= mpn_sub_n (bs2, bs2, b0, n);
-#endif
+  cy += mpn_add_n (bs2, b0, bs2, n);
 #endif
   bs2[n] = cy;
 
@@ -237,7 +224,7 @@
 #define vinf  (pp + 4 * n)			/* s+t */
 #define vm1   scratch				/* 2n+1 */
 #define v2    (scratch + 2 * n + 1)		/* 2n+2 */
-#define scratch_out  (scratch + 5 * n + 5)
+#define scratch_out  (scratch + 4 * n + 4)
 
   /* vm1, 2n+1 limbs */
 #ifdef SMALLER_RECURSION
@@ -298,5 +285,7 @@
 
   TOOM33_MUL_N_REC (v0, ap, bp, n, scratch_out);	/* v0, 2n limbs */
 
-  mpn_toom_interpolate_5pts (pp, v2, vm1, n, s + t, vm1_neg, vinf0);
+  mpn_toom_interpolate_5pts (pp, v2, vm1, n, s + t, 1^vm1_neg, vinf0, scratch_out);
+
+  TMP_FREE;
 }
--- 1/mpn/generic/toom3_sqr.c
+++ 2/mpn/generic/toom3_sqr.c
@@ -1,7 +1,6 @@
 /* mpn_toom3_sqr -- Square {ap,an}.
 
    Contributed to the GNU project by Torbjorn Granlund.
-   Additional improvements by Marco Bodrato.
 
    THE FUNCTION IN THIS FILE IS INTERNAL WITH A MUTABLE INTERFACE.  IT IS ONLY
    SAFE TO REACH IT THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
@@ -25,20 +24,30 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1 and asm1 could be
+     avoided by instead reusing the pp area and the scratch area.
+  2. Use new toom functions for the recursive calls.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
 /* Evaluate in: -1, 0, +1, +2, +inf
 
-  <-s--><--n--><--n-->
-   ____ ______ ______
-  |_a2_|___a1_|___a0_|
-
-  v0  =  a0         ^2 #   A(0)^2
-  v1  = (a0+ a1+ a2)^2 #   A(1)^2    ah  <= 2
-  vm1 = (a0- a1+ a2)^2 #  A(-1)^2   |ah| <= 1
-  v2  = (a0+2a1+4a2)^2 #   A(2)^2    ah  <= 6
-  vinf=          a2 ^2 # A(inf)^2
+  <-s-><--n--><--n--><--n-->
+   ___ ______ ______ ______
+  |a3_|___a2_|___a1_|___a0_|
+	       |_b1_|___b0_|
+	       <-t--><--n-->
+
+  v0  =  a0         * b0          #   A(0)*B(0)
+  v1  = (a0+ a1+ a2)*(b0+ b1+ b2) #   A(1)*B(1)      ah  <= 2  bh <= 2
+  vm1 = (a0- a1+ a2)*(b0- b1+ b2) #  A(-1)*B(-1)    |ah| <= 1  bh <= 1
+  v2  = (a0+2a1+4a2)*(b0+2b1+4b2) #   A(2)*B(2)      ah  <= 6  bh <= 6
+  vinf=          a2 *         b2  # A(inf)*B(inf)
 */
 
 #if TUNE_PROGRAM_BUILD
@@ -46,7 +55,7 @@
 #define MAYBE_sqr_toom3   1
 #else
 #define MAYBE_sqr_basecase						\
-  (SQR_TOOM3_THRESHOLD < 3 * SQR_TOOM2_THRESHOLD)
+  (SQR_TOOM3_THRESHOLD < 3 * SQR_KARATSUBA_THRESHOLD)
 #define MAYBE_sqr_toom3							\
   (SQR_TOOM4_THRESHOLD >= 3 * SQR_TOOM3_THRESHOLD)
 #endif
@@ -54,13 +63,13 @@
 #define TOOM3_SQR_N_REC(p, a, n, ws)					\
   do {									\
     if (MAYBE_sqr_basecase						\
-	&& BELOW_THRESHOLD (n, SQR_TOOM2_THRESHOLD))			\
+	&& BELOW_THRESHOLD (n, SQR_KARATSUBA_THRESHOLD))		\
       mpn_sqr_basecase (p, a, n);					\
     else if (! MAYBE_sqr_toom3						\
 	     || BELOW_THRESHOLD (n, SQR_TOOM3_THRESHOLD))		\
-      mpn_toom2_sqr (p, a, n, ws);					\
+      mpn_kara_sqr_n (p, a, n, ws);					\
     else								\
-      mpn_toom3_sqr (p, a, n, ws);					\
+      mpn_toom3_sqr_n (p, a, n, ws);					\
   } while (0)
 
 void
@@ -72,6 +81,7 @@
   mp_limb_t cy, vinf0;
   mp_ptr gp;
   mp_ptr as1, asm1, as2;
+  TMP_DECL;
 
 #define a0  ap
 #define a1  (ap + n)
@@ -83,24 +93,26 @@
 
   ASSERT (0 < s && s <= n);
 
-  as1 = scratch + 4 * n + 4;
-  asm1 = scratch + 2 * n + 2;
-  as2 = pp + n + 1;
+  TMP_MARK;
 
-  gp = scratch;
+  as1 = TMP_SALLOC_LIMBS (n + 1);
+  asm1 = TMP_SALLOC_LIMBS (n + 1);
+  as2 = TMP_SALLOC_LIMBS (n + 1);
+
+  gp = pp;
 
   /* Compute as1 and asm1.  */
   cy = mpn_add (gp, a0, n, a2, s);
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
   if (cy == 0 && mpn_cmp (gp, a1, n) < 0)
     {
-      cy = mpn_add_n_sub_n (as1, asm1, a1, gp, n);
+      cy = mpn_addsub_n (as1, asm1, a1, gp, n);
       as1[n] = 0;
       asm1[n] = 0;
     }
   else
     {
-      cy2 = mpn_add_n_sub_n (as1, asm1, gp, a1, n);
+      cy2 = mpn_addsub_n (as1, asm1, gp, a1, n);
       as1[n] = cy + (cy2 >> 1);
       asm1[n] = cy - (cy & 1);
     }
@@ -119,26 +131,18 @@
 #endif
 
   /* Compute as2.  */
-#if HAVE_NATIVE_mpn_rsblsh1_n
-  cy = mpn_add_n (as2, a2, as1, s);
-  if (s != n)
-    cy = mpn_add_1 (as2 + s, as1 + s, n - s, cy);
-  cy += as1[n];
-  cy = 2 * cy + mpn_rsblsh1_n (as2, a0, as2, n);
-#else
 #if HAVE_NATIVE_mpn_addlsh1_n
   cy  = mpn_addlsh1_n (as2, a1, a2, s);
   if (s != n)
     cy = mpn_add_1 (as2 + s, a1 + s, n - s, cy);
   cy = 2 * cy + mpn_addlsh1_n (as2, a0, as2, n);
 #else
-  cy = mpn_add_n (as2, a2, as1, s);
+  cy  = mpn_lshift (as2, a2, s, 1);
+  cy += mpn_add_n (as2, a1, as2, s);
   if (s != n)
-    cy = mpn_add_1 (as2 + s, as1 + s, n - s, cy);
-  cy += as1[n];
+    cy = mpn_add_1 (as2 + s, a1 + s, n - s, cy);
   cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
-  cy -= mpn_sub_n (as2, as2, a0, n);
-#endif
+  cy += mpn_add_n (as2, a0, as2, n);
 #endif
   as2[n] = cy;
 
@@ -150,7 +154,7 @@
 #define vinf  (pp + 4 * n)			/* s+s */
 #define vm1   scratch				/* 2n+1 */
 #define v2    (scratch + 2 * n + 1)		/* 2n+2 */
-#define scratch_out  (scratch + 5 * n + 5)
+#define scratch_out  (scratch + 4 * n + 4)
 
   /* vm1, 2n+1 limbs */
 #ifdef SMALLER_RECURSION
@@ -209,5 +213,7 @@
 
   TOOM3_SQR_N_REC (v0, ap, n, scratch_out);	/* v0, 2n limbs */
 
-  mpn_toom_interpolate_5pts (pp, v2, vm1, n, s + s, 0, vinf0);
+  mpn_toom_interpolate_5pts (pp, v2, vm1, n, s + s, 1, vinf0, scratch_out);
+
+  TMP_FREE;
 }
--- 1/mpn/generic/toom42_mul.c
+++ 2/mpn/generic/toom42_mul.c
@@ -29,6 +29,15 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1, asm1, bs1, and bsm1 could be
+     avoided by instead reusing the pp area and the scratch allocation.
+
+  2. Apply optimizations also to mul_toom32.c.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
@@ -47,9 +56,20 @@
   vinf=              a3 *     b1  # A(inf)*B(inf)
 */
 
-#define TOOM42_MUL_N_REC(p, a, b, n, ws)				\
+#if TUNE_PROGRAM_BUILD
+#define MAYBE_mul_toom22   1
+#else
+#define MAYBE_mul_toom22						\
+  (MUL_TOOM33_THRESHOLD >= 2 * MUL_TOOM22_THRESHOLD)
+#endif
+
+#define TOOM22_MUL_N_REC(p, a, b, n, ws)				\
   do {									\
-    mpn_mul_n (p, a, b, n);						\
+    if (! MAYBE_mul_toom22						\
+	|| BELOW_THRESHOLD (n, MUL_KARATSUBA_THRESHOLD))		\
+      mpn_mul_basecase (p, a, n, b, n);					\
+    else								\
+      mpn_toom22_mul (p, a, n, b, n, ws);				\
   } while (0)
 
 void
@@ -95,7 +115,32 @@
   a1_a3 = pp + n + 1;
 
   /* Compute as1 and asm1.  */
-  vm1_neg = mpn_toom_eval_dgr3_pm1 (as1, asm1, ap, n, s, a0_a2) & 1;
+  a0_a2[n] = mpn_add_n (a0_a2, a0, a2, n);
+  a1_a3[n] = mpn_add (a1_a3, a1, n, a3, s);
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (a0_a2, a1_a3, n + 1) < 0)
+    {
+      mpn_addsub_n (as1, asm1, a1_a3, a0_a2, n + 1);
+      vm1_neg = 1;
+    }
+  else
+    {
+      mpn_addsub_n (as1, asm1, a0_a2, a1_a3, n + 1);
+      vm1_neg = 0;
+    }
+#else
+  mpn_add_n (as1, a0_a2, a1_a3, n + 1);
+  if (mpn_cmp (a0_a2, a1_a3, n + 1) < 0)
+    {
+      mpn_sub_n (asm1, a1_a3, a0_a2, n + 1);
+      vm1_neg = 1;
+    }
+  else
+    {
+      mpn_sub_n (asm1, a0_a2, a1_a3, n + 1);
+      vm1_neg = 0;
+    }
+#endif
 
   /* Compute as2.  */
 #if HAVE_NATIVE_mpn_addlsh1_n
@@ -119,15 +164,15 @@
   /* Compute bs1 and bsm1.  */
   if (t == n)
     {
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
       if (mpn_cmp (b0, b1, n) < 0)
 	{
-	  cy = mpn_add_n_sub_n (bs1, bsm1, b1, b0, n);
+	  cy = mpn_addsub_n (bs1, bsm1, b1, b0, n);
 	  vm1_neg ^= 1;
 	}
       else
 	{
-	  cy = mpn_add_n_sub_n (bs1, bsm1, b0, b1, n);
+	  cy = mpn_addsub_n (bs1, bsm1, b0, b1, n);
 	}
       bs1[n] = cy >> 1;
 #else
@@ -175,16 +220,16 @@
 #define vinf  (pp + 4 * n)			/* s+t */
 #define vm1   scratch				/* 2n+1 */
 #define v2    (scratch + 2 * n + 1)		/* 2n+2 */
-#define scratch_out	scratch + 4 * n + 4	/* Currently unused. */
+#define scratch_out	scratch + 4 * n + 4
 
   /* vm1, 2n+1 limbs */
-  TOOM42_MUL_N_REC (vm1, asm1, bsm1, n, scratch_out);
+  TOOM22_MUL_N_REC (vm1, asm1, bsm1, n, scratch_out);
   cy = 0;
   if (asm1[n] != 0)
     cy = mpn_add_n (vm1 + n, vm1 + n, bsm1, n);
   vm1[2 * n] = cy;
 
-  TOOM42_MUL_N_REC (v2, as2, bs2, n + 1, scratch_out);	/* v2, 2n+1 limbs */
+  TOOM22_MUL_N_REC (v2, as2, bs2, n + 1, scratch_out);	/* v2, 2n+1 limbs */
 
   /* vinf, s+t limbs */
   if (s > t)  mpn_mul (vinf, a3, s, b1, t);
@@ -193,7 +238,7 @@
   vinf0 = vinf[0];				/* v1 overlaps with this */
 
   /* v1, 2n+1 limbs */
-  TOOM42_MUL_N_REC (v1, as1, bs1, n, scratch_out);
+  TOOM22_MUL_N_REC (v1, as1, bs1, n, scratch_out);
   if (as1[n] == 1)
     {
       cy = bs1[n] + mpn_add_n (v1 + n, v1 + n, bs1, n);
@@ -216,9 +261,9 @@
     cy += mpn_add_n (v1 + n, v1 + n, as1, n);
   v1[2 * n] = cy;
 
-  TOOM42_MUL_N_REC (v0, ap, bp, n, scratch_out);	/* v0, 2n limbs */
+  TOOM22_MUL_N_REC (v0, ap, bp, n, scratch_out);	/* v0, 2n limbs */
 
-  mpn_toom_interpolate_5pts (pp, v2, vm1, n, s + t, vm1_neg, vinf0);
+  mpn_toom_interpolate_5pts (pp, v2, vm1, n, s + t, 1^vm1_neg, vinf0, scratch + 4 * n + 4);
 
   TMP_FREE;
 }
--- 1/mpn/generic/toom44_mul.c
+++ 2/mpn/generic/toom44_mul.c
@@ -25,10 +25,18 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1, asm1, bs1, and bsm1 could be
+     avoided by instead reusing the pp area and the scratch area.
+  2. Use new toom functions for the recursive calls.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
-/* Evaluate in: 0, +1, -1, +2, -2, 1/2, +inf
+/* Evaluate in: -1, -1/2, 0, +1/2, +1, +2, +inf
 
   <-s--><--n--><--n--><--n-->
    ____ ______ ______ ______
@@ -40,8 +48,8 @@
   v1  = ( a0+ a1+ a2+ a3)*( b0+ b1+ b2+ b3) #    A(1)*B(1)      ah  <= 3   bh  <= 3
   vm1 = ( a0- a1+ a2- a3)*( b0- b1+ b2- b3) #   A(-1)*B(-1)    |ah| <= 1  |bh| <= 1
   v2  = ( a0+2a1+4a2+8a3)*( b0+2b1+4b2+8b3) #    A(2)*B(2)      ah  <= 14  bh  <= 14
-  vm2 = ( a0-2a1+4a2-8a3)*( b0-2b1+4b2-8b3) #    A(2)*B(2)      ah  <= 9  |bh| <= 9
   vh  = (8a0+4a1+2a2+ a3)*(8b0+4b1+2b2+ b3) #  A(1/2)*B(1/2)    ah  <= 14  bh  <= 14
+  vmh = (8a0-4a1+2a2- a3)*(8b0-4b1+2b2- b3) # A(-1/2)*B(-1/2)  -4<=ah<=9  -4<=bh<=9
   vinf=               a3 *          b2      #  A(inf)*B(inf)
 */
 
@@ -51,7 +59,7 @@
 #define MAYBE_mul_toom44   1
 #else
 #define MAYBE_mul_basecase						\
-  (MUL_TOOM44_THRESHOLD < 4 * MUL_TOOM22_THRESHOLD)
+  (MUL_TOOM44_THRESHOLD < 4 * MUL_KARATSUBA_THRESHOLD)
 #define MAYBE_mul_toom22						\
   (MUL_TOOM44_THRESHOLD < 4 * MUL_TOOM33_THRESHOLD)
 #define MAYBE_mul_toom44						\
@@ -61,41 +69,18 @@
 #define TOOM44_MUL_N_REC(p, a, b, n, ws)				\
   do {									\
     if (MAYBE_mul_basecase						\
-	&& BELOW_THRESHOLD (n, MUL_TOOM22_THRESHOLD))			\
+	&& BELOW_THRESHOLD (n, MUL_KARATSUBA_THRESHOLD))		\
       mpn_mul_basecase (p, a, n, b, n);					\
     else if (MAYBE_mul_toom22						\
 	     && BELOW_THRESHOLD (n, MUL_TOOM33_THRESHOLD))		\
-      mpn_toom22_mul (p, a, n, b, n, ws);				\
+      mpn_kara_mul_n (p, a, b, n, ws);					\
     else if (! MAYBE_mul_toom44						\
 	     || BELOW_THRESHOLD (n, MUL_TOOM44_THRESHOLD))		\
-      mpn_toom33_mul (p, a, n, b, n, ws);				\
+      mpn_toom3_mul_n (p, a, b, n, ws);					\
     else								\
       mpn_toom44_mul (p, a, n, b, n, ws);				\
   } while (0)
 
-/* Use of scratch space. In the product area, we store
-
-      ___________________
-     |vinf|____|_v1_|_v0_|
-      s+t  2n-1 2n+1  2n
-
-   The other recursive products, vm1, v2, vm2, vh are stored in the
-   scratch area. When computing them, we use the product area for
-   intermediate values.
-
-   Next, we compute v1. We can store the intermediate factors at v0
-   and at vh + 2n + 2.
-
-   Finally, for v0 and vinf, factors are parts of the input operands,
-   and we need scratch space only for the recursive multiplication.
-
-   In all, if S(an) is the scratch need, the needed space is bounded by
-
-     S(an) <= 4 (2*ceil(an/4) + 1) + 1 + S(ceil(an/4) + 1)
-
-   which should give S(n) = 8 n/3 + c log(n) for some constant c.
-*/
-
 void
 mpn_toom44_mul (mp_ptr pp,
 		mp_srcptr ap, mp_size_t an,
@@ -104,7 +89,11 @@
 {
   mp_size_t n, s, t;
   mp_limb_t cy;
-  enum toom7_flags flags;
+  mp_ptr gp, hp;
+  mp_ptr as1, asm1, as2, ash, asmh;
+  mp_ptr bs1, bsm1, bs2, bsh, bsmh;
+  enum toom4_flags flags;
+  TMP_DECL;
 
 #define a0  ap
 #define a1  (ap + n)
@@ -115,111 +104,227 @@
 #define b2  (bp + 2*n)
 #define b3  (bp + 3*n)
 
-  ASSERT (an >= bn);
-
   n = (an + 3) >> 2;
 
   s = an - 3 * n;
   t = bn - 3 * n;
 
+  ASSERT (an >= bn);
+
   ASSERT (0 < s && s <= n);
   ASSERT (0 < t && t <= n);
-  ASSERT (s >= t);
 
-  /* NOTE: The multiplications to v2, vm2, vh and vm1 overwrites the
-   * following limb, so these must be computed in order, and we need a
-   * one limb gap to tp. */
-#define v0    pp				/* 2n */
-#define v1    (pp + 2 * n)			/* 2n+1 */
-#define vinf  (pp + 6 * n)			/* s+t */
-#define v2    scratch				/* 2n+1 */
-#define vm2   (scratch + 2 * n + 1)		/* 2n+1 */
-#define vh    (scratch + 4 * n + 2)		/* 2n+1 */
-#define vm1   (scratch + 6 * n + 3)		/* 2n+1 */
-#define tp (scratch + 8*n + 5)
-
-  /* apx and bpx must not overlap with v1 */
-#define apx   pp				/* n+1 */
-#define amx   (pp + n + 1)			/* n+1 */
-#define bmx   (pp + 2*n + 2)			/* n+1 */
-#define bpx   (pp + 4*n + 2)			/* n+1 */
-
-  /* Total scratch need: 8*n + 5 + scratch for recursive calls. This
-     gives roughly 32 n/3 + log term. */
-
-  /* Compute apx = a0 + 2 a1 + 4 a2 + 8 a3 and amx = a0 - 2 a1 + 4 a2 - 8 a3.  */
-  flags = toom7_w1_neg & mpn_toom_eval_dgr3_pm2 (apx, amx, ap, n, s, tp);
+  TMP_MARK;
 
-  /* Compute bpx = b0 + 2 b1 + 4 b2 + 8 b3 and bmx = b0 - 2 b1 + 4 b2 - 8 b3.  */
-  flags ^= toom7_w1_neg & mpn_toom_eval_dgr3_pm2 (bpx, bmx, bp, n, t, tp);
+  as1  = TMP_ALLOC_LIMBS (10 * n + 10);
+  asm1 = as1  + n + 1;
+  as2  = asm1 + n + 1;
+  ash  = as2  + n + 1;
+  asmh = ash  + n + 1;
+  bs1  = asmh + n + 1;
+  bsm1 = bs1  + n + 1;
+  bs2  = bsm1 + n + 1;
+  bsh  = bs2  + n + 1;
+  bsmh = bsh  + n + 1;
+
+  gp = pp;
+  hp = pp + n + 1;
+
+  flags = 0;
+
+  /* Compute as1 and asm1.  */
+  gp[n]  = mpn_add_n (gp, a0, a2, n);
+  hp[n]  = mpn_add (hp, a1, n, a3, s);
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_addsub_n (as1, asm1, hp, gp, n + 1);
+      flags ^= toom4_w3_neg;
+    }
+  else
+    {
+      mpn_addsub_n (as1, asm1, gp, hp, n + 1);
+    }
+#else
+  mpn_add_n (as1, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (asm1, hp, gp, n + 1);
+      flags ^= toom4_w3_neg;
+    }
+  else
+    {
+      mpn_sub_n (asm1, gp, hp, n + 1);
+    }
+#endif
 
-  TOOM44_MUL_N_REC (v2, apx, bpx, n + 1, tp);	/* v2,  2n+1 limbs */
-  TOOM44_MUL_N_REC (vm2, amx, bmx, n + 1, tp);	/* vm2,  2n+1 limbs */
+  /* Compute as2.  */
+#if HAVE_NATIVE_mpn_addlsh1_n
+  cy  = mpn_addlsh1_n (as2, a2, a3, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a2 + s, n - s, cy);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a1, as2, n);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a0, as2, n);
+#else
+  cy  = mpn_lshift (as2, a3, s, 1);
+  cy += mpn_add_n (as2, a2, as2, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a2 + s, n - s, cy);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a1, as2, n);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a0, as2, n);
+#endif
+  as2[n] = cy;
 
-  /* Compute apx = 8 a0 + 4 a1 + 2 a2 + a3 = (((2*a0 + a1) * 2 + a2) * 2 + a3 */
+  /* Compute ash and asmh.  */
+  cy  = mpn_lshift (gp, a0, n, 3);			/*  8a0             */
 #if HAVE_NATIVE_mpn_addlsh1_n
-  cy = mpn_addlsh1_n (apx, a1, a0, n);
-  cy = 2*cy + mpn_addlsh1_n (apx, a2, apx, n);
-  if (s < n)
+  gp[n] = cy + mpn_addlsh1_n (gp, gp, a2, n);		/*  8a0 + 2a2       */
+#else
+  cy += mpn_lshift (hp, a2, n, 1);			/*        2a2       */
+  gp[n] = cy + mpn_add_n (gp, gp, hp, n);		/*  8a0 + 2a2       */
+#endif
+  cy = mpn_lshift (hp, a1, n, 2);			/*  4a1             */
+  hp[n] = cy + mpn_add (hp, hp, n, a3, s);		/*  4a1 +  a3       */
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
     {
-      mp_limb_t cy2;
-      cy2 = mpn_addlsh1_n (apx, a3, apx, s);
-      apx[n] = 2*cy + mpn_lshift (apx + s, apx + s, n - s, 1);
-      MPN_INCR_U (apx + s, n+1-s, cy2);
+      mpn_addsub_n (ash, asmh, hp, gp, n + 1);
+      flags ^= toom4_w1_neg;
     }
   else
-    apx[n] = 2*cy + mpn_addlsh1_n (apx, a3, apx, n);
+    {
+      mpn_addsub_n (ash, asmh, gp, hp, n + 1);
+    }
 #else
-  cy = mpn_lshift (apx, a0, n, 1);
-  cy += mpn_add_n (apx, apx, a1, n);
-  cy = 2*cy + mpn_lshift (apx, apx, n, 1);
-  cy += mpn_add_n (apx, apx, a2, n);
-  cy = 2*cy + mpn_lshift (apx, apx, n, 1);
-  apx[n] = cy + mpn_add (apx, apx, n, a3, s);
+  mpn_add_n (ash, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (asmh, hp, gp, n + 1);
+      flags ^= toom4_w1_neg;
+    }
+  else
+    {
+      mpn_sub_n (asmh, gp, hp, n + 1);
+    }
 #endif
 
-  /* Compute bpx = 8 b0 + 4 b1 + 2 b2 + b3 = (((2*b0 + b1) * 2 + b2) * 2 + b3 */
+  /* Compute bs1 and bsm1.  */
+  gp[n]  = mpn_add_n (gp, b0, b2, n);
+  hp[n]  = mpn_add (hp, b1, n, b3, t);
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_addsub_n (bs1, bsm1, hp, gp, n + 1);
+      flags ^= toom4_w3_neg;
+    }
+  else
+    {
+      mpn_addsub_n (bs1, bsm1, gp, hp, n + 1);
+    }
+#else
+  mpn_add_n (bs1, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (bsm1, hp, gp, n + 1);
+      flags ^= toom4_w3_neg;
+    }
+  else
+    {
+      mpn_sub_n (bsm1, gp, hp, n + 1);
+    }
+#endif
+
+  /* Compute bs2.  */
 #if HAVE_NATIVE_mpn_addlsh1_n
-  cy = mpn_addlsh1_n (bpx, b1, b0, n);
-  cy = 2*cy + mpn_addlsh1_n (bpx, b2, bpx, n);
-  if (t < n)
+  cy  = mpn_addlsh1_n (bs2, b2, b3, t);
+  if (t != n)
+    cy = mpn_add_1 (bs2 + t, b2 + t, n - t, cy);
+  cy = 2 * cy + mpn_addlsh1_n (bs2, b1, bs2, n);
+  cy = 2 * cy + mpn_addlsh1_n (bs2, b0, bs2, n);
+#else
+  cy  = mpn_lshift (bs2, b3, t, 1);
+  cy += mpn_add_n (bs2, b2, bs2, t);
+  if (t != n)
+    cy = mpn_add_1 (bs2 + t, b2 + t, n - t, cy);
+  cy = 2 * cy + mpn_lshift (bs2, bs2, n, 1);
+  cy += mpn_add_n (bs2, b1, bs2, n);
+  cy = 2 * cy + mpn_lshift (bs2, bs2, n, 1);
+  cy += mpn_add_n (bs2, b0, bs2, n);
+#endif
+  bs2[n] = cy;
+
+  /* Compute bsh and bsmh.  */
+  cy  = mpn_lshift (gp, b0, n, 3);			/*  8b0             */
+#if HAVE_NATIVE_mpn_addlsh1_n
+  gp[n] = cy + mpn_addlsh1_n (gp, gp, b2, n);		/*  8b0 + 2b2       */
+#else
+  cy += mpn_lshift (hp, b2, n, 1);			/*        2b2       */
+  gp[n] = cy + mpn_add_n (gp, gp, hp, n);		/*  8b0 + 2b2       */
+#endif
+  cy = mpn_lshift (hp, b1, n, 2);			/*  4b1             */
+  hp[n] = cy + mpn_add (hp, hp, n, b3, t);		/*  4b1 +  b3       */
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
     {
-      mp_limb_t cy2;
-      cy2 = mpn_addlsh1_n (bpx, b3, bpx, t);
-      bpx[n] = 2*cy + mpn_lshift (bpx + t, bpx + t, n - t, 1);
-      MPN_INCR_U (bpx + t, n+1-t, cy2);
+      mpn_addsub_n (bsh, bsmh, hp, gp, n + 1);
+      flags ^= toom4_w1_neg;
     }
   else
-    bpx[n] = 2*cy + mpn_addlsh1_n (bpx, b3, bpx, n);
+    {
+      mpn_addsub_n (bsh, bsmh, gp, hp, n + 1);
+    }
 #else
-  cy = mpn_lshift (bpx, b0, n, 1);
-  cy += mpn_add_n (bpx, bpx, b1, n);
-  cy = 2*cy + mpn_lshift (bpx, bpx, n, 1);
-  cy += mpn_add_n (bpx, bpx, b2, n);
-  cy = 2*cy + mpn_lshift (bpx, bpx, n, 1);
-  bpx[n] = cy + mpn_add (bpx, bpx, n, b3, t);
+  mpn_add_n (bsh, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (bsmh, hp, gp, n + 1);
+      flags ^= toom4_w1_neg;
+    }
+  else
+    {
+      mpn_sub_n (bsmh, gp, hp, n + 1);
+    }
 #endif
 
-  ASSERT (apx[n] < 15);
-  ASSERT (bpx[n] < 15);
+  ASSERT (as1[n] <= 3);
+  ASSERT (bs1[n] <= 3);
+  ASSERT (asm1[n] <= 1);
+  ASSERT (bsm1[n] <= 1);
+  ASSERT (as2[n] <= 14);
+  ASSERT (bs2[n] <= 14);
+  ASSERT (ash[n] <= 14);
+  ASSERT (bsh[n] <= 14);
+  ASSERT (asmh[n] <= 9);
+  ASSERT (bsmh[n] <= 9);
 
-  TOOM44_MUL_N_REC (vh, apx, bpx, n + 1, tp);	/* vh,  2n+1 limbs */
+#define v0    pp				/* 2n */
+#define v1    (scratch + 6 * n + 6)		/* 2n+1 */
+#define vm1   scratch				/* 2n+1 */
+#define v2    (scratch + 2 * n + 2)		/* 2n+1 */
+#define vinf  (pp + 6 * n)			/* s+t */
+#define vh    (pp + 2 * n)			/* 2n+1 */
+#define vmh   (scratch + 4 * n + 4)
+#define scratch_out  (scratch + 8 * n + 8)
 
-  /* Compute apx = a0 + a1 + a2 + a3 and amx = a0 - a1 + a2 - a3.  */
-  flags |= toom7_w3_neg & mpn_toom_eval_dgr3_pm1 (apx, amx, ap, n, s, tp);
+  /* vm1, 2n+1 limbs */
+  TOOM44_MUL_N_REC (vm1, asm1, bsm1, n + 1, scratch_out);	/* vm1, 2n+1 limbs */
 
-  /* Compute bpx = b0 + b1 + b2 + b3 bnd bmx = b0 - b1 + b2 - b3.  */
-  flags ^= toom7_w3_neg & mpn_toom_eval_dgr3_pm1 (bpx, bmx, bp, n, t, tp);
+  TOOM44_MUL_N_REC (v2 , as2 , bs2 , n + 1, scratch_out);	/* v2,  2n+1 limbs */
 
-  TOOM44_MUL_N_REC (vm1, amx, bmx, n + 1, tp);	/* vm1,  2n+1 limbs */
-  /* Clobbers amx, bmx. */
-  TOOM44_MUL_N_REC (v1, apx, bpx, n + 1, tp);	/* v1,  2n+1 limbs */
+  if (s > t)  mpn_mul (vinf, a3, s, b3, t);
+  else   TOOM44_MUL_N_REC (vinf, a3, b3, s, scratch_out);	/* vinf, s+t limbs */
 
-  TOOM44_MUL_N_REC (v0, a0, b0, n, tp);
-  if (s > t)
-    mpn_mul (vinf, a3, s, b3, t);
-  else
-    TOOM44_MUL_N_REC (vinf, a3, b3, s, tp);	/* vinf, s+t limbs */
+  TOOM44_MUL_N_REC (v1 , as1 , bs1 , n + 1, scratch_out);	/* v1,  2n+1 limbs */
+
+  TOOM44_MUL_N_REC (vh , ash , bsh , n + 1, scratch_out);
+
+  TOOM44_MUL_N_REC (vmh, asmh, bsmh, n + 1, scratch_out);
+
+  TOOM44_MUL_N_REC (v0 , ap  , bp  , n    , scratch_out);	/* v0,  2n limbs */
+
+  mpn_toom_interpolate_7pts (pp, n, flags, vmh, vm1, v1, v2, s + t, scratch_out);
 
-  mpn_toom_interpolate_7pts (pp, n, flags, vm2, vm1, v2, vh, s + t, tp);
+  TMP_FREE;
 }
--- 1/mpn/generic/toom4_sqr.c
+++ 2/mpn/generic/toom4_sqr.c
@@ -24,6 +24,14 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1, asm1, bs1, and bsm1 could be
+     avoided by instead reusing the pp area and the scratch area.
+  2. Use new toom functions for the recursive calls.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
@@ -32,14 +40,16 @@
   <-s--><--n--><--n--><--n-->
    ____ ______ ______ ______
   |_a3_|___a2_|___a1_|___a0_|
+   |b3_|___b2_|___b1_|___b0_|
+   <-t-><--n--><--n--><--n-->
 
-  v0  =   a0             ^2 #    A(0)^2
-  v1  = ( a0+ a1+ a2+ a3)^2 #    A(1)^2   ah  <= 3
-  vm1 = ( a0- a1+ a2- a3)^2 #   A(-1)^2  |ah| <= 1
-  v2  = ( a0+2a1+4a2+8a3)^2 #    A(2)^2   ah  <= 14
-  vh  = (8a0+4a1+2a2+ a3)^2 #  A(1/2)^2   ah  <= 14
-  vmh = (8a0-4a1+2a2- a3)^2 # A(-1/2)^2  -4<=ah<=9
-  vinf=               a3 ^2 #  A(inf)^2
+  v0  =   a0             *  b0              #    A(0)*B(0)
+  v1  = ( a0+ a1+ a2+ a3)*( b0+ b1+ b2+ b3) #    A(1)*B(1)      ah  <= 3   bh  <= 3
+  vm1 = ( a0- a1+ a2- a3)*( b0- b1+ b2- b3) #   A(-1)*B(-1)    |ah| <= 1  |bh| <= 1
+  v2  = ( a0+2a1+4a2+8a3)*( b0+2b1+4b2+8b3) #    A(2)*B(2)      ah  <= 14  bh  <= 14
+  vh  = (8a0+4a1+2a2+ a3)*(8b0+4b1+2b2+ b3) #  A(1/2)*B(1/2)    ah  <= 14  bh  <= 14
+  vmh = (8a0-4a1+2a2- a3)*(8b0-4b1+2b2- b3) # A(-1/2)*B(-1/2)  -4<=ah<=9  -4<=bh<=9
+  vinf=               a3 *          b2      #  A(inf)*B(inf)
 */
 
 #if TUNE_PROGRAM_BUILD
@@ -48,7 +58,7 @@
 #define MAYBE_sqr_toom4   1
 #else
 #define MAYBE_sqr_basecase						\
-  (SQR_TOOM4_THRESHOLD < 4 * SQR_TOOM2_THRESHOLD)
+  (SQR_TOOM4_THRESHOLD < 4 * SQR_KARATSUBA_THRESHOLD)
 #define MAYBE_sqr_toom2							\
   (SQR_TOOM4_THRESHOLD < 4 * SQR_TOOM3_THRESHOLD)
 #define MAYBE_sqr_toom4							\
@@ -58,14 +68,14 @@
 #define TOOM4_SQR_N_REC(p, a, n, ws)					\
   do {									\
     if (MAYBE_sqr_basecase						\
-	&& BELOW_THRESHOLD (n, SQR_TOOM2_THRESHOLD))			\
+	&& BELOW_THRESHOLD (n, SQR_KARATSUBA_THRESHOLD))		\
       mpn_sqr_basecase (p, a, n);					\
     else if (MAYBE_sqr_toom2						\
 	     && BELOW_THRESHOLD (n, SQR_TOOM3_THRESHOLD))		\
-      mpn_toom2_sqr (p, a, n, ws);					\
+      mpn_kara_sqr_n (p, a, n, ws);					\
     else if (! MAYBE_sqr_toom4						\
 	     || BELOW_THRESHOLD (n, SQR_TOOM4_THRESHOLD))		\
-      mpn_toom3_sqr (p, a, n, ws);					\
+      mpn_toom3_sqr_n (p, a, n, ws);					\
     else								\
       mpn_toom4_sqr (p, a, n, ws);					\
   } while (0)
@@ -77,6 +87,9 @@
 {
   mp_size_t n, s;
   mp_limb_t cy;
+  mp_ptr gp, hp;
+  mp_ptr as1, asm1, as2, ash, asmh;
+  TMP_DECL;
 
 #define a0  ap
 #define a1  (ap + n)
@@ -89,65 +102,122 @@
 
   ASSERT (0 < s && s <= n);
 
-  /* NOTE: The multiplications to v2, vm2, vh and vm1 overwrites the
-   * following limb, so these must be computed in order, and we need a
-   * one limb gap to tp. */
-#define v0    pp				/* 2n */
-#define v1    (pp + 2 * n)			/* 2n+1 */
-#define vinf  (pp + 6 * n)			/* s+t */
-#define v2    scratch				/* 2n+1 */
-#define vm2   (scratch + 2 * n + 1)		/* 2n+1 */
-#define vh    (scratch + 4 * n + 2)		/* 2n+1 */
-#define vm1   (scratch + 6 * n + 3)		/* 2n+1 */
-#define tp (scratch + 8*n + 5)
-
-  /* No overlap with v1 */
-#define apx   pp				/* n+1 */
-#define amx   (pp + 4*n + 2)			/* n+1 */
-
-  /* Total scratch need: 8*n + 5 + scratch for recursive calls. This
-     gives roughly 32 n/3 + log term. */
+  TMP_MARK;
 
-  /* Compute apx = a0 + 2 a1 + 4 a2 + 8 a3 and amx = a0 - 2 a1 + 4 a2 - 8 a3.  */
-  mpn_toom_eval_dgr3_pm2 (apx, amx, ap, n, s, tp);
+  as1  = TMP_SALLOC_LIMBS (n + 1);
+  asm1 = TMP_SALLOC_LIMBS (n + 1);
+  as2  = TMP_SALLOC_LIMBS (n + 1);
+  ash  = TMP_SALLOC_LIMBS (n + 1);
+  asmh = TMP_SALLOC_LIMBS (n + 1);
+
+  gp = pp;
+  hp = pp + n + 1;
+
+  /* Compute as1 and asm1.  */
+  gp[n]  = mpn_add_n (gp, a0, a2, n);
+  hp[n]  = mpn_add (hp, a1, n, a3, s);
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_addsub_n (as1, asm1, hp, gp, n + 1);
+    }
+  else
+    {
+      mpn_addsub_n (as1, asm1, gp, hp, n + 1);
+    }
+#else
+  mpn_add_n (as1, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (asm1, hp, gp, n + 1);
+    }
+  else
+    {
+      mpn_sub_n (asm1, gp, hp, n + 1);
+    }
+#endif
 
-  TOOM4_SQR_N_REC (v2, apx, n + 1, tp);	/* v2,  2n+1 limbs */
-  TOOM4_SQR_N_REC (vm2, amx, n + 1, tp);	/* vm2,  2n+1 limbs */
+  /* Compute as2.  */
+#if HAVE_NATIVE_mpn_addlsh1_n
+  cy  = mpn_addlsh1_n (as2, a2, a3, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a2 + s, n - s, cy);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a1, as2, n);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a0, as2, n);
+#else
+  cy  = mpn_lshift (as2, a3, s, 1);
+  cy += mpn_add_n (as2, a2, as2, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a2 + s, n - s, cy);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a1, as2, n);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a0, as2, n);
+#endif
+  as2[n] = cy;
 
-  /* Compute apx = 8 a0 + 4 a1 + 2 a2 + a3 = (((2*a0 + a1) * 2 + a2) * 2 + a3 */
+  /* Compute ash and asmh.  */
+  cy  = mpn_lshift (gp, a0, n, 3);			/*  8a0             */
 #if HAVE_NATIVE_mpn_addlsh1_n
-  cy = mpn_addlsh1_n (apx, a1, a0, n);
-  cy = 2*cy + mpn_addlsh1_n (apx, a2, apx, n);
-  if (s < n)
-    {
-      mp_limb_t cy2;
-      cy2 = mpn_addlsh1_n (apx, a3, apx, s);
-      apx[n] = 2*cy + mpn_lshift (apx + s, apx + s, n - s, 1);
-      MPN_INCR_U (apx + s, n+1-s, cy2);
+  gp[n] = cy + mpn_addlsh1_n (gp, gp, a2, n);		/*  8a0 + 2a2       */
+#else
+  cy += mpn_lshift (hp, a2, n, 1);			/*        2a2       */
+  gp[n] = cy + mpn_add_n (gp, gp, hp, n);		/*  8a0 + 2a2       */
+#endif
+  cy = mpn_lshift (hp, a1, n, 2);			/*  4a1             */
+  hp[n] = cy + mpn_add (hp, hp, n, a3, s);		/*  4a1 +  a3       */
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_addsub_n (ash, asmh, hp, gp, n + 1);
     }
   else
-    apx[n] = 2*cy + mpn_addlsh1_n (apx, a3, apx, n);
+    {
+      mpn_addsub_n (ash, asmh, gp, hp, n + 1);
+    }
 #else
-  cy = mpn_lshift (apx, a0, n, 1);
-  cy += mpn_add_n (apx, apx, a1, n);
-  cy = 2*cy + mpn_lshift (apx, apx, n, 1);
-  cy += mpn_add_n (apx, apx, a2, n);
-  cy = 2*cy + mpn_lshift (apx, apx, n, 1);
-  apx[n] = cy + mpn_add (apx, apx, n, a3, s);
+  mpn_add_n (ash, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (asmh, hp, gp, n + 1);
+    }
+  else
+    {
+      mpn_sub_n (asmh, gp, hp, n + 1);
+    }
 #endif
 
-  ASSERT (apx[n] < 15);
+  ASSERT (as1[n] <= 3);
+  ASSERT (asm1[n] <= 1);
+  ASSERT (as2[n] <= 14);
+  ASSERT (ash[n] <= 14);
+  ASSERT (asmh[n] <= 9);
+
+#define v0    pp				/* 2n */
+#define v1    (scratch + 6 * n + 6)		/* 2n+1 */
+#define vm1   scratch				/* 2n+1 */
+#define v2    (scratch + 2 * n + 2)		/* 2n+1 */
+#define vinf  (pp + 6 * n)			/* s+t */
+#define vh    (pp + 2 * n)			/* 2n+1 */
+#define vmh   (scratch + 4 * n + 4)
+#define scratch_out  (scratch + 8 * n + 8)
+
+  /* vm1, 2n+1 limbs */
+  TOOM4_SQR_N_REC (vm1, asm1, n + 1, scratch_out);	/* vm1, 2n+1 limbs */
+
+  TOOM4_SQR_N_REC (v2 , as2 , n + 1, scratch_out);	/* v2,  2n+1 limbs */
+
+  TOOM4_SQR_N_REC (vinf, a3 , s,     scratch_out);	/* vinf, 2s limbs */
+
+  TOOM4_SQR_N_REC (v1 , as1 , n + 1, scratch_out);	/* v1,  2n+1 limbs */
 
-  TOOM4_SQR_N_REC (vh, apx, n + 1, tp);	/* vh,  2n+1 limbs */
+  TOOM4_SQR_N_REC (vh , ash , n + 1, scratch_out);
 
-  /* Compute apx = a0 + a1 + a2 + a3 and amx = a0 - a1 + a2 - a3.  */
-  mpn_toom_eval_dgr3_pm1 (apx, amx, ap, n, s, tp);
+  TOOM4_SQR_N_REC (vmh, asmh, n + 1, scratch_out);
 
-  TOOM4_SQR_N_REC (v1, apx, n + 1, tp);	/* v1,  2n+1 limbs */
-  TOOM4_SQR_N_REC (vm1, amx, n + 1, tp);	/* vm1,  2n+1 limbs */
+  TOOM4_SQR_N_REC (v0 , ap  , n    , scratch_out);	/* v0,  2n limbs */
 
-  TOOM4_SQR_N_REC (v0, a0, n, tp);
-  TOOM4_SQR_N_REC (vinf, a3, s, tp);	/* vinf, 2s limbs */
+  mpn_toom_interpolate_7pts (pp, n, 0, vmh, vm1, v1, v2, s + s, scratch_out);
 
-  mpn_toom_interpolate_7pts (pp, n, 0, vm2, vm1, v2, vh, 2*s, tp);
+  TMP_FREE;
 }
--- 1/mpn/generic/toom53_mul.c
+++ 2/mpn/generic/toom53_mul.c
@@ -28,10 +28,17 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1, asm1, bs1, and bsm1 could be
+     avoided by instead reusing the pp area and the scratch allocation.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
-/* Evaluate in: 0, +1, -1, +2, -2, 1/2, +inf
+/* Evaluate in: -1, -1/2, 0, +1/2, +1, +2, +inf
 
   <-s-><--n--><--n--><--n--><--n-->
    ___ ______ ______ ______ ______
@@ -43,8 +50,8 @@
   v1  = (  a0+ a1+ a2+ a3+  a4)*( b0+ b1+ b2) #    A(1)*B(1)      ah  <= 4   bh <= 2
   vm1 = (  a0- a1+ a2- a3+  a4)*( b0- b1+ b2) #   A(-1)*B(-1)    |ah| <= 2   bh <= 1
   v2  = (  a0+2a1+4a2+8a3+16a4)*( b0+2b1+4b2) #    A(2)*B(2)      ah  <= 30  bh <= 6
-  vm2 = (  a0-2a1+4a2-8a3+16a4)*( b0-2b1+4b2) #    A(2)*B(2)     -9<=ah<=20 -1<=bh<=4
   vh  = (16a0+8a1+4a2+2a3+  a4)*(4b0+2b1+ b2) #  A(1/2)*B(1/2)    ah  <= 30  bh <= 6
+  vmh = (16a0-8a1+4a2-2a3+  a4)*(4b0-2b1+ b2) # A(-1/2)*B(-1/2)  -9<=ah<=20 -1<=bh<=4
   vinf=                     a4 *          b2  #  A(inf)*B(inf)
 */
 
@@ -55,11 +62,12 @@
 		mp_ptr scratch)
 {
   mp_size_t n, s, t;
+  int vm1_neg, vmh_neg;
   mp_limb_t cy;
-  mp_ptr gp;
-  mp_ptr as1, asm1, as2, asm2, ash;
-  mp_ptr bs1, bsm1, bs2, bsm2, bsh;
-  enum toom7_flags flags;
+  mp_ptr gp, hp;
+  mp_ptr as1, asm1, as2, ash, asmh;
+  mp_ptr bs1, bsm1, bs2, bsh, bsmh;
+  enum toom4_flags flags;
   TMP_DECL;
 
 #define a0  ap
@@ -84,61 +92,124 @@
   as1  = TMP_SALLOC_LIMBS (n + 1);
   asm1 = TMP_SALLOC_LIMBS (n + 1);
   as2  = TMP_SALLOC_LIMBS (n + 1);
-  asm2 = TMP_SALLOC_LIMBS (n + 1);
   ash  = TMP_SALLOC_LIMBS (n + 1);
+  asmh = TMP_SALLOC_LIMBS (n + 1);
 
   bs1  = TMP_SALLOC_LIMBS (n + 1);
   bsm1 = TMP_SALLOC_LIMBS (n + 1);
   bs2  = TMP_SALLOC_LIMBS (n + 1);
-  bsm2 = TMP_SALLOC_LIMBS (n + 1);
   bsh  = TMP_SALLOC_LIMBS (n + 1);
+  bsmh = TMP_SALLOC_LIMBS (n + 1);
 
   gp = pp;
+  hp = pp + n + 1;
 
   /* Compute as1 and asm1.  */
-  flags = toom7_w3_neg & mpn_toom_eval_pm1 (as1, asm1, 4, ap, n, s, gp);
-
-  /* Compute as2 and asm2. */
-  flags |= toom7_w1_neg & mpn_toom_eval_pm2 (as2, asm2, 4, ap, n, s, gp);
+  gp[n]  = mpn_add_n (gp, a0, a2, n);
+  gp[n] += mpn_add   (gp, gp, n, a4, s);
+  hp[n]  = mpn_add_n (hp, a1, a3, n);
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_addsub_n (as1, asm1, hp, gp, n + 1);
+      vm1_neg = 1;
+    }
+  else
+    {
+      mpn_addsub_n (as1, asm1, gp, hp, n + 1);
+      vm1_neg = 0;
+    }
+#else
+  mpn_add_n (as1, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (asm1, hp, gp, n + 1);
+      vm1_neg = 1;
+    }
+  else
+    {
+      mpn_sub_n (asm1, gp, hp, n + 1);
+      vm1_neg = 0;
+    }
+#endif
 
-  /* Compute ash = 16 a0 + 8 a1 + 4 a2 + 2 a3 + a4
-     = 2*(2*(2*(2*a0 + a1) + a2) + a3) + a4  */
+  /* Compute as2.  */
+#if !HAVE_NATIVE_mpn_addlsh_n
+  ash[n] = mpn_lshift (ash, a2, n, 2);			/*        4a2       */
+#endif
 #if HAVE_NATIVE_mpn_addlsh1_n
-  cy = mpn_addlsh1_n (ash, a1, a0, n);
-  cy = 2*cy + mpn_addlsh1_n (ash, a2, ash, n);
-  cy = 2*cy + mpn_addlsh1_n (ash, a3, ash, n);
-  if (s < n)
-    {
-      mp_limb_t cy2;
-      cy2 = mpn_addlsh1_n (ash, a4, ash, s);
-      ash[n] = 2*cy + mpn_lshift (ash + s, ash + s, n - s, 1);
-      MPN_INCR_U (ash + s, n+1-s, cy2);
+  cy  = mpn_addlsh1_n (as2, a3, a4, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a3 + s, n - s, cy);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a2, as2, n);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a1, as2, n);
+  as2[n] = 2 * cy + mpn_addlsh1_n (as2, a0, as2, n);
+#else
+  cy  = mpn_lshift (as2, a4, s, 1);
+  cy += mpn_add_n (as2, a3, as2, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a3 + s, n - s, cy);
+  cy = 4 * cy + mpn_lshift (as2, as2, n, 2);
+  cy += mpn_add_n (as2, a1, as2, n);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  as2[n] = cy + mpn_add_n (as2, a0, as2, n);
+  mpn_add_n (as2, ash, as2, n + 1);
+#endif
+
+  /* Compute ash and asmh.  */
+#if HAVE_NATIVE_mpn_addlsh_n
+  cy  = mpn_addlsh_n (gp, a2, a0, n, 2);		/* 4a0  +  a2       */
+  cy = 4 * cy + mpn_addlsh_n (gp, a4, gp, n, 2);	/* 16a0 + 4a2 +  a4 */ /* FIXME s */
+  gp[n] = cy;
+  cy  = mpn_addlsh_n (hp, a3, a1, n, 2);		/*  4a1 +  a3       */
+  cy = 2 * cy + mpn_lshift (hp, hp, n, 1);		/*  8a1 + 2a3       */
+  hp[n] = cy;
+#else
+  gp[n] = mpn_lshift (gp, a0, n, 4);			/* 16a0             */
+  mpn_add (gp, gp, n + 1, a4, s);			/* 16a0 +        a4 */
+  mpn_add_n (gp, ash, gp, n+1);				/* 16a0 + 4a2 +  a4 */
+  cy  = mpn_lshift (hp, a1, n, 3);			/*  8a1             */
+  cy += mpn_lshift (ash, a3, n, 1);			/*        2a3       */
+  cy += mpn_add_n (hp, ash, hp, n);			/*  8a1 + 2a3       */
+  hp[n] = cy;
+#endif
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_addsub_n (ash, asmh, hp, gp, n + 1);
+      vmh_neg = 1;
     }
   else
-    ash[n] = 2*cy + mpn_addlsh1_n (ash, a4, ash, n);
+    {
+      mpn_addsub_n (ash, asmh, gp, hp, n + 1);
+      vmh_neg = 0;
+    }
 #else
-  cy = mpn_lshift (ash, a0, n, 1);
-  cy += mpn_add_n (ash, ash, a1, n);
-  cy = 2*cy + mpn_lshift (ash, ash, n, 1);
-  cy += mpn_add_n (ash, ash, a2, n);
-  cy = 2*cy + mpn_lshift (ash, ash, n, 1);
-  cy += mpn_add_n (ash, ash, a3, n);
-  cy = 2*cy + mpn_lshift (ash, ash, n, 1);
-  ash[n] = cy + mpn_add (ash, ash, n, a4, s);
+  mpn_add_n (ash, gp, hp, n + 1);
+  if (mpn_cmp (gp, hp, n + 1) < 0)
+    {
+      mpn_sub_n (asmh, hp, gp, n + 1);
+      vmh_neg = 1;
+    }
+  else
+    {
+      mpn_sub_n (asmh, gp, hp, n + 1);
+      vmh_neg = 0;
+    }
 #endif
 
   /* Compute bs1 and bsm1.  */
   bs1[n] = mpn_add (bs1, b0, n, b2, t);		/* b0 + b2 */
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
   if (bs1[n] == 0 && mpn_cmp (bs1, b1, n) < 0)
     {
-      bs1[n] = mpn_add_n_sub_n (bs1, bsm1, b1, bs1, n) >> 1;
+      bs1[n] = mpn_addsub_n (bs1, bsm1, b1, bs1, n) >> 1;
       bsm1[n] = 0;
-      flags ^= toom7_w3_neg;
+      vm1_neg ^= 1;
     }
   else
     {
-      cy = mpn_add_n_sub_n (bs1, bsm1, bs1, b1, n);
+      cy = mpn_addsub_n (bs1, bsm1, bs1, b1, n);
       bsm1[n] = bs1[n] - (cy & 1);
       bs1[n] += (cy >> 1);
     }
@@ -147,7 +218,7 @@
     {
       mpn_sub_n (bsm1, b1, bs1, n);
       bsm1[n] = 0;
-      flags ^= toom7_w3_neg;
+      vm1_neg ^= 1;
     }
   else
     {
@@ -156,64 +227,46 @@
   bs1[n] += mpn_add_n (bs1, bs1, b1, n);  /* b0+b1+b2 */
 #endif
 
-  /* Compute bs2 and bsm2. */
-#if HAVE_NATIVE_mpn_addlsh_n || HAVE_NATIVE_mpn_addlsh2_n
-#if HAVE_NATIVE_mpn_addlsh2_n
-  cy = mpn_addlsh2_n (bs2, b0, b2, t);
-#else /* HAVE_NATIVE_mpn_addlsh_n */
-  cy = mpn_addlsh_n (bs2, b0, b2, t, 2);
-#endif
-  if (t < n)
-    cy = mpn_add_1 (bs2 + t, b0 + t, n - t, cy);
-  bs2[n] = cy;
+  /* Compute bs2 */
+  hp[n]   = mpn_lshift (hp, b1, n, 1);			/*       2b1       */
+
+#ifdef HAVE_NATIVE_mpn_addlsh1_n
+  cy = mpn_addlsh1_n (bs2, b1, b2, t);
+  if (t != n)
+    cy = mpn_add_1 (bs2 + t, b1 + t, n - t, cy);
+  bs2[n] = 2 * cy + mpn_addlsh1_n (bs2, b0, bs2, n);
 #else
-  cy = mpn_lshift (gp, b2, t, 2);
-  bs2[n] = mpn_add (bs2, b0, n, gp, t);
-  MPN_INCR_U (bs2 + t, n+1-t, cy);
+  bs2[t] = mpn_lshift (bs2, b2, t, 2);
+  mpn_add (bs2, hp, n + 1, bs2, t + 1);
+  bs2[n] += mpn_add_n (bs2, bs2, b0, n);
 #endif
 
-  gp[n] = mpn_lshift (gp, b1, n, 1);
-
-#if HAVE_NATIVE_mpn_add_n_sub_n
-  if (mpn_cmp (bs2, gp, n+1) < 0)
+  /* Compute bsh and bsmh.  */
+#if HAVE_NATIVE_mpn_addlsh_n
+  gp[n] = mpn_addlsh_n (gp, b2, b0, n, 2);		/* 4a0  +       a2 */
+#else
+  cy = mpn_lshift (gp, b0, n, 2);			/* 4b0             */
+  gp[n] = cy + mpn_add (gp, gp, n, b2, t);		/* 4b0 +        b2 */
+#endif
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (gp, hp, n + 1) < 0)
     {
-      ASSERT_NOCARRY (mpn_add_n_sub_n (bs2, bsm2, gp, bs2, n+1));
-      flags ^= toom7_w1_neg;
+      mpn_addsub_n (bsh, bsmh, hp, gp, n + 1);
+      vmh_neg^= 1;
     }
   else
-    {
-      ASSERT_NOCARRY (mpn_add_n_sub_n (bs2, bsm2, bs2, gp, n+1));
-    }
+    mpn_addsub_n (bsh, bsmh, gp, hp, n + 1);
 #else
-  if (mpn_cmp (bs2, gp, n+1) < 0)
+  mpn_add_n (bsh, gp, hp, n + 1);			/* 4b0 + 2b1 +  b2 */
+  if (mpn_cmp (gp, hp, n + 1) < 0)
     {
-      ASSERT_NOCARRY (mpn_sub_n (bsm2, gp, bs2, n+1));
-      flags ^= toom7_w1_neg;
+      mpn_sub_n (bsmh, hp, gp, n + 1);
+      vmh_neg ^= 1;
     }
   else
     {
-      ASSERT_NOCARRY (mpn_sub_n (bsm2, bs2, gp, n+1));
+      mpn_sub_n (bsmh, gp, hp, n + 1);
     }
-  mpn_add_n (bs2, bs2, gp, n+1);
-#endif
-
-  /* Compute bsh = 4 b0 + 2 b1 + b0 = 2*(2*b0 + b1)+b0.  */
-#if HAVE_NATIVE_mpn_addlsh1_n
-  cy = mpn_addlsh1_n (bsh, b1, b0, n);
-  if (t < n)
-    {
-      mp_limb_t cy2;
-      cy2 = mpn_addlsh1_n (bsh, b2, bsh, t);
-      bsh[n] = 2*cy + mpn_lshift (bsh + t, bsh + t, n - t, 1);
-      MPN_INCR_U (bsh + t, n+1-t, cy2);
-    }
-  else
-    bsh[n] = 2*cy + mpn_addlsh1_n (bsh, b2, bsh, n);
-#else
-  cy = mpn_lshift (bsh, b0, n, 1);
-  cy += mpn_add_n (bsh, bsh, b1, n);
-  cy = 2*cy + mpn_lshift (bsh, bsh, n, 1);
-  bsh[n] = cy + mpn_add (bsh, bsh, n, b2, t);
 #endif
 
   ASSERT (as1[n] <= 4);
@@ -222,26 +275,18 @@
   ASSERT (bsm1[n] <= 1);
   ASSERT (as2[n] <= 30);
   ASSERT (bs2[n] <= 6);
-  ASSERT (asm2[n] <= 20);
-  ASSERT (bsm2[n] <= 4);
   ASSERT (ash[n] <= 30);
   ASSERT (bsh[n] <= 6);
+  ASSERT (asmh[n] <= 20);
+  ASSERT (bsmh[n] <= 4);
 
 #define v0    pp				/* 2n */
-#define v1    (pp + 2 * n)			/* 2n+1 */
+#define v1    (scratch + 6 * n + 6)		/* 2n+1 */
+#define vm1   scratch				/* 2n+1 */
+#define v2    (scratch + 2 * n + 2)		/* 2n+1 */
 #define vinf  (pp + 6 * n)			/* s+t */
-#define v2    scratch				/* 2n+1 */
-#define vm2   (scratch + 2 * n + 1)		/* 2n+1 */
-#define vh    (scratch + 4 * n + 2)		/* 2n+1 */
-#define vm1   (scratch + 6 * n + 3)		/* 2n+1 */
-#define scratch_out (scratch + 8 * n + 4)		/* 2n+1 */
-  /* Total scratch need: 10*n+5 */
-
-  /* Must be in allocation order, as they overwrite one limb beyond
-   * 2n+1. */
-  mpn_mul_n (v2, as2, bs2, n + 1);		/* v2, 2n+1 limbs */
-  mpn_mul_n (vm2, asm2, bsm2, n + 1);		/* vm2, 2n+1 limbs */
-  mpn_mul_n (vh, ash, bsh, n + 1);		/* vh, 2n+1 limbs */
+#define vh    (pp + 2 * n)			/* 2n+1 */
+#define vmh   (scratch + 4 * n + 4)
 
   /* vm1, 2n+1 limbs */
 #ifdef SMALLER_RECURSION
@@ -268,6 +313,12 @@
   mpn_mul_n (vm1, asm1, bsm1, n + ((asm1[n] | bsm1[n]) != 0));
 #endif /* SMALLER_RECURSION */
 
+  mpn_mul_n (v2, as2, bs2, n + 1);		/* v2, 2n+1 limbs */
+
+  /* vinf, s+t limbs */
+  if (s > t)  mpn_mul (vinf, a4, s, b2, t);
+  else        mpn_mul (vinf, b2, t, a4, s);
+
   /* v1, 2n+1 limbs */
 #ifdef SMALLER_RECURSION
   mpn_mul_n (v1, as1, bs1, n);
@@ -307,14 +358,16 @@
   mpn_mul_n (v1, as1, bs1, n + ((as1[n] | bs1[n]) != 0));
 #endif /* SMALLER_RECURSION */
 
-  mpn_mul_n (v0, a0, b0, n);			/* v0, 2n limbs */
+  mpn_mul_n (vh, ash, bsh, n + 1);
 
-  /* vinf, s+t limbs */
-  if (s > t)  mpn_mul (vinf, a4, s, b2, t);
-  else        mpn_mul (vinf, b2, t, a4, s);
+  mpn_mul_n (vmh, asmh, bsmh, n + 1);
+
+  mpn_mul_n (v0, ap, bp, n);			/* v0, 2n limbs */
+
+  flags =  vm1_neg ? toom4_w3_neg : 0;
+  flags |= vmh_neg ? toom4_w1_neg : 0;
 
-  mpn_toom_interpolate_7pts (pp, n, flags, vm2, vm1, v2, vh, s + t,
-			     scratch_out);
+  mpn_toom_interpolate_7pts (pp, n, flags, vmh, vm1, v1, v2, s + t, scratch + 8 * n + 8);
 
   TMP_FREE;
 }
--- 1/mpn/generic/toom62_mul.c
+++ 2/mpn/generic/toom62_mul.c
@@ -28,13 +28,20 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
+/*
+  Things to work on:
+
+  1. Trim allocation.  The allocations for as1, asm1, bs1, and bsm1 could be
+     avoided by instead reusing the pp area and the scratch allocation.
+*/
+
 #include "gmp.h"
 #include "gmp-impl.h"
 
-/* Evaluate in:
-   0, +1, -1, +2, -2, 1/2, +inf
 
-  <-s-><--n--><--n--><--n--><--n--><--n-->
+/* Evaluate in: -1, -1/2, 0, +1/2, +1, +2, +inf
+
+  <-s-><--n--><--n--><--n-->
    ___ ______ ______ ______ ______ ______
   |a5_|___a4_|___a3_|___a2_|___a1_|___a0_|
 			     |_b1_|___b0_|
@@ -44,8 +51,8 @@
   v1  = (  a0+  a1+ a2+ a3+  a4+  a5)*( b0+ b1) #    A(1)*B(1)      ah  <= 5   bh <= 1
   vm1 = (  a0-  a1+ a2- a3+  a4-  a5)*( b0- b1) #   A(-1)*B(-1)    |ah| <= 2   bh  = 0
   v2  = (  a0+ 2a1+4a2+8a3+16a4+32a5)*( b0+2b1) #    A(2)*B(2)      ah  <= 62  bh <= 2
-  vm2 = (  a0- 2a1+4a2-8a3+16a4-32a5)*( b0-2b1) #   A(-2)*B(-2)    -41<=ah<=20 -1<=bh<=0
   vh  = (32a0+16a1+8a2+4a3+ 2a4+  a5)*(2b0+ b1) #  A(1/2)*B(1/2)    ah  <= 62  bh <= 2
+  vmh = (32a0-16a1+8a2-4a3+ 2a4-  a5)*(2b0- b1) # A(-1/2)*B(-1/2)  -20<=ah<=41 0<=bh<=1
   vinf=                           a5 *      b1  #  A(inf)*B(inf)
 */
 
@@ -56,11 +63,12 @@
 		mp_ptr scratch)
 {
   mp_size_t n, s, t;
+  int vm1_neg, vmh_neg, bsm_neg;
   mp_limb_t cy;
-  mp_ptr as1, asm1, as2, asm2, ash;
-  mp_ptr bs1, bsm1, bs2, bsm2, bsh;
-  mp_ptr gp;
-  enum toom7_flags aflags, bflags;
+  mp_ptr a0_a2, a1_a3;
+  mp_ptr as1, asm1, as2, ash, asmh;
+  mp_ptr bs1, bsm1, bs2, bsh, bsmh;
+  enum toom4_flags flags;
   TMP_DECL;
 
 #define a0  ap
@@ -72,7 +80,7 @@
 #define b0  bp
 #define b1  (bp + n)
 
-  n = 1 + (an >= 3 * bn ? (an - 1) / (size_t) 6 : (bn - 1) >> 1);
+  n = 1 + (an >= 3 * bn ? (an - 1) / (unsigned long) 6 : (bn - 1) >> 1);
 
   s = an - 5 * n;
   t = bn - n;
@@ -85,66 +93,133 @@
   as1 = TMP_SALLOC_LIMBS (n + 1);
   asm1 = TMP_SALLOC_LIMBS (n + 1);
   as2 = TMP_SALLOC_LIMBS (n + 1);
-  asm2 = TMP_SALLOC_LIMBS (n + 1);
   ash = TMP_SALLOC_LIMBS (n + 1);
+  asmh = TMP_SALLOC_LIMBS (n + 1);
 
   bs1 = TMP_SALLOC_LIMBS (n + 1);
   bsm1 = TMP_SALLOC_LIMBS (n);
   bs2 = TMP_SALLOC_LIMBS (n + 1);
-  bsm2 = TMP_SALLOC_LIMBS (n + 1);
   bsh = TMP_SALLOC_LIMBS (n + 1);
+  bsmh = TMP_SALLOC_LIMBS (n + 1);
 
-  gp = pp;
+  a0_a2 = pp;
+  a1_a3 = pp + n + 1;
 
   /* Compute as1 and asm1.  */
-  aflags = toom7_w3_neg & mpn_toom_eval_pm1 (as1, asm1, 5, ap, n, s, gp);
-
-  /* Compute as2 and asm2. */
-  aflags |= toom7_w1_neg & mpn_toom_eval_pm2 (as2, asm2, 5, ap, n, s, gp);
-
-  /* Compute ash = 32 a0 + 16 a1 + 8 a2 + 4 a3 + 2 a4 + a5
-     = 2*(2*(2*(2*(2*a0 + a1) + a2) + a3) + a4) + a5  */
+  a0_a2[n]  = mpn_add_n (a0_a2, a0, a2, n);
+  a0_a2[n] += mpn_add_n (a0_a2, a0_a2, a4, n);
+  a1_a3[n]  = mpn_add_n (a1_a3, a1, a3, n);
+  a1_a3[n] += mpn_add (a1_a3, a1_a3, n, a5, s);
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (a0_a2, a1_a3, n + 1) < 0)
+    {
+      mpn_addsub_n (as1, asm1, a1_a3, a0_a2, n + 1);
+      vm1_neg = 1;
+    }
+  else
+    {
+      mpn_addsub_n (as1, asm1, a0_a2, a1_a3, n + 1);
+      vm1_neg = 0;
+    }
+#else
+  mpn_add_n (as1, a0_a2, a1_a3, n + 1);
+  if (mpn_cmp (a0_a2, a1_a3, n + 1) < 0)
+    {
+      mpn_sub_n (asm1, a1_a3, a0_a2, n + 1);
+      vm1_neg = 1;
+    }
+  else
+    {
+      mpn_sub_n (asm1, a0_a2, a1_a3, n + 1);
+      vm1_neg = 0;
+    }
+#endif
 
+  /* Compute as2.  */
 #if HAVE_NATIVE_mpn_addlsh1_n
-  cy = mpn_addlsh1_n (ash, a1, a0, n);
-  cy = 2*cy + mpn_addlsh1_n (ash, a2, ash, n);
-  cy = 2*cy + mpn_addlsh1_n (ash, a3, ash, n);
-  cy = 2*cy + mpn_addlsh1_n (ash, a4, ash, n);
-  if (s < n)
-    {
-      mp_limb_t cy2;
-      cy2 = mpn_addlsh1_n (ash, a5, ash, s);
-      ash[n] = 2*cy + mpn_lshift (ash + s, ash + s, n - s, 1);
-      MPN_INCR_U (ash + s, n+1-s, cy2);
+  cy  = mpn_addlsh1_n (as2, a4, a5, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a4 + s, n - s, cy);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a3, as2, n);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a2, as2, n);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a1, as2, n);
+  cy = 2 * cy + mpn_addlsh1_n (as2, a0, as2, n);
+#else
+  cy  = mpn_lshift (as2, a5, s, 1);
+  cy += mpn_add_n (as2, a4, as2, s);
+  if (s != n)
+    cy = mpn_add_1 (as2 + s, a4 + s, n - s, cy);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a3, as2, n);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a2, as2, n);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a1, as2, n);
+  cy = 2 * cy + mpn_lshift (as2, as2, n, 1);
+  cy += mpn_add_n (as2, a0, as2, n);
+#endif
+  as2[n] = cy;
+
+  /* Compute ash and asmh.  */
+#if HAVE_NATIVE_mpn_addlsh_n
+  cy  = mpn_addlsh_n (a0_a2, a2, a0, n, 2);		/* 4a0  +  a2       */
+  cy = 4 * cy + mpn_addlsh_n (a0_a2, a4, a0_a2, n, 2);	/* 16a0 + 4a2 +  a4 */
+  cy = 2 * cy + mpn_lshift (a0_a2, a0_a2, n, 1);	/* 32a0 + 8a2 + 2a4 */
+  a0_a2[n] = cy;
+  cy  = mpn_addlsh_n (a1_a3, a3, a1, n, 2);		/* 4a1              */
+  cy = 4 * cy + mpn_addlsh_n (a1_a3, a5, a1_a3, n, 2);	/* 16a1 + 4a3       */
+  a1_a3[n] = cy;
+#else
+  cy  = mpn_lshift (a0_a2, a0, n, 2);			/* 4a0              */
+  cy += mpn_add_n (a0_a2, a2, a0_a2, n);		/* 4a0  +  a2       */
+  cy = 4 * cy + mpn_lshift (a0_a2, a0_a2, n, 2);	/* 16a0 + 4a2       */
+  cy += mpn_add_n (a0_a2, a4, a0_a2, n);		/* 16a0 + 4a2 +  a4 */
+  cy = 2 * cy + mpn_lshift (a0_a2, a0_a2, n, 1);	/* 32a0 + 8a2 + 2a4 */
+  a0_a2[n] = cy;
+  cy  = mpn_lshift (a1_a3, a1, n, 2);			/* 4a1              */
+  cy += mpn_add_n (a1_a3, a3, a1_a3, n);		/* 4a1  +  a3       */
+  cy = 4 * cy + mpn_lshift (a1_a3, a1_a3, n, 2);	/* 16a1 + 4a3       */
+  cy += mpn_add (a1_a3, a1_a3, n, a5, s);		/* 16a1 + 4a3 + a5  */
+  a1_a3[n] = cy;
+#endif
+#if HAVE_NATIVE_mpn_addsub_n
+  if (mpn_cmp (a0_a2, a1_a3, n + 1) < 0)
+    {
+      mpn_addsub_n (ash, asmh, a1_a3, a0_a2, n + 1);
+      vmh_neg = 1;
     }
   else
-    ash[n] = 2*cy + mpn_addlsh1_n (ash, a5, ash, n);
+    {
+      mpn_addsub_n (ash, asmh, a0_a2, a1_a3, n + 1);
+      vmh_neg = 0;
+    }
 #else
-  cy = mpn_lshift (ash, a0, n, 1);
-  cy += mpn_add_n (ash, ash, a1, n);
-  cy = 2*cy + mpn_lshift (ash, ash, n, 1);
-  cy += mpn_add_n (ash, ash, a2, n);
-  cy = 2*cy + mpn_lshift (ash, ash, n, 1);
-  cy += mpn_add_n (ash, ash, a3, n);
-  cy = 2*cy + mpn_lshift (ash, ash, n, 1);
-  cy += mpn_add_n (ash, ash, a4, n);
-  cy = 2*cy + mpn_lshift (ash, ash, n, 1);
-  ash[n] = cy + mpn_add (ash, ash, n, a5, s);
+  mpn_add_n (ash, a0_a2, a1_a3, n + 1);
+  if (mpn_cmp (a0_a2, a1_a3, n + 1) < 0)
+    {
+      mpn_sub_n (asmh, a1_a3, a0_a2, n + 1);
+      vmh_neg = 1;
+    }
+  else
+    {
+      mpn_sub_n (asmh, a0_a2, a1_a3, n + 1);
+      vmh_neg = 0;
+    }
 #endif
 
   /* Compute bs1 and bsm1.  */
   if (t == n)
     {
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
       if (mpn_cmp (b0, b1, n) < 0)
 	{
-	  cy = mpn_add_n_sub_n (bs1, bsm1, b1, b0, n);
-	  bflags = toom7_w3_neg;
+	  cy = mpn_addsub_n (bs1, bsm1, b1, b0, n);
+	  bsm_neg = 1;
 	}
       else
 	{
-	  cy = mpn_add_n_sub_n (bs1, bsm1, b0, b1, n);
-	  bflags = 0;
+	  cy = mpn_addsub_n (bs1, bsm1, b0, b1, n);
+	  bsm_neg = 0;
 	}
       bs1[n] = cy >> 1;
 #else
@@ -152,12 +227,12 @@
       if (mpn_cmp (b0, b1, n) < 0)
 	{
 	  mpn_sub_n (bsm1, b1, b0, n);
-	  bflags = toom7_w3_neg;
+	  bsm_neg = 1;
 	}
       else
 	{
 	  mpn_sub_n (bsm1, b0, b1, n);
-	  bflags = 0;
+	  bsm_neg = 0;
 	}
 #endif
     }
@@ -168,83 +243,56 @@
 	{
 	  mpn_sub_n (bsm1, b1, b0, t);
 	  MPN_ZERO (bsm1 + t, n - t);
-	  bflags = toom7_w3_neg;
+	  bsm_neg = 1;
 	}
       else
 	{
 	  mpn_sub (bsm1, b0, n, b1, t);
-	  bflags = 0;
+	  bsm_neg = 0;
 	}
     }
 
-  /* Compute bs2 and bsm2. Recycling bs1 and bsm1; bs2=bs1+b1, bsm2 =
-     bsm1 - b1 */
+  vm1_neg ^= bsm_neg;
+
+  /* Compute bs2, recycling bs1. bs2=bs1+b1  */
   mpn_add (bs2, bs1, n + 1, b1, t);
-  if (bflags & toom7_w3_neg)
-    {
-      bsm2[n] = mpn_add (bsm2, bsm1, n, b1, t);
-      bflags |= toom7_w1_neg;
-    }
-  else
+
+  /* Compute bsh and bsmh, recycling bs1 and bsm1. bsh=bs1+b0; bsmh=bsmh+b0  */
+  if (bsm_neg == 1)
     {
-      /* FIXME: Simplify this logic? */
-      if (t < n)
+      bsmh[n] = 0;
+      if (mpn_cmp (bsm1, b0, n) < 0)
 	{
-	  if (mpn_zero_p (bsm1 + t, n - t) && mpn_cmp (bsm1, b1, t) < 0)
-	    {
-	      ASSERT_NOCARRY (mpn_sub_n (bsm2, b1, bsm1, t));
-	      MPN_ZERO (bsm2 + t, n + 1 - t);
-	      bflags |= toom7_w1_neg;
-	    }
-	  else
-	    {
-	      ASSERT_NOCARRY (mpn_sub (bsm2, bsm1, n, b1, t));
-	      bsm2[n] = 0;
-	    }
+	  bsm_neg = 0;
+	  mpn_sub_n (bsmh, b0, bsm1, n);
 	}
       else
-	{
-	  if (mpn_cmp (bsm1, b1, n) < 0)
-	    {
-	      ASSERT_NOCARRY (mpn_sub_n (bsm2, b1, bsm1, n));
-	      bflags |= toom7_w1_neg;
-	    }
-	  else
-	    {
-	      ASSERT_NOCARRY (mpn_sub (bsm2, bsm1, n, b1, n));
-	    }
-	  bsm2[n] = 0;
-	}
+	mpn_sub_n (bsmh, bsm1, b0, n);
     }
-
-  /* Compute bsh, recycling bs1 and bsm1. bsh=bs1+b0;  */
+  else
+    bsmh[n] = mpn_add_n (bsmh, bsm1, b0, n);
   mpn_add (bsh, bs1, n + 1, b0, n);
+  vmh_neg ^= bsm_neg;
+
 
   ASSERT (as1[n] <= 5);
   ASSERT (bs1[n] <= 1);
   ASSERT (asm1[n] <= 2);
+/*ASSERT (bsm1[n] == 0);*/
   ASSERT (as2[n] <= 62);
   ASSERT (bs2[n] <= 2);
-  ASSERT (asm2[n] <= 41);
-  ASSERT (bsm2[n] <= 1);
   ASSERT (ash[n] <= 62);
   ASSERT (bsh[n] <= 2);
+  ASSERT (asmh[n] <= 41);
+  ASSERT (bsmh[n] <= 1);
 
 #define v0    pp				/* 2n */
-#define v1    (pp + 2 * n)			/* 2n+1 */
+#define v1    (scratch + 6 * n + 6)		/* 2n+1 */
 #define vinf  (pp + 6 * n)			/* s+t */
-#define v2    scratch				/* 2n+1 */
-#define vm2   (scratch + 2 * n + 1)		/* 2n+1 */
-#define vh    (scratch + 4 * n + 2)		/* 2n+1 */
-#define vm1   (scratch + 6 * n + 3)		/* 2n+1 */
-#define scratch_out (scratch + 8 * n + 4)		/* 2n+1 */
-  /* Total scratch need: 10*n+5 */
-
-  /* Must be in allocation order, as they overwrite one limb beyond
-   * 2n+1. */
-  mpn_mul_n (v2, as2, bs2, n + 1);		/* v2, 2n+1 limbs */
-  mpn_mul_n (vm2, asm2, bsm2, n + 1);		/* vm2, 2n+1 limbs */
-  mpn_mul_n (vh, ash, bsh, n + 1);		/* vh, 2n+1 limbs */
+#define vm1   scratch				/* 2n+1 */
+#define v2    (scratch + 2 * n + 2)		/* 2n+1 */
+#define vh    (pp + 2 * n)			/* 2n+1 */
+#define vmh   (scratch + 4 * n + 4)
 
   /* vm1, 2n+1 limbs */
   mpn_mul_n (vm1, asm1, bsm1, n);
@@ -263,6 +311,12 @@
     }
   vm1[2 * n] = cy;
 
+  mpn_mul_n (v2, as2, bs2, n + 1);		/* v2, 2n+1 limbs */
+
+  /* vinf, s+t limbs */
+  if (s > t)  mpn_mul (vinf, a5, s, b1, t);
+  else        mpn_mul (vinf, b1, t, a5, s);
+
   /* v1, 2n+1 limbs */
   mpn_mul_n (v1, as1, bs1, n);
   if (as1[n] == 1)
@@ -287,14 +341,16 @@
     cy += mpn_add_n (v1 + n, v1 + n, as1, n);
   v1[2 * n] = cy;
 
-  mpn_mul_n (v0, a0, b0, n);			/* v0, 2n limbs */
+  mpn_mul_n (vh, ash, bsh, n + 1);
 
-  /* vinf, s+t limbs */
-  if (s > t)  mpn_mul (vinf, a5, s, b1, t);
-  else        mpn_mul (vinf, b1, t, a5, s);
+  mpn_mul_n (vmh, asmh, bsmh, n + 1);
+
+  mpn_mul_n (v0, ap, bp, n);			/* v0, 2n limbs */
+
+  flags =  vm1_neg ? toom4_w3_neg : 0;
+  flags |= vmh_neg ? toom4_w1_neg : 0;
 
-  mpn_toom_interpolate_7pts (pp, n, aflags ^ bflags,
-			     vm2, vm1, v2, vh, s + t, scratch_out);
+  mpn_toom_interpolate_7pts (pp, n, flags, vmh, vm1, v1, v2, s + t, scratch + 8 * n + 8);
 
   TMP_FREE;
 }
--- 1/mpn/generic/toom_interpolate_5pts.c
+++ 2/mpn/generic/toom_interpolate_5pts.c
@@ -7,8 +7,8 @@
    SAFE TO REACH IT THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
    GUARANTEED THAT IT WILL CHANGE OR DISAPPEAR IN A FUTURE GNU MP RELEASE.
 
-Copyright 2000, 2001, 2002, 2003, 2005, 2006, 2007, 2009 Free Software
-Foundation, Inc.
+Copyright 2000, 2001, 2002, 2003, 2005, 2006, 2007 Free Software Foundation,
+Inc.
 
 This file is part of the GNU MP Library.
 
@@ -31,29 +31,28 @@
 void
 mpn_toom_interpolate_5pts (mp_ptr c, mp_ptr v2, mp_ptr vm1,
 			   mp_size_t k, mp_size_t twor, int sa,
-			   mp_limb_t vinf0)
+			   mp_limb_t vinf0, mp_ptr ws)
 {
   mp_limb_t cy, saved;
-  mp_size_t twok;
-  mp_size_t kk1;
-  mp_ptr c1, v1, c3, vinf;
-
-  twok = k + k;
-  kk1 = twok + 1;
+  mp_size_t twok = k + k;
+  mp_size_t kk1 = twok + 1;
+  mp_ptr c1, v1, c3, vinf, c5;
+  mp_limb_t cout; /* final carry, should be zero at the end */
 
   c1 = c  + k;
   v1 = c1 + k;
   c3 = v1 + k;
   vinf = c3 + k;
+  c5 = vinf + k;
 
 #define v0 (c)
   /* (1) v2 <- v2-vm1 < v2+|vm1|,       (16 8 4 2 1) - (1 -1 1 -1  1) =
      thus 0 <= v2 < 50*B^(2k) < 2^6*B^(2k)             (15 9 3  3  0)
   */
-  if (sa)
-    ASSERT_NOCARRY (mpn_add_n (v2, v2, vm1, kk1));
+  if (sa <= 0)
+    mpn_add_n (v2, v2, vm1, kk1);
   else
-    ASSERT_NOCARRY (mpn_sub_n (v2, v2, vm1, kk1));
+    mpn_sub_n (v2, v2, vm1, kk1);
 
   /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
        v0       v1       hi(vinf)       |vm1|     v2-vm1      EMPTY */
@@ -64,18 +63,17 @@
   /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
        v0       v1      hi(vinf)       |vm1|     (v2-vm1)/3    EMPTY */
 
-  /* (2) vm1 <- tm1 := (v1 - vm1) / 2  [(1 1 1 1 1) - (1 -1 1 -1 1)] / 2 =
-     tm1 >= 0                                         (0  1 0  1 0)
+  /* (2) vm1 <- tm1 := (v1 - sa*vm1) / 2  [(1 1 1 1 1) - (1 -1 1 -1 1)] / 2 =
+     tm1 >= 0                                            (0  1 0  1 0)
      No carry comes out from {v1, kk1} +/- {vm1, kk1},
-     and the division by two is exact.
-     If (sa!=0) the sign of vm1 is negative */
-  if (sa)
+     and the division by two is exact */
+  if (sa <= 0)
     {
 #ifdef HAVE_NATIVE_mpn_rsh1add_n
       mpn_rsh1add_n (vm1, v1, vm1, kk1);
 #else
-      ASSERT_NOCARRY (mpn_add_n (vm1, v1, vm1, kk1));
-      ASSERT_NOCARRY (mpn_rshift (vm1, vm1, kk1, 1));
+      mpn_add_n (vm1, v1, vm1, kk1);
+      mpn_rshift (vm1, vm1, kk1, 1);
 #endif
     }
   else
@@ -83,8 +81,8 @@
 #ifdef HAVE_NATIVE_mpn_rsh1sub_n
       mpn_rsh1sub_n (vm1, v1, vm1, kk1);
 #else
-      ASSERT_NOCARRY (mpn_sub_n (vm1, v1, vm1, kk1));
-      ASSERT_NOCARRY (mpn_rshift (vm1, vm1, kk1, 1));
+      mpn_sub_n (vm1, v1, vm1, kk1);
+      mpn_rshift (vm1, vm1, kk1, 1);
 #endif
     }
 
@@ -105,8 +103,8 @@
 #ifdef HAVE_NATIVE_mpn_rsh1sub_n
   mpn_rsh1sub_n (v2, v2, v1, kk1);
 #else
-  ASSERT_NOCARRY (mpn_sub_n (v2, v2, v1, kk1));
-  ASSERT_NOCARRY (mpn_rshift (v2, v2, kk1, 1));
+  mpn_sub_n (v2, v2, v1, kk1);
+  mpn_rshift (v2, v2, kk1, 1);
 #endif
 
   /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
@@ -115,12 +113,10 @@
   /* (5) v1 <- t1-tm1           (1 1 1 1 0) - (0 1 0 1 0) = (1 0 1 0 0)
      result is v1 >= 0
   */
-  ASSERT_NOCARRY (mpn_sub_n (v1, v1, vm1, kk1));
+  mpn_sub_n (v1, v1, vm1, kk1);
 
-  /* We do not need to read the value in vm1, so we add it in {c+k, ...} */
-  cy = mpn_add_n (c1, c1, vm1, kk1);
-  MPN_INCR_U (c3 + 1, twor + k - 1, cy); /* 2n-(3k+1) = 2r+k-1 */
-  /* Memory allocated for vm1 is now free, it can be recycled ...*/
+  /* {c,2k} {c+2k,2k+1} {c+4k+1,2r-1} {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+       v0   v1-v0-tm1      hi(vinf)     tm1    (v2-vm1-3t1)/6    EMPTY */
 
   /* (6) v2 <- v2 - 2*vinf,     (2 1 0 0 0) - 2*(1 0 0 0 0) = (0 1 0 0 0)
      result is v2 >= 0 */
@@ -129,61 +125,46 @@
 #ifdef HAVE_NATIVE_mpn_sublsh1_n
   cy = mpn_sublsh1_n (v2, v2, vinf, twor);
 #else
-  /* Overwrite unused vm1 */
-  cy = mpn_lshift (vm1, vinf, twor, 1);
-  cy += mpn_sub_n (v2, v2, vm1, twor);
+  cy = mpn_lshift (ws, vinf, twor, 1);
+  cy += mpn_sub_n (v2, v2, ws, twor);
 #endif
   MPN_DECR_U (v2 + twor, kk1 - twor, cy);
 
-  /* Current matrix is
-     [1 0 0 0 0; vinf
-      0 1 0 0 0; v2
-      1 0 1 0 0; v1
-      0 1 0 1 0; vm1
-      0 0 0 0 1] v0
-     Some vaues already are in-place (we added vm1 in the correct position)
-     | vinf|  v1 |  v0 |
-	      | vm1 |
-     One still is in a separated area
-	| +v2 |
-     We have to compute v1-=vinf; vm1 -= v2,
-	   |-vinf|
-	      | -v2 |
-     Carefully reordering operations we can avoid to compute twice the sum
-     of the high half of v2 plus the low half of vinf.
-  */
-
-  /* Add the high half of t2 in {vinf} */
-  if ( LIKELY(twor > k + 1) ) { /* This is the expected flow  */
-    cy = mpn_add_n (vinf, vinf, v2 + k, k + 1);
-    MPN_INCR_U (c3 + kk1, twor - k - 1, cy); /* 2n-(5k+1) = 2r-k-1 */
-  } else { /* triggered only by very unbalanced cases like
-	      (k+k+(k-2))x(k+k+1) , should be handled by toom32 */
-    ASSERT_NOCARRY (mpn_add_n (vinf, vinf, v2 + k, twor));
-  }
   /* (7) v1 <- v1 - vinf,       (1 0 1 0 0) - (1 0 0 0 0) = (0 0 1 0 0)
      result is >= 0 */
-  /* Side effect: we also subtracted (high half) vm1 -= v2 */
   cy = mpn_sub_n (v1, v1, vinf, twor);          /* vinf is at most twor long.  */
-  vinf0 = vinf[0];                     /* Save again the right value for vinf0 */
   vinf[0] = saved;
   MPN_DECR_U (v1 + twor, kk1 - twor, cy);       /* Treat the last bytes.       */
+  __GMPN_ADD_1 (cout, vinf, vinf, twor, vinf0); /* Add vinf0, propagate carry. */
 
-  /* (8) vm1 <- vm1-v2          (0 1 0 1 0) - (0 1 0 0 0) = (0 0 0 1 0)
-     Operate only on the low half.
+  /* (8) vm1 <- vm1-t2          (0 1 0 1 0) - (0 1 0 0 0) = (0 0 0 1 0)
+     vm1 >= 0
   */
-  cy = mpn_sub_n (c1, c1, v2, k);
-  MPN_DECR_U (v1, kk1, cy);
+  mpn_sub_n (vm1, vm1, v2, kk1);            /* No overlapping here.        */
 
   /********************* Beginning the final phase **********************/
 
-  /* Most of the recomposition was done */
+  /* {c,2k} {c+2k,2k  } {c+4k ,2r } {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+       v0       t1      hi(t1)+vinf   tm1    (v2-vm1-3t1)/6    EMPTY */
+
+  /* (9) add t2 in {c+3k, ...} */
+  cy = mpn_add_n (c3, c3, v2, kk1);
+  __GMPN_ADD_1 (cout, c5 + 1, c5 + 1, twor - k - 1, cy); /* 2n-(5k+1) = 2r-k-1 */
+
+  /* {c,2k} {c+2k,2k  } {c+4k ,2r } {t,2k+1} {t+2k+1,2k+1} {t+4k+2,2r}
+       v0       t1      hi(t1)+vinf   tm1    (v2-vm1-3t1)/6    EMPTY */
+  /* c   c+k  c+2k  c+3k  c+4k      t   t+2k+1  t+4k+2
+     v0       t1         vinf      tm1  t2
+		    +t2 */
+
+  /* add vm1 in {c+k, ...} */
+  cy = mpn_add_n (c1, c1, vm1, kk1);
+  __GMPN_ADD_1 (cout, c3 + 1, c3 + 1, twor + k - 1, cy); /* 2n-(3k+1) = 2r+k-1 */
 
-  /* add t2 in {c+3k, ...}, but only the low half */
-  cy = mpn_add_n (c3, c3, v2, k);
-  vinf[0] += cy;
-  ASSERT(vinf[0] >= cy); /* No carry */
-  MPN_INCR_U (vinf, twor, vinf0); /* Add vinf0, propagate carry. */
+  /* c   c+k  c+2k  c+3k  c+4k      t   t+2k+1  t+4k+2
+     v0       t1         vinf      tm1  t2
+	  +tm1      +t2    */
 
 #undef v0
+#undef t2
 }
--- 1/mpn/generic/toom_interpolate_7pts.c
+++ 2/mpn/generic/toom_interpolate_7pts.c
@@ -1,7 +1,6 @@
 /* mpn_toom_interpolate_7pts -- Interpolate for toom44, 53, 62.
 
    Contributed to the GNU project by Niels Möller.
-   Improvements by Marco Bodrato.
 
    THE FUNCTION IN THIS FILE IS INTERNAL WITH A MUTABLE INTERFACE.  IT IS ONLY
    SAFE TO REACH IT THROUGH DOCUMENTED INTERFACES.  IN FACT, IT IS ALMOST
@@ -27,190 +26,134 @@
 #include "gmp.h"
 #include "gmp-impl.h"
 
-#define BINVERT_3 MODLIMB_INVERSE_3
-
-#define BINVERT_9 \
-  ((((GMP_NUMB_MAX / 9) << (6 - GMP_NUMB_BITS % 6)) * 8 & GMP_NUMB_MAX) | 0x39)
-
-#define BINVERT_15 \
-  ((((GMP_NUMB_MAX >> (GMP_NUMB_BITS % 4)) / 15) * 14 * 16 & GMP_NUMB_MAX) + 15))
-
-/* For the various mpn_divexact_byN here, fall back to using either
-   mpn_bdiv_q_1_pi1 or mpn_divexact_1.  The former has less overhead and is
-   many faster if it is native.  For now, since mpn_divexact_1 is native on
-   several platforms where mpn_bdiv_q_1_pi1 does not yet exist, do not use
-   mpn_bdiv_q_1_pi1 unconditionally.  FIXME.  */
+/* Arithmetic right shift, requiring that the shifted out bits are zero. */
+static inline void
+divexact_2exp (mp_ptr rp, mp_srcptr sp, mp_size_t n, unsigned shift)
+{
+  mp_limb_t sign;
+  sign = LIMB_HIGHBIT_TO_MASK (sp[n-1] << GMP_NAIL_BITS) << (GMP_NUMB_BITS - shift);
+  ASSERT_NOCARRY (mpn_rshift (rp, sp, n, shift));
+  rp[n-1] |= sign & GMP_NUMB_MASK;
+}
 
 /* For odd divisors, mpn_divexact_1 works fine with two's complement. */
 #ifndef mpn_divexact_by3
-#if HAVE_NATIVE_mpn_bdiv_q_1_pi1
-#define mpn_divexact_by3(dst,src,size) mpn_bdiv_q_1_pi1(dst,src,size,3,BINVERT_3,0)
-#else
 #define mpn_divexact_by3(dst,src,size) mpn_divexact_1(dst,src,size,3)
 #endif
-#endif
-
 #ifndef mpn_divexact_by9
-#if HAVE_NATIVE_mpn_bdiv_q_1_pi1
-#define mpn_divexact_by9(dst,src,size) mpn_bdiv_q_1_pi1(dst,src,size,9,BINVERT_9,0)
-#else
 #define mpn_divexact_by9(dst,src,size) mpn_divexact_1(dst,src,size,9)
 #endif
-#endif
-
 #ifndef mpn_divexact_by15
-#if HAVE_NATIVE_mpn_bdiv_q_1_pi1
-#define mpn_divexact_by15(dst,src,size) mpn_bdiv_q_1_pi1(dst,src,size,15,BINVERT_15,0)
-#else
 #define mpn_divexact_by15(dst,src,size) mpn_divexact_1(dst,src,size,15)
 #endif
-#endif
 
-/* Interpolation for toom4, using the evaluation points 0, infinity,
-   1, -1, 2, -2, 1/2. More precisely, we want to compute
+/* Interpolation for toom4, using the evaluation points infinity, 2,
+   1, -1, 1/2, -1/2. More precisely, we want to compute
    f(2^(GMP_NUMB_BITS * n)) for a polynomial f of degree 6, given the
    seven values
 
      w0 = f(0),
-     w1 = f(-2),
-     w2 = f(1),
+     w1 = 64 f(-1/2),
+     w2 = 64 f(1/2),
      w3 = f(-1),
-     w4 = f(2)
-     w5 = 64 * f(1/2)
+     w4 = f(1)
+     w5 = f(2)
      w6 = limit at infinity of f(x) / x^6,
 
    The result is 6*n + w6n limbs. At entry, w0 is stored at {rp, 2n },
    w2 is stored at { rp + 2n, 2n+1 }, and w6 is stored at { rp + 6n,
    w6n }. The other values are 2n + 1 limbs each (with most
    significant limbs small). f(-1) and f(-1/2) may be negative, signs
-   determined by the flag bits. Inputs are destroyed.
+   determined by the flag bits. All intermediate results are
+   represented in two's complement. Inputs are destroyed.
 
    Needs (2*n + 1) limbs of temporary storage.
 */
 
 void
-mpn_toom_interpolate_7pts (mp_ptr rp, mp_size_t n, enum toom7_flags flags,
+mpn_toom_interpolate_7pts (mp_ptr rp, mp_size_t n, enum toom4_flags flags,
 			   mp_ptr w1, mp_ptr w3, mp_ptr w4, mp_ptr w5,
 			   mp_size_t w6n, mp_ptr tp)
 {
-  mp_size_t m;
+  mp_size_t m = 2*n + 1;
+  mp_ptr w2 = rp + 2*n;
+  mp_ptr w6 = rp + 6*n;
   mp_limb_t cy;
 
-  m = 2*n + 1;
-#define w0 rp
-#define w2 (rp + 2*n)
-#define w6 (rp + 6*n)
-
   ASSERT (w6n > 0);
   ASSERT (w6n <= 2*n);
 
-  /* Using formulas similar to Marco Bodrato's
+  /* Using Marco Bodrato's formulas
 
-     W5 = W5 + W4
-     W1 =(W4 - W1)/2
-     W4 = W4 - W0
-     W4 =(W4 - W1)/4 - W6*16
-     W3 =(W2 - W3)/2
-     W2 = W2 - W3
-
-     W5 = W5 - W2*65      May be negative.
-     W2 = W2 - W6 - W0
-     W5 =(W5 + W2*45)/2   Now >= 0 again.
-     W4 =(W4 - W2)/3
-     W2 = W2 - W4
+     W5 = W5 + W2
+     W3 =(W3 + W4)/2
+     W1 = W1 + W2
+     W2 = W2 - W6 - W0*64
+     W2 =(W2*2 - W1)/8
+     W4 = W4 - W3
+
+     W5 = W5 - W4*65
+     W4 = W4 - W6 - W0
+     W5 = W5 + W4*45
+     W2 =(W2 - W4)/3
+     W4 = W4 - W2
 
-     W1 = W5 - W1         May be negative.
-     W5 =(W5 - W3*8)/9
+     W1 = W1 - W5
+     W5 =(W5 - W3*16)/ 18
      W3 = W3 - W5
-     W1 =(W1/15 + W5)/2   Now >= 0 again.
+     W1 =(W1/30 + W5)/ 2
      W5 = W5 - W1
 
-     where W0 = f(0), W1 = f(-2), W2 = f(1), W3 = f(-1),
-	   W4 = f(2), W5 = f(1/2), W6 = f(oo),
-
-     Note that most intermediate results are positive; the ones that
-     may be negative are represented in two's complement. We must
-     never shift right a value that may be negative, since that would
-     invalidate the sign bit. On the other hand, divexact by odd
-     numbers work fine with two's complement.
+     where W0 = f(0), W1 = 64 f(-1/2), W2 = 64 f(1/2), W3 = f(-1),
+	   W4 = f(1), W5 = f(2), W6 = f(oo),
   */
 
-  mpn_add_n (w5, w5, w4, m);
-  if (flags & toom7_w1_neg)
-    {
-#ifdef HAVE_NATIVE_mpn_rsh1add_n
-      mpn_rsh1add_n (w1, w1, w4, m);
-#else
-      mpn_add_n (w1, w1, w4, m);  ASSERT (!(w1[0] & 1));
-      mpn_rshift (w1, w1, m, 1);
-#endif
-    }
+  mpn_add_n (w5, w5, w2, m);
+  if (flags & toom4_w3_neg)
+    mpn_add_n (w3, w3, w4, m);
   else
-    {
-#ifdef HAVE_NATIVE_mpn_rsh1sub_n
-      mpn_rsh1sub_n (w1, w4, w1, m);
-#else
-      mpn_sub_n (w1, w4, w1, m);  ASSERT (!(w1[0] & 1));
-      mpn_rshift (w1, w1, m, 1);
-#endif
-    }
-  mpn_sub (w4, w4, m, w0, 2*n);
-  mpn_sub_n (w4, w4, w1, m);  ASSERT (!(w4[0] & 3));
-  mpn_rshift (w4, w4, m, 2); /* w4>=0 */
-
-  tp[w6n] = mpn_lshift (tp, w6, w6n, 4);
-  mpn_sub (w4, w4, m, tp, w6n+1);
-
-  if (flags & toom7_w3_neg)
-    {
-#ifdef HAVE_NATIVE_mpn_rsh1add_n
-      mpn_rsh1add_n (w3, w3, w2, m);
-#else
-      mpn_add_n (w3, w3, w2, m);  ASSERT (!(w3[0] & 1));
-      mpn_rshift (w3, w3, m, 1);
-#endif
-    }
+    mpn_sub_n (w3, w4, w3, m);
+  divexact_2exp (w3, w3, m, 1);
+  if (flags & toom4_w1_neg)
+    mpn_add_n (w1, w1, w2, m);
   else
-    {
-#ifdef HAVE_NATIVE_mpn_rsh1sub_n
-      mpn_rsh1sub_n (w3, w2, w3, m);
-#else
-      mpn_sub_n (w3, w2, w3, m);  ASSERT (!(w3[0] & 1));
-      mpn_rshift (w3, w3, m, 1);
-#endif
-    }
-
-  mpn_sub_n (w2, w2, w3, m);
-
-  mpn_submul_1 (w5, w2, m, 65);
+    mpn_sub_n (w1, w2, w1, m);
   mpn_sub (w2, w2, m, w6, w6n);
-  mpn_sub (w2, w2, m, w0, 2*n);
-
-  mpn_addmul_1 (w5, w2, m, 45);  ASSERT (!(w5[0] & 1));
-  mpn_rshift (w5, w5, m, 1);
-  mpn_sub_n (w4, w4, w2, m);
-
-  mpn_divexact_by3 (w4, w4, m);
+  tp[2*n] = mpn_lshift (tp, rp, 2*n, 6);
+  mpn_sub_n (w2, w2, tp, m);
+  mpn_lshift (w2, w2, m, 1);
+  mpn_sub_n (w2, w2, w1, m);
+  divexact_2exp (w2, w2, m, 3);
+  mpn_sub_n (w4, w4, w3, m);
+
+  mpn_submul_1 (w5, w4, m, 65);
+  mpn_sub (w4, w4, m, w6, w6n);
+  mpn_sub (w4, w4, m, rp, 2*n);
+  mpn_addmul_1 (w5, w4, m, 45);
   mpn_sub_n (w2, w2, w4, m);
+  /* Rely on divexact working with two's complement */
+  mpn_divexact_by3 (w2, w2, m);
+  mpn_sub_n (w4, w4, w2, m);
 
-  mpn_sub_n (w1, w5, w1, m);
-  mpn_lshift (tp, w3, m, 3);
+  mpn_sub_n (w1, w1, w5, m);
+  mpn_lshift (tp, w3, m, 4);
   mpn_sub_n (w5, w5, tp, m);
+  divexact_2exp (w5, w5, m, 1);
   mpn_divexact_by9 (w5, w5, m);
   mpn_sub_n (w3, w3, w5, m);
-
+  divexact_2exp (w1, w1, m, 1);
   mpn_divexact_by15 (w1, w1, m);
-  mpn_add_n (w1, w1, w5, m);  ASSERT (!(w1[0] & 1));
-  mpn_rshift (w1, w1, m, 1); /* w1>=0 now */
+  mpn_add_n (w1, w1, w5, m);
+  divexact_2exp (w1, w1, m, 1);
   mpn_sub_n (w5, w5, w1, m);
 
-  /* These bounds are valid for the 4x4 polynomial product of toom44,
-   * and they are conservative for toom53 and toom62. */
-  ASSERT (w1[2*n] < 2);
-  ASSERT (w2[2*n] < 3);
-  ASSERT (w3[2*n] < 4);
-  ASSERT (w4[2*n] < 3);
-  ASSERT (w5[2*n] < 2);
+  /* Two's complement coefficients must be non-negative at the end of
+     this procedure. */
+  ASSERT ( !(w1[2*n] & GMP_LIMB_HIGHBIT));
+  ASSERT ( !(w2[2*n] & GMP_LIMB_HIGHBIT));
+  ASSERT ( !(w3[2*n] & GMP_LIMB_HIGHBIT));
+  ASSERT ( !(w4[2*n] & GMP_LIMB_HIGHBIT));
+  ASSERT ( !(w5[2*n] & GMP_LIMB_HIGHBIT));
 
   /* Addition chain. Note carries and the 2n'th limbs that need to be
    * added in.
@@ -231,8 +174,8 @@
    *        c7   c6   c5   c4   c3                 Carries to propagate
    */
 
-  cy = mpn_add_n (rp + n, rp + n, w1, m);
-  MPN_INCR_U (w2 + n + 1, n , cy);
+  cy = mpn_add_n (rp + n, rp + n, w1, 2*n);
+  MPN_INCR_U (w2 + n, n + 1, w1[2*n] + cy);
   cy = mpn_add_n (rp + 3*n, rp + 3*n, w3, n);
   MPN_INCR_U (w3 + n, n + 1, w2[2*n] + cy);
   cy = mpn_add_n (rp + 4*n, w3 + n, w4, n);
@@ -240,7 +183,10 @@
   cy = mpn_add_n (rp + 5*n, w4 + n, w5, n);
   MPN_INCR_U (w5 + n, n + 1, w4[2*n] + cy);
   if (w6n > n + 1)
-    ASSERT_NOCARRY (mpn_add (rp + 6*n, rp + 6*n, w6n, w5 + n, n + 1));
+    {
+      mp_limb_t c7 = mpn_add_n (rp + 6*n, rp + 6*n, w5 + n, n + 1);
+      MPN_INCR_U (rp + 7*n + 1, w6n - n - 1, c7);
+    }
   else
     {
       ASSERT_NOCARRY (mpn_add_n (rp + 6*n, rp + 6*n, w5 + n, w6n));
--- 1/mpn/generic/udiv_w_sdiv.c
+++ 2/mpn/generic/udiv_w_sdiv.c
@@ -42,7 +42,7 @@
 
   if ((mp_limb_signed_t) d >= 0)
     {
-      if (a1 < d - a1 - (a0 >> (GMP_LIMB_BITS - 1)))
+      if (a1 < d - a1 - (a0 >> (BITS_PER_MP_LIMB - 1)))
 	{
 	  /* dividend, divisor, and quotient are nonnegative */
 	  sdiv_qrnnd (q, r, a1, a0, d);
@@ -50,18 +50,18 @@
       else
 	{
 	  /* Compute c1*2^32 + c0 = a1*2^32 + a0 - 2^31*d */
-	  sub_ddmmss (c1, c0, a1, a0, d >> 1, d << (GMP_LIMB_BITS - 1));
+	  sub_ddmmss (c1, c0, a1, a0, d >> 1, d << (BITS_PER_MP_LIMB - 1));
 	  /* Divide (c1*2^32 + c0) by d */
 	  sdiv_qrnnd (q, r, c1, c0, d);
 	  /* Add 2^31 to quotient */
-	  q += (mp_limb_t) 1 << (GMP_LIMB_BITS - 1);
+	  q += (mp_limb_t) 1 << (BITS_PER_MP_LIMB - 1);
 	}
     }
   else
     {
       b1 = d >> 1;			/* d/2, between 2^30 and 2^31 - 1 */
       c1 = a1 >> 1;			/* A/2 */
-      c0 = (a1 << (GMP_LIMB_BITS - 1)) + (a0 >> 1);
+      c0 = (a1 << (BITS_PER_MP_LIMB - 1)) + (a0 >> 1);
 
       if (a1 < b1)			/* A < 2^32*b1, so A/2 < 2^31*b1 */
 	{
--- 1/mpn/ia64/gcd_1.asm
+++ 2/mpn/ia64/gcd_1.asm
@@ -21,8 +21,8 @@
 
 
 C           cycles/bitpair (1x1 gcd)
-C Itanium:      14 (approx)
-C Itanium 2:     6.3
+C itanium2:      6.3
+C itanium:      14 (approx)
 
 
 C mpn_gcd_1 (mp_srcptr xp, mp_size_t xsize, mp_limb_t y);
--- 1/mpn/ia64/gmp-mparam.h
+++ 2/mpn/ia64/gmp-mparam.h
@@ -18,64 +18,55 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 /* 1300MHz Itanium2 (babe.fsffrance.org) */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.2 */
+/* Generated by tuneup.c, 2009-03-04, gcc 4.2 */
 
-#define MUL_TOOM22_THRESHOLD             44
-#define MUL_TOOM33_THRESHOLD             89
-#define MUL_TOOM44_THRESHOLD            232
-
-#define SQR_BASECASE_THRESHOLD           26
-#define SQR_TOOM2_THRESHOLD             119
-#define SQR_TOOM3_THRESHOLD             141
+#define MUL_KARATSUBA_THRESHOLD          44
+#define MUL_TOOM3_THRESHOLD             137
+#define MUL_TOOM44_THRESHOLD            230
+
+#define SQR_BASECASE_THRESHOLD           25
+#define SQR_KARATSUBA_THRESHOLD         119
+#define SQR_TOOM3_THRESHOLD             146
 #define SQR_TOOM4_THRESHOLD             284
 
-#define MUL_FFT_TABLE  { 528, 1120, 1856, 3840, 11264, 28672, 114688, 327680, 0 }
-#define MUL_FFT_MODF_THRESHOLD          624
-#define MUL_FFT_THRESHOLD              7680
-
-#define SQR_FFT_TABLE  { 592, 1248, 2368, 3840, 11264, 28672, 81920, 327680, 0 }
-#define SQR_FFT_MODF_THRESHOLD          608
-#define SQR_FFT_THRESHOLD              4992
-
-#define MULLO_BASECASE_THRESHOLD         17
-#define MULLO_DC_THRESHOLD               88
-#define MULLO_MUL_N_THRESHOLD         15280
-
-#define MULMOD_BNM1_THRESHOLD            25
-
-#define DC_DIV_QR_THRESHOLD              67
-#define DC_DIVAPPR_Q_THRESHOLD          268
-#define DC_BDIV_QR_THRESHOLD            108
-#define DC_BDIV_Q_THRESHOLD             292
-#define INV_MULMOD_BNM1_THRESHOLD       163
-#define INV_NEWTON_THRESHOLD            188
-#define INV_APPR_THRESHOLD                9
-#define BINV_NEWTON_THRESHOLD           300
-#define REDC_1_TO_REDC_2_THRESHOLD       10
-#define REDC_2_TO_REDC_N_THRESHOLD      164
-
-#define MATRIX22_STRASSEN_THRESHOLD      23
-#define HGCD_THRESHOLD                  120
-#define GCD_DC_THRESHOLD                630
-#define GCDEXT_DC_THRESHOLD             440
-#define JACOBI_BASE_METHOD                2
+#define MULLOW_BASECASE_THRESHOLD        19
+#define MULLOW_DC_THRESHOLD             120
+#define MULLOW_MUL_N_THRESHOLD          357
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* preinv always */
+#define DIV_DC_THRESHOLD                 70
+#define POWM_THRESHOLD                  312
+
+#define MATRIX22_STRASSEN_THRESHOLD      29
+#define HGCD_THRESHOLD                  118
+#define GCD_DC_THRESHOLD                595
+#define GCDEXT_DC_THRESHOLD             584
+#define JACOBI_BASE_METHOD                1
 
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 9
-#define MOD_1_2_THRESHOLD                10
-#define MOD_1_4_THRESHOLD                16
+#define MOD_1_1_THRESHOLD                 8
+#define MOD_1_2_THRESHOLD                 9
+#define MOD_1_4_THRESHOLD                20
 #define USE_PREINV_DIVREM_1               1  /* native */
 #define USE_PREINV_MOD_1                  1  /* preinv always */
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always */
 
-#define GET_STR_DC_THRESHOLD             15
-#define GET_STR_PRECOMPUTE_THRESHOLD     21
-#define SET_STR_DC_THRESHOLD           1339
-#define SET_STR_PRECOMPUTE_THRESHOLD   3565
+#define GET_STR_DC_THRESHOLD             17
+#define GET_STR_PRECOMPUTE_THRESHOLD     25
+#define SET_STR_DC_THRESHOLD           1488
+#define SET_STR_PRECOMPUTE_THRESHOLD   3590
+
+#define MUL_FFT_TABLE  { 528, 1184, 1856, 3840, 11264, 28672, 114688, 327680, 0 }
+#define MUL_FFT_MODF_THRESHOLD          784
+#define MUL_FFT_THRESHOLD              6656
+
+#define SQR_FFT_TABLE  { 592, 1248, 2368, 3840, 11264, 28672, 81920, 327680, 0 }
+#define SQR_FFT_MODF_THRESHOLD          608
+#define SQR_FFT_THRESHOLD              4992
--- 1/mpn/ia64/hamdist.asm
+++ 2/mpn/ia64/hamdist.asm
@@ -20,7 +20,6 @@
 include(`../config.m4')
 
 C           cycles/limb
-C Itanium:       2
 C Itanium 2:     1
 
 C INPUT PARAMETERS
--- 1/mpn/ia64/ia64-defs.m4
+++ 2/mpn/ia64/ia64-defs.m4
@@ -75,7 +75,7 @@
 dnl  Emit a ".align" directive.  "bytes" is eval()ed, so can be an
 dnl  expression.
 dnl
-dnl  This version overrides the definition in mpn/asm-defs.m4.  We suppress
+dnl  This version overrides the definition in mpn/asm-defs.m4.  We supress
 dnl  any .align if the gas byte-swapped-nops bug was detected by configure
 dnl  GMP_ASM_IA64_ALIGN_OK.
 
--- 1/mpn/ia64/invert_limb.asm
+++ 2/mpn/ia64/invert_limb.asm
@@ -23,7 +23,7 @@
 C d = r32
 
 C           cycles
-C Itanium:    74
+C Itanium:    ?
 C Itanium 2:  50+6
 
 C It should be possible to avoid the xmpy.hu and the following tests by
--- 1/mpn/ia64/README
+++ 2/mpn/ia64/README
@@ -237,7 +237,7 @@
 much unrolling.
 
 Depending on size or operand alignment, we get 1 c/l or 0.5 c/l on Itanium
-2, according to tune/speed.  Cache bank conflicts?
+2, according to tests/devel/try.  Cache bank conflicts?
 
 
 
@@ -268,3 +268,10 @@
 All the above documents can be found online at
 
     http://developer.intel.com/design/itanium/manuals.htm
+
+
+----------------
+Local variables:
+mode: text
+fill-column: 76
+End:
--- 1/mpn/m68k/gmp-mparam.h
+++ 2/mpn/m68k/gmp-mparam.h
@@ -18,7 +18,7 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -26,11 +26,11 @@
 
 /* Generated by tuneup.c, 2004-02-05, gcc 3.2 */
 
-#define MUL_TOOM22_THRESHOLD             14
-#define MUL_TOOM33_THRESHOLD             90
+#define MUL_KARATSUBA_THRESHOLD          14
+#define MUL_TOOM3_THRESHOLD              90
 
 #define SQR_BASECASE_THRESHOLD            5
-#define SQR_TOOM2_THRESHOLD              28
+#define SQR_KARATSUBA_THRESHOLD          28
 #define SQR_TOOM3_THRESHOLD              98
 
 #define DIV_SB_PREINV_THRESHOLD       MP_SIZE_T_MAX  /* never */
--- 1/mpn/Makefile.am
+++ 2/mpn/Makefile.am
@@ -44,14 +44,8 @@
   matrix22_mul.c mod_1.c mod_34lsub1.c mode1o.c	    \
   mod_1_1.c mod_1_2.c mod_1_3.c mod_1_4.c				    \
   mul.c mul_1.c mul_2.c mul_3.c mul_4.c mul_fft.c mul_n.c mul_basecase.c    \
-  toom22_mul.c toom32_mul.c toom42_mul.c toom52_mul.c toom62_mul.c	    \
-  toom33_mul.c toom43_mul.c toom53_mul.c				    \
-  toom44_mul.c								    \
-  sqr_toom2.c sqr_toom3.c sqr_toom4.c					    \
-  toom_eval_dgr3_pm1.c toom_eval_dgr3_pm2.c 				    \
-  toom_eval_pm1.c toom_eval_pm1.c toom_eval_pm2exp.c 			    \
-  toom_interpolate_5pts.c toom_interpolate_6pts.c toom_interpolate_7pts.c   \
-  mullo_n.c mullo_basecase.c nand_n.c neg_n.c nior_n.c perfsqr.c	    \
+  mul_toom22.c mul_toom32.c mul_toom42.c				    \
+  mullow_n.c mullow_basecase.c nand_n.c neg_n.c nior_n.c perfsqr.c	    \
   popcount.c pre_divrem_1.c pre_mod_1.c pow_1.c random.c random2.c rshift.c \
   rootrem.c sb_divrem_mn.c scan0.c scan1.c set_str.c			    \
   sqr_basecase.c sqr_diagonal.c						    \
--- 1/mpn/minithres/gmp-mparam.h
+++ 2/mpn/minithres/gmp-mparam.h
@@ -1,7 +1,6 @@
 /* Minimal values gmp-mparam.h -- Compiler/machine parameter header file.
 
-Copyright 1991, 1993, 1994, 2000, 2006, 2008, 2009 Free Software Foundation,
-Inc.
+Copyright 1991, 1993, 1994, 2000, 2006, 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -21,42 +20,23 @@
 /* The values in this file are not currently minimal.
    Trimming them further would be good.  */
 
-#define MUL_TOOM22_THRESHOLD              8
-#define MUL_TOOM33_THRESHOLD             20
+#define MUL_KARATSUBA_THRESHOLD           8
+#define MUL_TOOM3_THRESHOLD              20
 #define MUL_TOOM44_THRESHOLD             24
 
 #define SQR_BASECASE_THRESHOLD            0
-#define SQR_TOOM2_THRESHOLD               8
+#define SQR_KARATSUBA_THRESHOLD           8
 #define SQR_TOOM3_THRESHOLD              20
 #define SQR_TOOM4_THRESHOLD              24
 
-#define MUL_FFT_TABLE  {64, 256, 1024, 4096, 8192, 65536, 0}
-#define MUL_FFT_MODF_THRESHOLD  65
-#define MUL_FFT_THRESHOLD      200
-
-#define SQR_FFT_TABLE  {64, 256, 1024, 4096, 8192, 65536, 0}
-#define SQR_FFT_MODF_THRESHOLD  65
-#define SQR_FFT_THRESHOLD      200
-
-#define MULLO_BASECASE_THRESHOLD          0
-#define MULLO_DC_THRESHOLD                2
-#define MULLO_MUL_N_THRESHOLD             4
-
-#define MULMOD_BNM1_THRESHOLD            10
-
-#define DC_DIV_QR_THRESHOLD               6
-#define DC_DIVAPPR_Q_THRESHOLD            6
-#define DC_BDIV_QR_THRESHOLD              4
-#define MU_BDIV_QR_THRESHOLD             10
-#define DC_BDIV_Q_THRESHOLD               4
-#define MU_BDIV_Q_THRESHOLD              10
-#define BINV_NEWTON_THRESHOLD             6
-#define INV_MULMOD_BNM1_THRESHOLD         2
-#define INV_NEWTON_THRESHOLD              6
-#define INV_APPR_THRESHOLD                4
-#define REDC_1_TO_REDC_N_THRESHOLD        4
+#define MULLOW_BASECASE_THRESHOLD         0
+#define MULLOW_DC_THRESHOLD               2
+#define MULLOW_MUL_N_THRESHOLD            4
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                  6
+#define POWM_THRESHOLD                    4
 
-#define MATRIX22_STRASSEN_THRESHOLD       2
 #define HGCD_THRESHOLD                   10
 #define GCD_DC_THRESHOLD                 20
 #define GCDEXT_SCHOENHAGE_THRESHOLD      20
@@ -64,9 +44,6 @@
 
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 2
-#define MOD_1_2_THRESHOLD                 3
-#define MOD_1_4_THRESHOLD                 4
 #define USE_PREINV_DIVREM_1               1  /* native */
 #define USE_PREINV_MOD_1                  1
 #define DIVREM_2_THRESHOLD                0  /* always */
@@ -77,3 +54,11 @@
 #define GET_STR_PRECOMPUTE_THRESHOLD     10
 #define SET_STR_THRESHOLD                64
 #define SET_STR_PRECOMPUTE_THRESHOLD    100
+
+#define MUL_FFT_TABLE  {64-1, 256-1, 1024-1, 4096-1, 8192-1, 65536-1, 0}
+#define MUL_FFT_MODF_THRESHOLD  65
+#define MUL_FFT_THRESHOLD      200
+
+#define SQR_FFT_TABLE  {64-1, 256-1, 1024-1, 4096-1, 8192-1, 65536-1, 0}
+#define SQR_FFT_MODF_THRESHOLD  65
+#define SQR_FFT_THRESHOLD      200
--- 1/mpn/mips32/gmp-mparam.h
+++ 2/mpn/mips32/gmp-mparam.h
@@ -19,17 +19,17 @@
 with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
 /* Generated by tuneup.c, 2002-02-20, gcc 2.95 (R3000) */
 
-#define MUL_TOOM22_THRESHOLD             20
-#define MUL_TOOM33_THRESHOLD             50
+#define MUL_KARATSUBA_THRESHOLD          20
+#define MUL_TOOM3_THRESHOLD              50
 
 #define SQR_BASECASE_THRESHOLD            7
-#define SQR_TOOM2_THRESHOLD              57
+#define SQR_KARATSUBA_THRESHOLD          57
 #define SQR_TOOM3_THRESHOLD              78
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* always */
--- 1/mpn/mips64/gmp-mparam.h
+++ 2/mpn/mips64/gmp-mparam.h
@@ -19,17 +19,17 @@
 with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 
 /* Generated by tuneup.c, 2004-02-10, gcc 3.2 & MIPSpro C 7.2.1 (R1x000) */
 
-#define MUL_TOOM22_THRESHOLD             16
-#define MUL_TOOM33_THRESHOLD             89
+#define MUL_KARATSUBA_THRESHOLD          16
+#define MUL_TOOM3_THRESHOLD              89
 
 #define SQR_BASECASE_THRESHOLD            6
-#define SQR_TOOM2_THRESHOLD              32
+#define SQR_KARATSUBA_THRESHOLD          32
 #define SQR_TOOM3_THRESHOLD              98
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* always */
--- 1/mpn/pa32/gmp-mparam.h
+++ 2/mpn/pa32/gmp-mparam.h
@@ -18,21 +18,21 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 /* These values are for the PA7100 using GCC.  */
 /* Generated by tuneup.c, 2000-10-27. */
 
-#ifndef MUL_TOOM22_THRESHOLD   
-#define MUL_TOOM22_THRESHOLD      30
+#ifndef MUL_KARATSUBA_THRESHOLD
+#define MUL_KARATSUBA_THRESHOLD   30
 #endif
-#ifndef MUL_TOOM33_THRESHOLD
-#define MUL_TOOM33_THRESHOLD     141
+#ifndef MUL_TOOM3_THRESHOLD
+#define MUL_TOOM3_THRESHOLD      141
 #endif
 
-#ifndef SQR_TOOM2_THRESHOLD    
-#define SQR_TOOM2_THRESHOLD       59
+#ifndef SQR_KARATSUBA_THRESHOLD
+#define SQR_KARATSUBA_THRESHOLD   59
 #endif
 #ifndef SQR_TOOM3_THRESHOLD
 #define SQR_TOOM3_THRESHOLD      177
--- 1/mpn/pa32/hppa1_1/gmp-mparam.h
+++ 2/mpn/pa32/hppa1_1/gmp-mparam.h
@@ -18,16 +18,16 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 /* Generated by tuneup.c, 2004-02-07, gcc 2.8 (pa7100/100MHz) */
 
-#define MUL_TOOM22_THRESHOLD             30
-#define MUL_TOOM33_THRESHOLD             89
+#define MUL_KARATSUBA_THRESHOLD          30
+#define MUL_TOOM3_THRESHOLD              89
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              55
+#define SQR_KARATSUBA_THRESHOLD          55
 #define SQR_TOOM3_THRESHOLD             101
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* always */
--- 1/mpn/pa32/hppa2_0/gmp-mparam.h
+++ 2/mpn/pa32/hppa2_0/gmp-mparam.h
@@ -18,66 +18,56 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
-/* 552 MHz PA8600 (gcc61.fsffrance.org) */
+/* Generated by tuneup.c, 2009-03-05, gcc 4.3 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.2 */
-
-#define MUL_TOOM22_THRESHOLD             22
-#define MUL_TOOM33_THRESHOLD            104
+#define MUL_KARATSUBA_THRESHOLD          15
+#define MUL_TOOM3_THRESHOLD              98
 #define MUL_TOOM44_THRESHOLD            158
 
 #define SQR_BASECASE_THRESHOLD            6
-#define SQR_TOOM2_THRESHOLD              51
-#define SQR_TOOM3_THRESHOLD              89
-#define SQR_TOOM4_THRESHOLD             250
-
-#define MUL_FFT_TABLE  { 304, 672, 1152, 2560, 6144, 24576, 98304, 393216, 0 }
-#define MUL_FFT_MODF_THRESHOLD          232
-#define MUL_FFT_THRESHOLD              1792
-
-#define SQR_FFT_TABLE  { 304, 672, 1152, 2560, 6144, 24576, 98304, 393216, 0 }
-#define SQR_FFT_MODF_THRESHOLD          232
-#define SQR_FFT_THRESHOLD              1792
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               90
-#define MULLO_MUL_N_THRESHOLD          2350
-
-#define MULMOD_BNM1_THRESHOLD            16
-
-#define DC_DIV_QR_THRESHOLD             100
-#define DC_DIVAPPR_Q_THRESHOLD          348
-#define DC_BDIV_QR_THRESHOLD            119
-#define DC_BDIV_Q_THRESHOLD             248
-#define INV_MULMOD_BNM1_THRESHOLD        42
-#define INV_NEWTON_THRESHOLD            296
-#define INV_APPR_THRESHOLD               16
-#define BINV_NEWTON_THRESHOLD           330
-#define REDC_1_TO_REDC_N_THRESHOLD       78
-
-#define MATRIX22_STRASSEN_THRESHOLD      11
-#define HGCD_THRESHOLD                  101
-#define GCD_DC_THRESHOLD                368
-#define GCDEXT_DC_THRESHOLD             330
+#define SQR_KARATSUBA_THRESHOLD          48
+#define SQR_TOOM3_THRESHOLD              97
+#define SQR_TOOM4_THRESHOLD             232
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              92
+#define MULLOW_MUL_N_THRESHOLD          363
+
+#define DIV_SB_PREINV_THRESHOLD           4
+#define DIV_DC_THRESHOLD                 92
+#define POWM_THRESHOLD                  142
+
+#define MATRIX22_STRASSEN_THRESHOLD      17
+#define HGCD_THRESHOLD                  100
+#define GCD_DC_THRESHOLD                365
+#define GCDEXT_DC_THRESHOLD             339
 #define JACOBI_BASE_METHOD                2
 
-#define DIVREM_1_NORM_THRESHOLD           0  /* always */
-#define DIVREM_1_UNNORM_THRESHOLD         4
+#define DIVREM_1_NORM_THRESHOLD           3
+#define DIVREM_1_UNNORM_THRESHOLD         5
 #define MOD_1_NORM_THRESHOLD              4
-#define MOD_1_UNNORM_THRESHOLD            4
-#define MOD_1_1_THRESHOLD                12
-#define MOD_1_2_THRESHOLD                13
-#define MOD_1_4_THRESHOLD                18
+#define MOD_1_UNNORM_THRESHOLD            5
+#define MOD_1_1_THRESHOLD                 6
+#define MOD_1_2_THRESHOLD                 9
+#define MOD_1_4_THRESHOLD                24
 #define USE_PREINV_DIVREM_1               1
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1
 #define DIVREM_2_THRESHOLD                0  /* always */
 #define DIVEXACT_1_THRESHOLD              0  /* always */
 #define MODEXACT_1_ODD_THRESHOLD      MP_SIZE_T_MAX  /* never */
 
-#define GET_STR_DC_THRESHOLD              7
-#define GET_STR_PRECOMPUTE_THRESHOLD     14
+#define GET_STR_DC_THRESHOLD              8
+#define GET_STR_PRECOMPUTE_THRESHOLD     13
 #define SET_STR_DC_THRESHOLD            224
 #define SET_STR_PRECOMPUTE_THRESHOLD    702
+
+#define MUL_FFT_TABLE  { 272, 672, 896, 2560, 6144, 24576, 98304, 393216, 0 }
+#define MUL_FFT_MODF_THRESHOLD          232
+#define MUL_FFT_THRESHOLD              1792
+
+#define SQR_FFT_TABLE  { 304, 672, 1152, 2560, 10240, 24576, 98304, 393216, 0 }
+#define SQR_FFT_MODF_THRESHOLD          232
+#define SQR_FFT_THRESHOLD              1792
--- 1/mpn/pa32/pa-defs.m4
+++ 2/mpn/pa32/pa-defs.m4
@@ -22,7 +22,7 @@
 
 dnl  hppa assembler comments are introduced with ";".
 dnl
-dnl  For cooperation with cpp, apparently lines "# 123" set the line number,
+dnl  For cooperation with cpp, aparently lines "# 123" set the line number,
 dnl  and other lines starting with a "#" are ignored.
 
 changecom(;)
--- 1/mpn/pa64/gmp-mparam.h
+++ 2/mpn/pa64/gmp-mparam.h
@@ -1,6 +1,6 @@
 /* gmp-mparam.h -- Compiler/machine parameter header file.
 
-Copyright 1991, 1993, 1994, 1999, 2000, 2001, 2002, 2003, 2004, 2008, 2009
+Copyright 1991, 1993, 1994, 1999, 2000, 2001, 2002, 2003, 2004, 2008
 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
@@ -18,65 +18,55 @@
 You should have received a copy of the GNU Lesser General Public License along
 with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 /* 440MHz PA8200 */
 
-/* Generated by tuneup.c, 2009-11-29, system compiler */
+/* Generated by tuneup.c, 2009-01-04, system compiler */
 
-#define MUL_TOOM22_THRESHOLD             30
-#define MUL_TOOM33_THRESHOLD            113
-#define MUL_TOOM44_THRESHOLD            195
+#define MUL_KARATSUBA_THRESHOLD          30
+#define MUL_TOOM3_THRESHOLD             114
+#define MUL_TOOM44_THRESHOLD            244
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              56
-#define SQR_TOOM3_THRESHOLD             169
-#define SQR_TOOM4_THRESHOLD             284
-
-#define MUL_FFT_TABLE  { 336, 800, 1600, 2816, 7168, 20480, 81920, 327680, 0 }
-#define MUL_FFT_MODF_THRESHOLD          280
-#define MUL_FFT_THRESHOLD              1664
-
-#define SQR_FFT_TABLE  { 368, 800, 1728, 3328, 7168, 20480, 81920, 327680, 786432, 0 }
-#define SQR_FFT_MODF_THRESHOLD          264
-#define SQR_FFT_THRESHOLD              1792
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD              111
-#define MULLO_MUL_N_THRESHOLD          2764
-
-#define MULMOD_BNM1_THRESHOLD            20
-
-#define DC_DIV_QR_THRESHOLD             124
-#define DC_BDIV_QR_THRESHOLD            150
-#define DC_BDIV_Q_THRESHOLD             312
-#define INV_MULMOD_BNM1_THRESHOLD        58
-#define INV_NEWTON_THRESHOLD            324
-#define INV_APPR_THRESHOLD                4
-#define BINV_NEWTON_THRESHOLD           360
-#define REDC_1_TO_REDC_N_THRESHOLD      106
+#define SQR_KARATSUBA_THRESHOLD          58
+#define SQR_TOOM3_THRESHOLD             174
+#define SQR_TOOM4_THRESHOLD             312
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD             142
+#define MULLOW_MUL_N_THRESHOLD          507
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                124
+#define POWM_THRESHOLD                  224
 
 #define MATRIX22_STRASSEN_THRESHOLD      11
-#define HGCD_THRESHOLD                  236
-#define GCD_DC_THRESHOLD                758
-#define GCDEXT_DC_THRESHOLD             744
+#define HGCD_THRESHOLD                  294
+#define GCD_DC_THRESHOLD                913
+#define GCDEXT_DC_THRESHOLD             830
 #define JACOBI_BASE_METHOD                2
 
 #define DIVREM_1_NORM_THRESHOLD           0  /* always */
 #define DIVREM_1_UNNORM_THRESHOLD         0  /* always */
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD             MP_SIZE_T_MAX  /* never */
-#define MOD_1_2_THRESHOLD             MP_SIZE_T_MAX  /* never */
-#define MOD_1_4_THRESHOLD             MP_SIZE_T_MAX  /* never */
 #define USE_PREINV_DIVREM_1               1
 #define USE_PREINV_MOD_1                  1
 #define DIVREM_2_THRESHOLD                0  /* always */
 #define DIVEXACT_1_THRESHOLD              0  /* always */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always */
 
-#define GET_STR_DC_THRESHOLD             20
-#define GET_STR_PRECOMPUTE_THRESHOLD     23
-#define SET_STR_DC_THRESHOLD           2008
-#define SET_STR_PRECOMPUTE_THRESHOLD   4066
+#define GET_STR_DC_THRESHOLD             23
+#define GET_STR_PRECOMPUTE_THRESHOLD     26
+#define SET_STR_DC_THRESHOLD           2743
+#define SET_STR_PRECOMPUTE_THRESHOLD   5147
+
+#define MUL_FFT_TABLE  { 400, 800, 1600, 2816, 7168, 20480, 81920, 327680, 0 }
+#define MUL_FFT_MODF_THRESHOLD          280
+#define MUL_FFT_THRESHOLD              1664
+
+#define SQR_FFT_TABLE  { 368, 800, 1728, 3328, 7168, 20480, 81920, 327680, 786432, 0 }
+#define SQR_FFT_MODF_THRESHOLD          264
+#define SQR_FFT_THRESHOLD              1632
--- 1/mpn/power/gmp-mparam.h
+++ 2/mpn/power/gmp-mparam.h
@@ -19,11 +19,11 @@
 
 /* Generated by tuneup.c, 2003-02-10, gcc 3.2, POWER2 66.7MHz */
 
-#define MUL_TOOM22_THRESHOLD             12
-#define MUL_TOOM33_THRESHOLD             75
+#define MUL_KARATSUBA_THRESHOLD          12
+#define MUL_TOOM3_THRESHOLD              75
 
 #define SQR_BASECASE_THRESHOLD            7
-#define SQR_TOOM2_THRESHOLD              28
+#define SQR_KARATSUBA_THRESHOLD          28
 #define SQR_TOOM3_THRESHOLD              86
 
 #define DIV_SB_PREINV_THRESHOLD       MP_SIZE_T_MAX  /* never */
--- 1/mpn/powerpc32/750/gmp-mparam.h
+++ 2/mpn/powerpc32/750/gmp-mparam.h
@@ -1,6 +1,6 @@
 /* PowerPC-32 gmp-mparam.h -- Compiler/machine parameter header file.
 
-Copyright 2002, 2004, 2009 Free Software Foundation, Inc.
+Copyright 2002, 2004 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -17,7 +17,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -26,60 +26,49 @@
 
 /* 450 MHz PPC 7400 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.0 */
+/* Generated by tuneup.c, 2008-12-23, gcc 4.0 */
 
-#define MUL_TOOM22_THRESHOLD             10
-#define MUL_TOOM33_THRESHOLD             38
-#define MUL_TOOM44_THRESHOLD             99
+#define MUL_KARATSUBA_THRESHOLD          10
+#define MUL_TOOM3_THRESHOLD              41
+#define MUL_TOOM44_THRESHOLD             88
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              18
+#define SQR_KARATSUBA_THRESHOLD          18
 #define SQR_TOOM3_THRESHOLD              57
-#define SQR_TOOM4_THRESHOLD             143
+#define SQR_TOOM4_THRESHOLD              88
 
-#define MUL_FFT_TABLE  { 240, 672, 896, 2560, 6144, 40960, 98304, 393216, 0 }
-#define MUL_FFT_MODF_THRESHOLD          232
-#define MUL_FFT_THRESHOLD              2816
-
-#define SQR_FFT_TABLE  { 240, 544, 896, 2560, 6144, 24576, 98304, 393216, 0 }
-#define SQR_FFT_MODF_THRESHOLD          216
-#define SQR_FFT_THRESHOLD              2304
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              32
+#define MULLOW_MUL_N_THRESHOLD          194
 
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               33
-#define MULLO_MUL_N_THRESHOLD          5472
-
-#define MULMOD_BNM1_THRESHOLD            10
-
-#define DC_DIV_QR_THRESHOLD              31
-#define DC_DIVAPPR_Q_THRESHOLD          117
-#define DC_BDIV_QR_THRESHOLD             35
-#define DC_BDIV_Q_THRESHOLD              86
-#define INV_MULMOD_BNM1_THRESHOLD        76
-#define INV_NEWTON_THRESHOLD            146
-#define INV_APPR_THRESHOLD               23
-#define BINV_NEWTON_THRESHOLD           157
-#define REDC_1_TO_REDC_N_THRESHOLD       39
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 33
+#define POWM_THRESHOLD                   48
 
-#define MATRIX22_STRASSEN_THRESHOLD      11
+#define MATRIX22_STRASSEN_THRESHOLD      13
 #define HGCD_THRESHOLD                   91
-#define GCD_DC_THRESHOLD                258
-#define GCDEXT_DC_THRESHOLD             209
+#define GCD_DC_THRESHOLD                256
+#define GCDEXT_DC_THRESHOLD             256
 #define JACOBI_BASE_METHOD                1
 
 #define DIVREM_1_NORM_THRESHOLD           0  /* always */
 #define DIVREM_1_UNNORM_THRESHOLD         0  /* always */
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 4
-#define MOD_1_2_THRESHOLD             MP_SIZE_T_MAX  /* never */
-#define MOD_1_4_THRESHOLD             MP_SIZE_T_MAX  /* never */
 #define USE_PREINV_DIVREM_1               1
 #define USE_PREINV_MOD_1                  1
 #define DIVEXACT_1_THRESHOLD              0  /* always */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             12
+#define GET_STR_DC_THRESHOLD             13
 #define GET_STR_PRECOMPUTE_THRESHOLD     27
-#define SET_STR_DC_THRESHOLD            387
-#define SET_STR_PRECOMPUTE_THRESHOLD    824
+#define SET_STR_DC_THRESHOLD            390
+#define SET_STR_PRECOMPUTE_THRESHOLD    814
+
+#define MUL_FFT_TABLE  { 240, 608, 896, 2560, 6144, 40960, 0 }
+#define MUL_FFT_MODF_THRESHOLD          232
+#define MUL_FFT_THRESHOLD              1792
+
+#define SQR_FFT_TABLE  { 240, 544, 896, 2560, 6144, 24576, 0 }
+#define SQR_FFT_MODF_THRESHOLD          216
+#define SQR_FFT_THRESHOLD              1792
--- 1/mpn/powerpc32/gmp-mparam.h
+++ 2/mpn/powerpc32/gmp-mparam.h
@@ -18,7 +18,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -32,60 +32,52 @@
 
 /* 1417 MHz PPC 7447A */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.3 */
+/* Generated by tuneup.c, 2009-01-14, gcc 4.3 */
 
-#define MUL_TOOM22_THRESHOLD             14
-#define MUL_TOOM33_THRESHOLD             73
+#define MUL_KARATSUBA_THRESHOLD          14
+#define MUL_TOOM3_THRESHOLD              73
 #define MUL_TOOM44_THRESHOLD            106
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              24
+#define SQR_KARATSUBA_THRESHOLD          24
 #define SQR_TOOM3_THRESHOLD              77
 #define SQR_TOOM4_THRESHOLD             130
 
-#define MUL_FFT_TABLE  { 304, 672, 896, 2560, 6144, 24576, 98304, 393216, 0 }
-#define MUL_FFT_MODF_THRESHOLD          320
-#define MUL_FFT_THRESHOLD              2816
-
-#define SQR_FFT_TABLE  { 272, 672, 1152, 2560, 10240, 24576, 98304, 393216, 0 }
-#define SQR_FFT_MODF_THRESHOLD          288
-#define SQR_FFT_THRESHOLD              2304
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               45
-#define MULLO_MUL_N_THRESHOLD          5472
-
-#define MULMOD_BNM1_THRESHOLD             9
-
-#define DC_DIV_QR_THRESHOLD              44
-#define DC_DIVAPPR_Q_THRESHOLD          153
-#define DC_BDIV_QR_THRESHOLD             54
-#define DC_BDIV_Q_THRESHOLD             124
-#define INV_MULMOD_BNM1_THRESHOLD       108
-#define INV_NEWTON_THRESHOLD            196
-#define INV_APPR_THRESHOLD               19
-#define BINV_NEWTON_THRESHOLD           232
-#define REDC_1_TO_REDC_N_THRESHOLD       54
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              52
+#define MULLOW_MUL_N_THRESHOLD          292
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 46
+#define POWM_THRESHOLD                   87
 
 #define MATRIX22_STRASSEN_THRESHOLD      15
-#define HGCD_THRESHOLD                  119
-#define GCD_DC_THRESHOLD                348
-#define GCDEXT_DC_THRESHOLD             306
+#define HGCD_THRESHOLD                  127
+#define GCD_DC_THRESHOLD                361
+#define GCDEXT_DC_THRESHOLD             382
 #define JACOBI_BASE_METHOD                1
 
 #define DIVREM_1_NORM_THRESHOLD           0  /* always */
 #define DIVREM_1_UNNORM_THRESHOLD         0  /* always */
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 6
-#define MOD_1_2_THRESHOLD                15
-#define MOD_1_4_THRESHOLD                74
+#define MOD_1_1_THRESHOLD                 7
+#define MOD_1_2_THRESHOLD                21
+#define MOD_1_4_THRESHOLD                68
 #define USE_PREINV_DIVREM_1               1
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1
 #define DIVEXACT_1_THRESHOLD              0  /* always */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             17
-#define GET_STR_PRECOMPUTE_THRESHOLD     37
+#define GET_STR_DC_THRESHOLD             22
+#define GET_STR_PRECOMPUTE_THRESHOLD     42
 #define SET_STR_DC_THRESHOLD            788
-#define SET_STR_PRECOMPUTE_THRESHOLD   1495
+#define SET_STR_PRECOMPUTE_THRESHOLD   1554
+
+#define MUL_FFT_TABLE  { 304, 672, 1152, 2560, 6144, 24576, 0 }
+#define MUL_FFT_MODF_THRESHOLD          320
+#define MUL_FFT_THRESHOLD              2816
+
+#define SQR_FFT_TABLE  { 272, 672, 1152, 2560, 10240, 24576, 0 }
+#define SQR_FFT_MODF_THRESHOLD          288
+#define SQR_FFT_THRESHOLD              2304
--- 1/mpn/powerpc64/gmp-mparam.h
+++ 2/mpn/powerpc64/gmp-mparam.h
@@ -18,7 +18,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 
@@ -26,11 +26,11 @@
 
 /* Generated by tuneup.c, 2004-02-10, gcc "2.9" */
 
-#define MUL_TOOM22_THRESHOLD              8
-#define MUL_TOOM33_THRESHOLD             41
+#define MUL_KARATSUBA_THRESHOLD           8
+#define MUL_TOOM3_THRESHOLD              41
 
 #define SQR_BASECASE_THRESHOLD            0  /* always */
-#define SQR_TOOM2_THRESHOLD              14
+#define SQR_KARATSUBA_THRESHOLD          14
 #define SQR_TOOM3_THRESHOLD              48
 
 #define DIV_SB_PREINV_THRESHOLD           0
--- 1/mpn/powerpc64/mode64/dive_1.asm
+++ 2/mpn/powerpc64/mode64/dive_1.asm
@@ -25,7 +25,7 @@
 C POWER5:	     16
 
 C TODO
-C  * Check if n=1 code is really an improvement.  It probably isn't.
+C  * Check if n=1 code is really an improvment.  It probably isn't.
 C  * Perhaps remove L(norm) code, it is currently unreachable.
 C  * Make more similar to mode1o.asm.
 
--- 1/mpn/powerpc64/mode64/gmp-mparam.h
+++ 2/mpn/powerpc64/mode64/gmp-mparam.h
@@ -17,50 +17,50 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 /* 1600MHz PPC970 */
 
 /* Generated by tuneup.c, 2009-01-14, gcc 4.0 */
 
-#define MUL_TOOM22_THRESHOLD             14
-#define MUL_TOOM33_THRESHOLD             93
-#define MUL_TOOM44_THRESHOLD            135
-
-#define SQR_BASECASE_THRESHOLD            6
-#define SQR_TOOM2_THRESHOLD              32
-#define SQR_TOOM3_THRESHOLD              74
-#define SQR_TOOM4_THRESHOLD             136
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               44
-#define MULLO_MUL_N_THRESHOLD           234
+#define MUL_KARATSUBA_THRESHOLD          14
+#define MUL_TOOM3_THRESHOLD              57
+#define MUL_TOOM44_THRESHOLD            155
+
+#define SQR_BASECASE_THRESHOLD            5
+#define SQR_KARATSUBA_THRESHOLD          32
+#define SQR_TOOM3_THRESHOLD              89
+#define SQR_TOOM4_THRESHOLD             154
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              40
+#define MULLOW_MUL_N_THRESHOLD          234
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* always */
-#define DIV_DC_THRESHOLD                 33
-#define POWM_THRESHOLD                   89
+#define DIV_DC_THRESHOLD                 32
+#define POWM_THRESHOLD                   93
 
-#define MATRIX22_STRASSEN_THRESHOLD      15
-#define HGCD_THRESHOLD                   93
-#define GCD_DC_THRESHOLD                237
-#define GCDEXT_DC_THRESHOLD             273
+#define MATRIX22_STRASSEN_THRESHOLD      19
+#define HGCD_THRESHOLD                   96
+#define GCD_DC_THRESHOLD                242
+#define GCDEXT_DC_THRESHOLD             353
 #define JACOBI_BASE_METHOD                1
 
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 6
+#define MOD_1_1_THRESHOLD                 7
 #define MOD_1_2_THRESHOLD                 9
-#define MOD_1_4_THRESHOLD                23
+#define MOD_1_4_THRESHOLD                44
 #define USE_PREINV_DIVREM_1               0
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
-#define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
+#define MODEXACT_1_ODD_THRESHOLD          0  /* always */
 
-#define GET_STR_DC_THRESHOLD             12
-#define GET_STR_PRECOMPUTE_THRESHOLD     24
-#define SET_STR_DC_THRESHOLD            650
-#define SET_STR_PRECOMPUTE_THRESHOLD   1713
+#define GET_STR_DC_THRESHOLD             10
+#define GET_STR_PRECOMPUTE_THRESHOLD     20
+#define SET_STR_DC_THRESHOLD            532
+#define SET_STR_PRECOMPUTE_THRESHOLD   1790
 
 #define MUL_FFT_TABLE  { 336, 672, 1856, 2816, 7168, 20480, 81920, 327680, 0 }
 #define MUL_FFT_MODF_THRESHOLD          304
--- 1/mpn/powerpc64/mode64/mode1o.asm
+++ 2/mpn/powerpc64/mode64/mode1o.asm
@@ -25,8 +25,8 @@
 C POWER5:            16
 
 C TODO
-C  * Check if n=1 code is really an improvement.  It probably isn't.
-C  * Make more similar to dive_1.asm.
+C  * Check if n=1 code is really an improvment.  It probably isn't.
+C  * Make more similar to dive_1.asm..
 
 C INPUT PARAMETERS
 define(`up', `r3')
--- 1/mpn/s390/gmp-mparam.h
+++ 2/mpn/s390/gmp-mparam.h
@@ -18,16 +18,16 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-/* GMP_LIMB_BITS etc generated by configure */
+/* BITS_PER_MP_LIMB etc generated by configure */
 
 
 /* Generated by tuneup.c, 2001-12-03, gcc 2.95 */
 
-#define MUL_TOOM22_THRESHOLD          18
-#define MUL_TOOM33_THRESHOLD         210
+#define MUL_KARATSUBA_THRESHOLD       18
+#define MUL_TOOM3_THRESHOLD          210
 
 #define SQR_BASECASE_THRESHOLD         8
-#define SQR_TOOM2_THRESHOLD           40
+#define SQR_KARATSUBA_THRESHOLD       40
 #define SQR_TOOM3_THRESHOLD          250
 
 #define DIV_SB_PREINV_THRESHOLD            0
--- 1/mpn/sparc32/gmp-mparam.h
+++ 2/mpn/sparc32/gmp-mparam.h
@@ -20,11 +20,11 @@
 
 /* Generated by tuneup.c, 2002-03-13, gcc 2.95, Weitek 8701 */
 
-#define MUL_TOOM22_THRESHOLD              8
-#define MUL_TOOM33_THRESHOLD            466
+#define MUL_KARATSUBA_THRESHOLD           8
+#define MUL_TOOM3_THRESHOLD             466
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              16
+#define SQR_KARATSUBA_THRESHOLD          16
 #define SQR_TOOM3_THRESHOLD             258
 
 #define DIV_SB_PREINV_THRESHOLD           4
--- 1/mpn/sparc32/v8/gmp-mparam.h
+++ 2/mpn/sparc32/v8/gmp-mparam.h
@@ -19,16 +19,16 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 /* Generated by tuneup.c, 2004-02-07, gcc 2.95 */
 
-#define MUL_TOOM22_THRESHOLD             10
-#define MUL_TOOM33_THRESHOLD             65
+#define MUL_KARATSUBA_THRESHOLD          10
+#define MUL_TOOM3_THRESHOLD              65
 
 #define SQR_BASECASE_THRESHOLD            4
-#define SQR_TOOM2_THRESHOLD              18
+#define SQR_KARATSUBA_THRESHOLD          18
 #define SQR_TOOM3_THRESHOLD              65
 
 #define DIV_SB_PREINV_THRESHOLD           5
--- 1/mpn/sparc32/v8/supersparc/gmp-mparam.h
+++ 2/mpn/sparc32/v8/supersparc/gmp-mparam.h
@@ -19,16 +19,16 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 /* Generated by tuneup.c, 2004-02-10, gcc 3.3 */
 
-#define MUL_TOOM22_THRESHOLD             14
-#define MUL_TOOM33_THRESHOLD             81
+#define MUL_KARATSUBA_THRESHOLD          14
+#define MUL_TOOM3_THRESHOLD              81
 
 #define SQR_BASECASE_THRESHOLD            5
-#define SQR_TOOM2_THRESHOLD              28
+#define SQR_KARATSUBA_THRESHOLD          28
 #define SQR_TOOM3_THRESHOLD              86
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* always */
--- 1/mpn/sparc32/v9/gmp-mparam.h
+++ 2/mpn/sparc32/v9/gmp-mparam.h
@@ -18,64 +18,56 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
-/* Generated by tuneup.c, 2009-12-14, gcc 4.2 */
+/* Generated by tuneup.c, 2009-02-12, gcc 2.95 */
 
-#define MUL_TOOM22_THRESHOLD             30
-#define MUL_TOOM33_THRESHOLD             89
-#define MUL_TOOM44_THRESHOLD            230
+#define MUL_KARATSUBA_THRESHOLD          28
+#define MUL_TOOM3_THRESHOLD              97
+#define MUL_TOOM44_THRESHOLD            136
 
 #define SQR_BASECASE_THRESHOLD            8
-#define SQR_TOOM2_THRESHOLD              63
-#define SQR_TOOM3_THRESHOLD              97
-#define SQR_TOOM4_THRESHOLD             266
-
-#define MUL_FFT_TABLE  { 304, 800, 1408, 3584, 10240, 24576, 98304, 393216, 0 }
-#define MUL_FFT_MODF_THRESHOLD          264
-#define MUL_FFT_THRESHOLD              1472
-
-#define SQR_FFT_TABLE  { 336, 672, 1408, 3584, 10240, 24576, 98304, 393216, 0 }
-#define SQR_FFT_MODF_THRESHOLD          232
-#define SQR_FFT_THRESHOLD              2304
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD              103
-#define MULLO_MUL_N_THRESHOLD          2764
-
-#define MULMOD_BNM1_THRESHOLD            13
-
-#define DC_DIV_QR_THRESHOLD             116
-#define DC_DIVAPPR_Q_THRESHOLD          387
-#define DC_BDIV_QR_THRESHOLD            116
-#define DC_BDIV_Q_THRESHOLD             272
-#define INV_MULMOD_BNM1_THRESHOLD        58
-#define INV_NEWTON_THRESHOLD            360
-#define INV_APPR_THRESHOLD               13
-#define BINV_NEWTON_THRESHOLD           348
-#define REDC_1_TO_REDC_N_THRESHOLD       86
+#define SQR_KARATSUBA_THRESHOLD          60
+#define SQR_TOOM3_THRESHOLD             138
+#define SQR_TOOM4_THRESHOLD             278
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD             111
+#define MULLOW_MUL_N_THRESHOLD          434
+
+#define DIV_SB_PREINV_THRESHOLD           7
+#define DIV_DC_THRESHOLD                122
+#define POWM_THRESHOLD                  154
 
 #define MATRIX22_STRASSEN_THRESHOLD      12
-#define HGCD_THRESHOLD                  142
-#define GCD_DC_THRESHOLD                630
-#define GCDEXT_DC_THRESHOLD             416
-#define JACOBI_BASE_METHOD                3
+#define HGCD_THRESHOLD                  155
+#define GCD_DC_THRESHOLD                614
+#define GCDEXT_DC_THRESHOLD             438
+#define JACOBI_BASE_METHOD                2
 
 #define DIVREM_1_NORM_THRESHOLD           5
-#define DIVREM_1_UNNORM_THRESHOLD         7
+#define DIVREM_1_UNNORM_THRESHOLD        14
 #define MOD_1_NORM_THRESHOLD              4
-#define MOD_1_UNNORM_THRESHOLD            6
-#define MOD_1_1_THRESHOLD                 5
-#define MOD_1_2_THRESHOLD                 7
-#define MOD_1_4_THRESHOLD                14
+#define MOD_1_UNNORM_THRESHOLD            5
+#define MOD_1_1_THRESHOLD                 7
+#define MOD_1_2_THRESHOLD                 8
+#define MOD_1_4_THRESHOLD                16
 #define USE_PREINV_DIVREM_1               1
 #define USE_PREINV_MOD_1                  1
 #define DIVREM_2_THRESHOLD                0  /* always */
 #define DIVEXACT_1_THRESHOLD              0  /* always */
 #define MODEXACT_1_ODD_THRESHOLD      MP_SIZE_T_MAX  /* never */
 
-#define GET_STR_DC_THRESHOLD             10
-#define GET_STR_PRECOMPUTE_THRESHOLD     17
-#define SET_STR_DC_THRESHOLD            527
-#define SET_STR_PRECOMPUTE_THRESHOLD   1576
+#define GET_STR_DC_THRESHOLD             12
+#define GET_STR_PRECOMPUTE_THRESHOLD     19
+#define SET_STR_DC_THRESHOLD            802
+#define SET_STR_PRECOMPUTE_THRESHOLD   1647
+
+#define MUL_FFT_TABLE  { 304, 736, 1152, 3584, 10240, 24576, 98304, 393216, 0 }
+#define MUL_FFT_MODF_THRESHOLD          264
+#define MUL_FFT_THRESHOLD              2304
+
+#define SQR_FFT_TABLE  { 336, 800, 1408, 3584, 10240, 24576, 98304, 393216, 0 }
+#define SQR_FFT_MODF_THRESHOLD          248
+#define SQR_FFT_THRESHOLD              2304
--- 1/mpn/sparc64/gmp-mparam.h
+++ 2/mpn/sparc64/gmp-mparam.h
@@ -18,67 +18,63 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
-/* 500 MHz ultrasparc2 running GNU/Linux */
+/* Tell the toom3 multiply implementation to call low-level mpn
+   functions instead of open-coding operations in C.  */
+#ifndef USE_MORE_MPN
+#define USE_MORE_MPN 1
+#endif
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.1 */
+/* Generated by tuneup.c, 2009-01-15, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             30
-#define MUL_TOOM33_THRESHOLD            171
+#define MUL_KARATSUBA_THRESHOLD          33
+#define MUL_TOOM3_THRESHOLD             189
 #define MUL_TOOM44_THRESHOLD            256
 
-#define SQR_BASECASE_THRESHOLD           10
-#define SQR_TOOM2_THRESHOLD              71
-#define SQR_TOOM3_THRESHOLD             116
-#define SQR_TOOM4_THRESHOLD             336
-
-#define MUL_FFT_TABLE  { 240, 608, 1600, 2816, 7168, 20480, 81920, 196608, 786432, 0 }
-#define MUL_FFT_MODF_THRESHOLD          184
-#define MUL_FFT_THRESHOLD              1664
-
-#define SQR_FFT_TABLE  { 336, 672, 1600, 2816, 7168, 20480, 49152, 196608, 786432, 0 }
-#define SQR_FFT_MODF_THRESHOLD          200
-#define SQR_FFT_THRESHOLD              1536
-
-#define MULLO_BASECASE_THRESHOLD         18
-#define MULLO_DC_THRESHOLD               33
-#define MULLO_MUL_N_THRESHOLD          3152
-
-#define MULMOD_BNM1_THRESHOLD            16
-
-#define DC_DIV_QR_THRESHOLD              20
-#define DC_DIVAPPR_Q_THRESHOLD           77
-#define DC_BDIV_QR_THRESHOLD             38
-#define DC_BDIV_Q_THRESHOLD             120
-#define INV_MULMOD_BNM1_THRESHOLD        66
-#define INV_NEWTON_THRESHOLD             20
-#define INV_APPR_THRESHOLD               12
-#define BINV_NEWTON_THRESHOLD           139
-#define REDC_1_TO_REDC_2_THRESHOLD       10
-#define REDC_2_TO_REDC_N_THRESHOLD      117
-
-#define MATRIX22_STRASSEN_THRESHOLD      12
-#define HGCD_THRESHOLD                   58
-#define GCD_DC_THRESHOLD                283
-#define GCDEXT_DC_THRESHOLD             186
+#define SQR_BASECASE_THRESHOLD            9
+#define SQR_KARATSUBA_THRESHOLD          70
+#define SQR_TOOM3_THRESHOLD             226
+#define SQR_TOOM4_THRESHOLD             345
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              46
+#define MULLOW_MUL_N_THRESHOLD          143
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 34
+#define POWM_THRESHOLD                  116
+
+#define MATRIX22_STRASSEN_THRESHOLD      18
+#define HGCD_THRESHOLD                   51
+#define GCD_DC_THRESHOLD                293
+#define GCDEXT_DC_THRESHOLD             198
 #define JACOBI_BASE_METHOD                3
 
 #define DIVREM_1_NORM_THRESHOLD           3
 #define DIVREM_1_UNNORM_THRESHOLD         3
 #define MOD_1_NORM_THRESHOLD              3
 #define MOD_1_UNNORM_THRESHOLD            3
-#define MOD_1_1_THRESHOLD                 5
-#define MOD_1_2_THRESHOLD             MP_SIZE_T_MAX  /* never */
-#define MOD_1_4_THRESHOLD             MP_SIZE_T_MAX  /* never */
+#define MOD_1_1_THRESHOLD                12
+#define MOD_1_2_THRESHOLD                13
+#define MOD_1_4_THRESHOLD                16
 #define USE_PREINV_DIVREM_1               1
 #define USE_PREINV_MOD_1                  1
-#define DIVREM_2_THRESHOLD                7
+#define DIVREM_2_THRESHOLD                6
 #define DIVEXACT_1_THRESHOLD              0  /* always */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always */
 
-#define GET_STR_DC_THRESHOLD             12
-#define GET_STR_PRECOMPUTE_THRESHOLD     17
-#define SET_STR_DC_THRESHOLD            399
-#define SET_STR_PRECOMPUTE_THRESHOLD   1659
+#define GET_STR_DC_THRESHOLD             13
+#define GET_STR_PRECOMPUTE_THRESHOLD     21
+#define SET_STR_DC_THRESHOLD            638
+#define SET_STR_PRECOMPUTE_THRESHOLD   1889
+
+#define MUL_FFT_TABLE  { 304, 608, 1600, 2816, 7168, 20480, 81920, 196608, 786432, 0 }
+#define MUL_FFT_MODF_THRESHOLD          216
+#define MUL_FFT_THRESHOLD              1664
+
+#define SQR_FFT_TABLE  { 336, 736, 1600, 2816, 7168, 20480, 49152, 196608, 786432, 0 }
+#define SQR_FFT_MODF_THRESHOLD          216
+#define SQR_FFT_THRESHOLD              1312
+
--- 1/mpn/vax/gmp-mparam.h
+++ 2/mpn/vax/gmp-mparam.h
@@ -20,11 +20,11 @@
 /* These numbers were measured manually using the tune/speed program.
    The standard tune/tunup takes too long.  (VAX 8800) */
 
-#define MUL_TOOM22_THRESHOLD             14
-#define MUL_TOOM33_THRESHOLD            110
+#define MUL_KARATSUBA_THRESHOLD          14
+#define MUL_TOOM3_THRESHOLD             110
 
 #define SQR_BASECASE_THRESHOLD            6
-#define SQR_TOOM2_THRESHOLD              42
+#define SQR_KARATSUBA_THRESHOLD          42
 #define SQR_TOOM3_THRESHOLD             250
 
 /* #define DIV_SB_PREINV_THRESHOLD         */
--- 1/mpn/x86/dive_1.asm
+++ 2/mpn/x86/dive_1.asm
@@ -97,7 +97,7 @@
 
 	subl	%edx, %eax		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C expect d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C expect d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	pushl	%eax	FRAME_pushl()
 	imull	PARAM_DIVISOR, %eax
 	cmpl	$1, %eax
--- 1/mpn/x86/fat/gmp-mparam.h
+++ 2/mpn/x86/fat/gmp-mparam.h
@@ -18,7 +18,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -40,9 +40,9 @@
 
 /* Sensible fallbacks for these, when not taken from a cpu-specific
    gmp-mparam.h.  */
-#define MUL_TOOM22_THRESHOLD      20
-#define MUL_TOOM33_THRESHOLD     130
-#define SQR_TOOM2_THRESHOLD       30
+#define MUL_KARATSUBA_THRESHOLD   20
+#define MUL_TOOM3_THRESHOLD      130
+#define SQR_KARATSUBA_THRESHOLD   30
 #define SQR_TOOM3_THRESHOLD      200
 
 /* These are values more or less in the middle of what the typical x86 chips
--- 1/mpn/x86/gmp-mparam.h
+++ 2/mpn/x86/gmp-mparam.h
@@ -17,7 +17,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
--- 1/mpn/x86/i486/gmp-mparam.h
+++ 2/mpn/x86/i486/gmp-mparam.h
@@ -17,7 +17,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -25,11 +25,11 @@
 
 /* Generated by tuneup.c, 2003-02-13, gcc 2.95 */
 
-#define MUL_TOOM22_THRESHOLD             18
-#define MUL_TOOM33_THRESHOLD            228
+#define MUL_KARATSUBA_THRESHOLD          18
+#define MUL_TOOM3_THRESHOLD             228
 
 #define SQR_BASECASE_THRESHOLD           13
-#define SQR_TOOM2_THRESHOLD              49
+#define SQR_KARATSUBA_THRESHOLD          49
 #define SQR_TOOM3_THRESHOLD             238
 
 #define DIV_SB_PREINV_THRESHOLD       MP_SIZE_T_MAX  /* never */
--- 1/mpn/x86/k6/aorsmul_1.asm
+++ 2/mpn/x86/k6/aorsmul_1.asm
@@ -36,7 +36,7 @@
 C K8:
 
 
-dnl  K6:           large multipliers  small multipliers
+dnl  K6:           large multpliers  small multpliers
 dnl  UNROLL_COUNT    cycles/limb       cycles/limb
 dnl        4             9.5              7.78
 dnl        8             9.0              7.78
@@ -247,7 +247,7 @@
 C
 C The add/adc for the initial carry in %esi is necessary only for the
 C mpn_addmul/submul_1c entry points.  Duplicating the startup code to
-C eliminate this for the plain mpn_add/submul_1 doesn't seem like a good
+C eliminiate this for the plain mpn_add/submul_1 doesn't seem like a good
 C idea.
 
 dnl  overlapping with parameters already fetched
--- 1/mpn/x86/k6/gmp-mparam.h
+++ 2/mpn/x86/k6/gmp-mparam.h
@@ -18,7 +18,7 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -26,18 +26,18 @@
 
 /* Generated by tuneup.c, 2009-01-05, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             19
-#define MUL_TOOM33_THRESHOLD             73
+#define MUL_KARATSUBA_THRESHOLD          19
+#define MUL_TOOM3_THRESHOLD              73
 #define MUL_TOOM44_THRESHOLD            104
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              32
+#define SQR_KARATSUBA_THRESHOLD          32
 #define SQR_TOOM3_THRESHOLD             105
 #define SQR_TOOM4_THRESHOLD             143
 
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               64
-#define MULLO_MUL_N_THRESHOLD           232
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              64
+#define MULLOW_MUL_N_THRESHOLD          232
 
 #define DIV_SB_PREINV_THRESHOLD           4
 #define DIV_DC_THRESHOLD                 67
--- 1/mpn/x86/k6/mmx/dive_1.asm
+++ 2/mpn/x86/k6/mmx/dive_1.asm
@@ -117,7 +117,7 @@
 	subl	%ebp, %eax		C inv = 2*inv - inv*inv*d
 	subl	$1, %edx		C shift amount, and clear carry
 
-	ASSERT(e,`	C expect d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C expect d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	pushl	%eax	FRAME_pushl()
 	imull	PARAM_DIVISOR, %eax
 	cmpl	$1, %eax
--- 1/mpn/x86/k6/mode1o.asm
+++ 2/mpn/x86/k6/mode1o.asm
@@ -103,7 +103,7 @@
 
 	subl	%ecx, %edi		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	pushl	%eax
 	movl	%esi, %eax
 	imull	%edi, %eax
--- 1/mpn/x86/k6/sqr_basecase.asm
+++ 2/mpn/x86/k6/sqr_basecase.asm
@@ -25,35 +25,35 @@
 C     which is roughly the Karatsuba recursing range).
 
 
-dnl  SQR_TOOM2_THRESHOLD_MAX is the maximum SQR_TOOM2_THRESHOLD this
+dnl  SQR_KARATSUBA_THRESHOLD_MAX is the maximum SQR_KARATSUBA_THRESHOLD this
 dnl  code supports.  This value is used only by the tune program to know
 dnl  what it can go up to.  (An attempt to compile with a bigger value will
 dnl  trigger some m4_assert()s in the code, making the build fail.)
 dnl
 dnl  The value is determined by requiring the displacements in the unrolled
 dnl  addmul to fit in single bytes.  This means a maximum UNROLL_COUNT of
-dnl  63, giving a maximum SQR_TOOM2_THRESHOLD of 66.
+dnl  63, giving a maximum SQR_KARATSUBA_THRESHOLD of 66.
 
-deflit(SQR_TOOM2_THRESHOLD_MAX, 66)
+deflit(SQR_KARATSUBA_THRESHOLD_MAX, 66)
 
 
 dnl  Allow a value from the tune program to override config.m4.
 
-ifdef(`SQR_TOOM2_THRESHOLD_OVERRIDE',
-`define(`SQR_TOOM2_THRESHOLD',SQR_TOOM2_THRESHOLD_OVERRIDE)')
+ifdef(`SQR_KARATSUBA_THRESHOLD_OVERRIDE',
+`define(`SQR_KARATSUBA_THRESHOLD',SQR_KARATSUBA_THRESHOLD_OVERRIDE)')
 
 
 dnl  UNROLL_COUNT is the number of code chunks in the unrolled addmul.  The
-dnl  number required is determined by SQR_TOOM2_THRESHOLD, since
-dnl  mpn_sqr_basecase only needs to handle sizes < SQR_TOOM2_THRESHOLD.
+dnl  number required is determined by SQR_KARATSUBA_THRESHOLD, since
+dnl  mpn_sqr_basecase only needs to handle sizes < SQR_KARATSUBA_THRESHOLD.
 dnl
 dnl  The first addmul is the biggest, and this takes the second least
 dnl  significant limb and multiplies it by the third least significant and
-dnl  up.  Hence for a maximum operand size of SQR_TOOM2_THRESHOLD-1
-dnl  limbs, UNROLL_COUNT needs to be SQR_TOOM2_THRESHOLD-3.
+dnl  up.  Hence for a maximum operand size of SQR_KARATSUBA_THRESHOLD-1
+dnl  limbs, UNROLL_COUNT needs to be SQR_KARATSUBA_THRESHOLD-3.
 
-m4_config_gmp_mparam(`SQR_TOOM2_THRESHOLD')
-deflit(UNROLL_COUNT, eval(SQR_TOOM2_THRESHOLD-3))
+m4_config_gmp_mparam(`SQR_KARATSUBA_THRESHOLD')
+deflit(UNROLL_COUNT, eval(SQR_KARATSUBA_THRESHOLD-3))
 
 
 C void mpn_sqr_basecase (mp_ptr dst, mp_srcptr src, mp_size_t size);
--- 1/mpn/x86/k7/dive_1.asm
+++ 2/mpn/x86/k7/dive_1.asm
@@ -105,7 +105,7 @@
 
 	subl	%edx, %eax		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C expect d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C expect d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	pushl	%eax	FRAME_pushl()
 	imull	PARAM_DIVISOR, %eax
 	cmpl	$1, %eax
--- 1/mpn/x86/k7/gcd_1.asm
+++ 2/mpn/x86/k7/gcd_1.asm
@@ -1,6 +1,6 @@
 dnl  AMD K7 mpn_gcd_1 -- mpn by 1 gcd.
 
-dnl  Copyright 2000, 2001, 2002, 2009 Free Software Foundation, Inc.
+dnl  Copyright 2000, 2001, 2002 Free Software Foundation, Inc.
 dnl
 dnl  This file is part of the GNU MP Library.
 dnl
@@ -44,8 +44,8 @@
 C it's own cache line or plonked in the middle of the code.  Presumably
 C since TEXT is read-only there's no worries about coherency.
 
+deflit(MASK, 63)
 deflit(MAXSHIFT, 6)
-deflit(MASK, eval((1<<MAXSHIFT)-1))
 
 	TEXT
 	ALIGN(64)
@@ -82,37 +82,37 @@
 	ASSERT(ne, `cmpl $0, PARAM_LIMB')	C y!=0
 	ASSERT(ae, `cmpl $1, PARAM_SIZE')	C size>=1
 
-	mov	PARAM_SRC, %eax
-	mov	PARAM_LIMB, %edx
-	sub	$STACK_SPACE, %esp	deflit(`FRAME',STACK_SPACE)
+	movl	PARAM_SRC, %eax
+	movl	PARAM_LIMB, %edx
+	subl	$STACK_SPACE, %esp	deflit(`FRAME',STACK_SPACE)
 
-	mov	%esi, SAVE_ESI
-	mov	%ebx, SAVE_EBX
+	movl	%esi, SAVE_ESI
+	movl	%ebx, SAVE_EBX
 
-	mov	(%eax), %esi		C src low limb
+	movl	(%eax), %esi		C src low limb
 
 ifdef(`PIC',`
-	mov	%edi, SAVE_EDI
+	movl	%edi, SAVE_EDI
 	call	L(movl_eip_to_edi)
 L(here):
-	add	$L(table)-L(here), %edi
+	addl	$L(table)-L(here), %edi
 ')
 
-	mov	%esi, %ebx
-	or	%edx, %esi	C x|y
-	mov	$-1, %ecx
+	movl	%esi, %ebx
+	orl	%edx, %esi	C x|y
+	movl	$-1, %ecx
 
 L(twos):
-	inc	%ecx
-	shr	%esi
+	incl	%ecx
+	shrl	%esi
 	jnc	L(twos)		C 3/4 chance of x or y odd already
 
-	shr	%cl, %ebx
-	shr	%cl, %edx
-	mov	%ecx, %esi	C common twos
+	shrl	%cl, %ebx
+	shrl	%cl, %edx
+	movl	%ecx, %esi	C common twos
 
-	mov	PARAM_SIZE, %ecx
-	cmp	$1, %ecx
+	movl	PARAM_SIZE, %ecx
+	cmpl	$1, %ecx
 	ja	L(divide)
 
 
@@ -124,8 +124,8 @@
 	C edi	[PIC] L(table)
 	C ebp
 
-	mov	%edx, %eax
-	cmp	%ebx, %edx
+	movl	%edx, %eax
+	cmpl	%ebx, %edx
 
 	cmovb(	%ebx, %eax)	C swap to make x bigger than y
 	cmovb(	%edx, %ebx)
@@ -141,9 +141,9 @@
 	C ebp
 
 	ASSERT(nz,`orl %ebx,%ebx')
-	shr	%ebx
+	shrl	%ebx
 	jnc	L(strip_y)
-	rcl	%ebx
+	rcll	%ebx
 
 
 	C eax	x
@@ -154,22 +154,22 @@
 	C edi	[PIC] L(table)
 	C ebp
 
-	mov	%eax, %ecx
-	mov	%ebx, %edx
-	shr	$DIV_THRESHOLD, %eax
+	movl	%eax, %ecx
+	movl	%ebx, %edx
+	shrl	$DIV_THRESHOLD, %eax
 
-	cmp	%eax, %ebx
-	mov	%ecx, %eax
+	cmpl	%eax, %ebx
+	movl	%ecx, %eax
 	ja	L(strip_x_entry)	C do x%y if x much bigger than y
 
 
-	xor	%edx, %edx
+	xorl	%edx, %edx
 
-	div	%ebx
+	divl	%ebx
 
-	or	%edx, %edx
-	mov	%edx, %ecx		C remainder -> x
-	mov	%ebx, %edx		C y
+	orl	%edx, %edx
+	movl	%edx, %eax		C remainder -> x
+	movl	%ebx, %edx		C y
 
 	jz	L(done_ebx)
 	jmp	L(strip_x)
@@ -194,43 +194,43 @@
 	cmovc(	%eax, %edx)
 
 L(strip_x):
-	mov	%ecx, %eax
+	movl	%ecx, %eax
 L(strip_x_entry):
-	and	$MASK, %ecx
+	andl	$MASK, %ecx
 
 	ASSERT(nz, `orl %eax, %eax')
 
 ifdef(`PIC',`
-	mov	(%ecx,%edi), %cl
+	movb	(%ecx,%edi), %cl
 ',`
-	mov	L(table) (%ecx), %cl
+	movb	L(table) (%ecx), %cl
 ')
 
-	shr	%cl, %eax
-	cmp	$MAXSHIFT, %cl
+	shrl	%cl, %eax
+	cmpb	$MAXSHIFT, %cl
 
-	mov	%eax, %ecx
-	mov	%edx, %ebx
+	movl	%eax, %ecx
+	movl	%edx, %ebx
 	je	L(strip_x)
 
-	ASSERT(nz, `test $1, %eax')	C both odd
-	ASSERT(nz, `test $1, %edx')
+	ASSERT(nz, `testl $1, %eax')	C both odd
+	ASSERT(nz, `testl $1, %edx')
 
-	sub	%eax, %ebx
-	sub	%edx, %ecx
+	subl	%eax, %ebx
+	subl	%edx, %ecx
 	jnz	L(top)
 
 
 L(done):
-	mov	%esi, %ecx
-	mov	SAVE_ESI, %esi
+	movl	%esi, %ecx
+	movl	SAVE_ESI, %esi
 ifdef(`PIC',`
-	mov	SAVE_EDI, %edi
+	movl	SAVE_EDI, %edi
 ')
 
-	shl	%cl, %eax
-	mov	SAVE_EBX, %ebx
-	add	$FRAME, %esp
+	shll	%cl, %eax
+	movl	SAVE_EBX, %ebx
+	addl	$FRAME, %esp
 
 	ret
 
@@ -254,23 +254,23 @@
 	C ebp
 
 L(divide_strip_y):
-	ASSERT(nz,`or %edx,%edx')
-	shr	%edx
+	ASSERT(nz,`orl %edx,%edx')
+	shrl	%edx
 	jnc	L(divide_strip_y)
-	lea	1(%edx,%edx), %ebx		C y now odd
+	leal	1(%edx,%edx), %ebx		C y now odd
 
-	mov	%ebp, SAVE_EBP
-	mov	%eax, %ebp
-	mov	-4(%eax,%ecx,4), %eax		C src high limb
+	movl	%ebp, SAVE_EBP
+	movl	%eax, %ebp
+	movl	-4(%eax,%ecx,4), %eax		C src high limb
 
 	cmp	$MODEXACT_THRESHOLD, %ecx
 	jae	L(modexact)
 
-	cmp	%ebx, %eax			C high cmp divisor
-	mov	$0, %edx
+	cmpl	%ebx, %eax			C high cmp divisor
+	movl	$0, %edx
 
 	cmovc(	%eax, %edx)			C skip a div if high<divisor
-	sbb	$0, %ecx
+	sbbl	$0, %ecx
 
 
 L(divide_top):
@@ -282,11 +282,11 @@
 	C edi	[PIC] L(table)
 	C ebp	src
 
-	mov	-4(%ebp,%ecx,4), %eax
+	movl	-4(%ebp,%ecx,4), %eax
 
-	div	%ebx
+	divl	%ebx
 
-	dec	%ecx
+	decl	%ecx
 	jnz	L(divide_top)
 
 
@@ -298,17 +298,17 @@
 	C edi	[PIC] L(table)
 	C ebp
 
-	or	%edx, %edx
-	mov	SAVE_EBP, %ebp
-	mov	%edx, %eax
+	orl	%edx, %edx
+	movl	SAVE_EBP, %ebp
+	movl	%edx, %eax
 
-	mov	%edx, %ecx
-	mov	%ebx, %edx
+	movl	%edx, %ecx
+	movl	%ebx, %edx
 	jnz	L(strip_x_entry)
 
 
 L(done_ebx):
-	mov	%ebx, %eax
+	movl	%ebx, %eax
 	jmp	L(done)
 
 
@@ -323,20 +323,20 @@
 	C ebp	src
 
 ifdef(`PIC',`
-	mov	%ebp, CALL_SRC
-	mov	%ebx, %ebp		C y
-	mov	%edi, %ebx		C L(table)
-
-	add	$_GLOBAL_OFFSET_TABLE_+[.-L(table)], %ebx
-	mov	%ebp, CALL_DIVISOR
-	mov	%ecx, CALL_SIZE
+	movl	%ebp, CALL_SRC
+	movl	%ebx, %ebp		C y
+	movl	%edi, %ebx		C L(table)
+
+	addl	$_GLOBAL_OFFSET_TABLE_+[.-L(table)], %ebx
+	movl	%ebp, CALL_DIVISOR
+	movl	%ecx, CALL_SIZE
 
 	call	GSYM_PREFIX`'mpn_modexact_1_odd@PLT
 ',`
 dnl non-PIC
-	mov	%ebx, CALL_DIVISOR
-	mov	%ebp, CALL_SRC
-	mov	%ecx, CALL_SIZE
+	movl	%ebx, CALL_DIVISOR
+	movl	%ebp, CALL_SRC
+	movl	%ecx, CALL_SIZE
 
 	call	GSYM_PREFIX`'mpn_modexact_1_odd
 ')
@@ -349,20 +349,20 @@
 	C edi	[PIC] L(table)
 	C ebp	[PIC] y
 
-	or	%eax, %eax
-	mov	ifdef(`PIC',`%ebp',`%ebx'), %edx
-	mov	SAVE_EBP, %ebp
+	orl	%eax, %eax
+	movl	ifdef(`PIC',`%ebp',`%ebx'), %edx
+	movl	SAVE_EBP, %ebp
 
-	mov	%eax, %ecx
+	movl	%eax, %ecx
 	jnz	L(strip_x_entry)
 
-	mov	%edx, %eax
+	movl	%edx, %eax
 	jmp	L(done)
 
 
 ifdef(`PIC', `
 L(movl_eip_to_edi):
-	mov	(%esp), %edi
+	movl	(%esp), %edi
 	ret_internal
 ')
 
--- 1/mpn/x86/k7/gmp-mparam.h
+++ 2/mpn/x86/k7/gmp-mparam.h
@@ -1,7 +1,7 @@
 /* AMD K7 gmp-mparam.h -- Compiler/machine parameter header file.
 
-Copyright 1991, 1993, 1994, 2000, 2001, 2002, 2003, 2004, 2005, 2008, 2009
-Free Software Foundation, Inc.
+Copyright 1991, 1993, 1994, 2000, 2001, 2002, 2003, 2004, 2005, 2008 Free
+Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -18,67 +18,53 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
-/* 1883 MHz Athlon */
+/* 2083 MHz Athlon */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.2 */
+/* Generated by tuneup.c, 2008-12-23, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             28
-#define MUL_TOOM33_THRESHOLD             85
-#define MUL_TOOM44_THRESHOLD            147
+#define MUL_KARATSUBA_THRESHOLD          28
+#define MUL_TOOM3_THRESHOLD              89
+#define MUL_TOOM44_THRESHOLD            130
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              50
-#define SQR_TOOM3_THRESHOLD              87
-#define SQR_TOOM4_THRESHOLD             216
-
-#define MUL_FFT_TABLE  { 432, 800, 1408, 3584, 10240, 40960, 163840, 655360, 0 }
-#define MUL_FFT_MODF_THRESHOLD          656
-#define MUL_FFT_THRESHOLD              7680
-
-#define SQR_FFT_TABLE  { 432, 800, 1408, 3584, 10240, 24576, 229376, 655360, 0 }
-#define SQR_FFT_MODF_THRESHOLD          560
-#define SQR_FFT_THRESHOLD              3840
-
-#define MULLO_BASECASE_THRESHOLD         10
-#define MULLO_DC_THRESHOLD               56
-#define MULLO_MUL_N_THRESHOLD         11138
-
-#define MULMOD_BNM1_THRESHOLD            18
-
-#define DC_DIV_QR_THRESHOLD              66
-#define DC_DIVAPPR_Q_THRESHOLD          357
-#define DC_BDIV_QR_THRESHOLD             71
-#define DC_BDIV_Q_THRESHOLD             260
-#define INV_MULMOD_BNM1_THRESHOLD        86
-#define INV_NEWTON_THRESHOLD            202
-#define INV_APPR_THRESHOLD               19
-#define BINV_NEWTON_THRESHOLD           246
-#define REDC_1_TO_REDC_N_THRESHOLD       85
-
-#define MATRIX22_STRASSEN_THRESHOLD      17
-#define HGCD_THRESHOLD                  166
-#define GCD_DC_THRESHOLD                599
-#define GCDEXT_DC_THRESHOLD             435
+#define SQR_KARATSUBA_THRESHOLD          52
+#define SQR_TOOM3_THRESHOLD              89
+#define SQR_TOOM4_THRESHOLD             196
+
+#define MULLOW_BASECASE_THRESHOLD        10
+#define MULLOW_DC_THRESHOLD              96
+#define MULLOW_MUL_N_THRESHOLD          234
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 86
+#define POWM_THRESHOLD                  134
+#define MATRIX22_STRASSEN_THRESHOLD      18
+#define HGCD_THRESHOLD                  163
+#define GCD_DC_THRESHOLD                665
+#define GCDEXT_DC_THRESHOLD             605
 #define JACOBI_BASE_METHOD                1
 
-#define MOD_1_NORM_THRESHOLD              0  /* always */
-#define MOD_1_UNNORM_THRESHOLD            3
-#define MOD_1_1_THRESHOLD                 6
-#define MOD_1_2_THRESHOLD                 7
-#define MOD_1_4_THRESHOLD                18
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1  /* native */
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             15
-#define GET_STR_PRECOMPUTE_THRESHOLD     32
-#define SET_STR_DC_THRESHOLD            537
-#define SET_STR_PRECOMPUTE_THRESHOLD   1576
+#define GET_STR_DC_THRESHOLD             19
+#define GET_STR_PRECOMPUTE_THRESHOLD     35
+#define SET_STR_DC_THRESHOLD            826
+#define SET_STR_PRECOMPUTE_THRESHOLD   1691
+
+#define MUL_FFT_TABLE  { 432, 864, 1664, 4608, 10240, 40960, 163840, 655360, 0 }
+#define MUL_FFT_MODF_THRESHOLD          496
+#define MUL_FFT_THRESHOLD              4864
+
+#define SQR_FFT_TABLE  { 432, 864, 1664, 4608, 10240, 40960, 98304, 655360, 0 }
+#define SQR_FFT_MODF_THRESHOLD          432
+#define SQR_FFT_THRESHOLD              3840
 
 /* These tables need to be updated.  */
 
--- 1/mpn/x86/k7/mode1o.asm
+++ 2/mpn/x86/k7/mode1o.asm
@@ -111,7 +111,7 @@
 
 	subl	%eax, %edi		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	movl	%esi, %eax
 	imull	%edi, %eax
 	cmpl	$1, %eax')
--- 1/mpn/x86/k7/sqr_basecase.asm
+++ 2/mpn/x86/k7/sqr_basecase.asm
@@ -28,18 +28,18 @@
 dnl  These are the same as mpn/x86/k6/sqr_basecase.asm, see that code for
 dnl  some comments.
 
-deflit(SQR_TOOM2_THRESHOLD_MAX, 66)
+deflit(SQR_KARATSUBA_THRESHOLD_MAX, 66)
 
-ifdef(`SQR_TOOM2_THRESHOLD_OVERRIDE',
-`define(`SQR_TOOM2_THRESHOLD',SQR_TOOM2_THRESHOLD_OVERRIDE)')
+ifdef(`SQR_KARATSUBA_THRESHOLD_OVERRIDE',
+`define(`SQR_KARATSUBA_THRESHOLD',SQR_KARATSUBA_THRESHOLD_OVERRIDE)')
 
-m4_config_gmp_mparam(`SQR_TOOM2_THRESHOLD')
-deflit(UNROLL_COUNT, eval(SQR_TOOM2_THRESHOLD-3))
+m4_config_gmp_mparam(`SQR_KARATSUBA_THRESHOLD')
+deflit(UNROLL_COUNT, eval(SQR_KARATSUBA_THRESHOLD-3))
 
 
 C void mpn_sqr_basecase (mp_ptr dst, mp_srcptr src, mp_size_t size);
 C
-C With a SQR_TOOM2_THRESHOLD around 50 this code is about 1500 bytes,
+C With a SQR_KARATSUBA_THRESHOLD around 50 this code is about 1500 bytes,
 C which is quite a bit, but is considered good value since squares big
 C enough to use most of the code will be spending quite a few cycles in it.
 
--- 1/mpn/x86/p6/aorsmul_1.asm
+++ 2/mpn/x86/p6/aorsmul_1.asm
@@ -170,7 +170,7 @@
 C
 C The add/adc for the initial carry in %ebx is necessary only for the
 C mpn_add/submul_1c entry points.  Duplicating the startup code to
-C eliminate this for the plain mpn_add/submul_1 doesn't seem like a good
+C eliminiate this for the plain mpn_add/submul_1 doesn't seem like a good
 C idea.
 
 dnl  overlapping with parameters already fetched
--- 1/mpn/x86/p6/aors_n.asm
+++ 2/mpn/x86/p6/aors_n.asm
@@ -20,7 +20,7 @@
 include(`../config.m4')
 
 C TODO:
-C  * Avoid indexed addressing, it makes us stall on the two-ported register
+C  * Avoid indexed adressing, it makes us stall on the two-ported register
 C    file.
 
 C                           cycles/limb
--- 1/mpn/x86/p6/dive_1.asm
+++ 2/mpn/x86/p6/dive_1.asm
@@ -106,7 +106,7 @@
 
 	subl	%eax, %ebp		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	movl	PARAM_DIVISOR, %eax
 	imull	%ebp, %eax
 	cmpl	$1, %eax')
--- 1/mpn/x86/p6/gmp-mparam.h
+++ 2/mpn/x86/p6/gmp-mparam.h
@@ -19,25 +19,25 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
-/* NOTE: In a fat binary build SQR_TOOM2_THRESHOLD here cannot be smaller than
-   the value in mpn/x86/p6/mmx/gmp-mparam.h.  The former is used as a hard
-   limit in mpn/x86/p6/sqr_basecase.asm, and that file will be run by the
-   p6/mmx cpus (pentium2, pentium3).  */
+/* NOTE: In a fat binary build SQR_KARATSUBA_THRESHOLD here cannot be
+   smaller than the value in mpn/x86/p6/mmx/gmp-mparam.h.  The former is
+   used as a hard limit in mpn/x86/p6/sqr_basecase.asm, and that file will
+   be run by the p6/mmx cpus (pentium2, pentium3).  */
 
 
 /* 200MHz Pentium Pro */
 
 /* Generated by tuneup.c, 2003-02-12, gcc 2.95 */
 
-#define MUL_TOOM22_THRESHOLD             23
-#define MUL_TOOM33_THRESHOLD            140
+#define MUL_KARATSUBA_THRESHOLD          23
+#define MUL_TOOM3_THRESHOLD             140
 
 #define SQR_BASECASE_THRESHOLD            0  /* always */
-#define SQR_TOOM2_THRESHOLD              52
+#define SQR_KARATSUBA_THRESHOLD          52
 #define SQR_TOOM3_THRESHOLD             189
 
 #define DIV_SB_PREINV_THRESHOLD           0  /* always */
--- 1/mpn/x86/p6/mmx/gmp-mparam.h
+++ 2/mpn/x86/p6/mmx/gmp-mparam.h
@@ -19,72 +19,59 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
-/* NOTE: In a fat binary build SQR_TOOM2_THRESHOLD here cannot be more than the
-   value in mpn/x86/p6/gmp-mparam.h.  The latter is used as a hard limit in
-   mpn/x86/p6/sqr_basecase.asm.  */
+/* NOTE: In a fat binary build SQR_KARATSUBA_THRESHOLD here cannot be more
+   than the value in mpn/x86/p6/gmp-mparam.h.  The latter is used as a hard
+   limit in mpn/x86/p6/sqr_basecase.asm.  */
 
 
-/* 800 MHz P6 model 8 */
+/* 1867 MHz P6 model 13 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 3.4 */
+/* Generated by tuneup.c, 2009-03-02, gcc 4.3 */
 
-#define MUL_TOOM22_THRESHOLD             22
-#define MUL_TOOM33_THRESHOLD             73
-#define MUL_TOOM44_THRESHOLD            184
+#define MUL_KARATSUBA_THRESHOLD          20
+#define MUL_TOOM3_THRESHOLD              74
+#define MUL_TOOM44_THRESHOLD            166
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              48
-#define SQR_TOOM3_THRESHOLD              81
-#define SQR_TOOM4_THRESHOLD             136
-
-#define MUL_FFT_TABLE  { 368, 800, 1408, 3584, 10240, 40960, 163840, 393216, 0 }
-#define MUL_FFT_MODF_THRESHOLD          400
-#define MUL_FFT_THRESHOLD              5632
-
-#define SQR_FFT_TABLE  { 336, 800, 1408, 3584, 10240, 40960, 163840, 393216, 0 }
-#define SQR_FFT_MODF_THRESHOLD          368
-#define SQR_FFT_THRESHOLD              3328
-
-#define MULLO_BASECASE_THRESHOLD          7
-#define MULLO_DC_THRESHOLD               62
-#define MULLO_MUL_N_THRESHOLD         11138
-
-#define MULMOD_BNM1_THRESHOLD            15
-
-#define DC_DIV_QR_THRESHOLD              76
-#define DC_DIVAPPR_Q_THRESHOLD          244
-#define DC_BDIV_QR_THRESHOLD             76
-#define DC_BDIV_Q_THRESHOLD             168
-#define INV_MULMOD_BNM1_THRESHOLD        62
-#define INV_NEWTON_THRESHOLD            264
-#define INV_APPR_THRESHOLD               19
-#define BINV_NEWTON_THRESHOLD           294
-#define REDC_1_TO_REDC_N_THRESHOLD       74
-
-#define MATRIX22_STRASSEN_THRESHOLD      16
-#define HGCD_THRESHOLD                  115
-#define GCD_DC_THRESHOLD                505
-#define GCDEXT_DC_THRESHOLD             354
+#define SQR_KARATSUBA_THRESHOLD          30
+#define SQR_TOOM3_THRESHOLD             101
+#define SQR_TOOM4_THRESHOLD             154
+
+#define MULLOW_BASECASE_THRESHOLD         7
+#define MULLOW_DC_THRESHOLD              39
+#define MULLOW_MUL_N_THRESHOLD          230
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 21
+#define POWM_THRESHOLD                  154
+
+#define MATRIX22_STRASSEN_THRESHOLD      23
+#define HGCD_THRESHOLD                   72
+#define GCD_DC_THRESHOLD                321
+#define GCDEXT_DC_THRESHOLD             416
 #define JACOBI_BASE_METHOD                1
 
-#define MOD_1_NORM_THRESHOLD              4
-#define MOD_1_UNNORM_THRESHOLD            5
-#define MOD_1_1_THRESHOLD                 8
-#define MOD_1_2_THRESHOLD                 9
-#define MOD_1_4_THRESHOLD                12
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  1
+#define USE_PREINV_MOD_1                  1  /* native */
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
 #define GET_STR_DC_THRESHOLD             15
-#define GET_STR_PRECOMPUTE_THRESHOLD     27
-#define SET_STR_DC_THRESHOLD            272
-#define SET_STR_PRECOMPUTE_THRESHOLD   1183
+#define GET_STR_PRECOMPUTE_THRESHOLD     24
+#define SET_STR_DC_THRESHOLD            587
+#define SET_STR_PRECOMPUTE_THRESHOLD   1083
+
+#define MUL_FFT_TABLE  { 400, 928, 1664, 4608, 10240, 57344, 163840, 393216, 0 }
+#define MUL_FFT_MODF_THRESHOLD          496
+#define MUL_FFT_THRESHOLD              7168
+
+#define SQR_FFT_TABLE  { 432, 928, 1664, 3584, 10240, 40960, 98304, 393216, 0 }
+#define SQR_FFT_MODF_THRESHOLD          448
+#define SQR_FFT_THRESHOLD              3840
 
 /* These tables need updating */
 #define MUL_FFT_TABLE2 {{1,4}, {305,5}, {321,4}, {337,5}, {353,4}, {369,5}, {801,6}, {833,5}, {865,6}, {897,5}, {929,6}, {961,5}, {993,6}, {1345,7}, {1409,6}, {1537,7}, {1665,6}, {1729,7}, {2689,8}, {2817,7}, {3201,8}, {3329,7}, {3457,8}, {3841,7}, {3969,8}, {4097,7}, {4225,8}, {4353,7}, {4481,8}, {5889,7}, {6017,8}, {6401,7}, {6529,8}, {6913,9}, {7681,8}, {8961,9}, {9729,8}, {9985,9}, {10241,8}, {11009,9}, {11777,8}, {12289,9}, {13825,10}, {15361,9}, {15873,8}, {16129,9}, {19969,10}, {23553,9}, {24065,8}, {24321,9}, {26113,10}, {27649,11}, {28673,10}, {31745,9}, {34305,10}, {34817,9}, {35329,10}, {39937,9}, {40449,10}, {48129,11}, {55297,10}, {56321,11}, {63489,10}, {80897,11}, {96257,10}, {97281,12}, {126977,11}, {129025,10}, {130049,9}, {130561,10}, {131073,11}, {133121,10}, {134145,11}, {137217,10}, {138241,11}, {161793,10}, {162817,11}, {194561,12}, {258049,11}, {260097,10}, {261121,9}, {261633,10}, {266241,11}, {268289,10}, {277505,11}, {292865,10}, {293889,9}, {294401,10}, {310273,9}, {310785,11}, {325633,10}, {326657,12}, {389121,13}, {516097,12}, {520193,11}, {522241,10}, {523265,11}, {555009,10}, {556033,11}, {587777,10}, {588801,11}, {620545,10}, {621569,9}, {622081,11}, {622593,12}, {651265,11}, {653313,10}, {654337,11}, {655361,10}, {657409,11}, {663553,10}, {664577,11}, {686081,10}, {687105,11}, {718849,10}, {719873,11}, {720897,10}, {722945,11}, {737281,10}, {740353,11}, {745473,10}, {749569,11}, {751617,10}, {752641,9}, {753153,11}, {753665,12}, {770049,11}, {774145,12}, {782337,11}, {786433,10}, {787457,11}, {817153,10}, {818177,11}, {849921,10}, {850945,11}, {854017,10}, {855041,11}, {862209,10}, {863233,11}, {866305,10}, {867329,11}, {876545,10}, {877569,11}, {882689,10}, {883713,9}, {884225,11}, {884737,13}, {1040385,12}, {1044481,11}, {1112065,10}, {1113089,12}, {1175553,11}, {1243137,12}, {1306625,11}, {1374209,10}, {1375233,12}, {1437697,11}, {1505281,10}, {1506305,12}, {1515521,13}, {1523713,12}, {1527809,13}, {1540097,12}, {1544193,13}, {1548289,12}, {1568769,11}, {1636353,10}, {1637377,12}, {1699841,11}, {MP_SIZE_T_MAX,0}}
--- 1/mpn/x86/p6/mode1o.asm
+++ 2/mpn/x86/p6/mode1o.asm
@@ -101,7 +101,7 @@
 
 	subl	%eax, %edi		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	movl	PARAM_DIVISOR, %eax
 	imull	%edi, %eax
 	cmpl	$1, %eax')
--- 1/mpn/x86/p6/sqr_basecase.asm
+++ 2/mpn/x86/p6/sqr_basecase.asm
@@ -27,15 +27,15 @@
 
 dnl  These are the same as in mpn/x86/k6/sqr_basecase.asm, see that file for
 dnl  a description.  The only difference here is that UNROLL_COUNT can go up
-dnl  to 64 (not 63) making SQR_TOOM2_THRESHOLD_MAX 67.
+dnl  to 64 (not 63) making SQR_KARATSUBA_THRESHOLD_MAX 67.
 
-deflit(SQR_TOOM2_THRESHOLD_MAX, 67)
+deflit(SQR_KARATSUBA_THRESHOLD_MAX, 67)
 
-ifdef(`SQR_TOOM2_THRESHOLD_OVERRIDE',
-`define(`SQR_TOOM2_THRESHOLD',SQR_TOOM2_THRESHOLD_OVERRIDE)')
+ifdef(`SQR_KARATSUBA_THRESHOLD_OVERRIDE',
+`define(`SQR_KARATSUBA_THRESHOLD',SQR_KARATSUBA_THRESHOLD_OVERRIDE)')
 
-m4_config_gmp_mparam(`SQR_TOOM2_THRESHOLD')
-deflit(UNROLL_COUNT, eval(SQR_TOOM2_THRESHOLD-3))
+m4_config_gmp_mparam(`SQR_KARATSUBA_THRESHOLD')
+deflit(UNROLL_COUNT, eval(SQR_KARATSUBA_THRESHOLD-3))
 
 
 C void mpn_sqr_basecase (mp_ptr dst, mp_srcptr src, mp_size_t size);
--- 1/mpn/x86/p6/sse2/gmp-mparam.h
+++ 2/mpn/x86/p6/sse2/gmp-mparam.h
@@ -19,69 +19,56 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
-/* NOTE: In a fat binary build SQR_TOOM2_THRESHOLD here cannot be more than the
-   value in mpn/x86/p6/gmp-mparam.h.  The latter is used as a hard limit in
-   mpn/x86/p6/sqr_basecase.asm.  */
+/* NOTE: In a fat binary build SQR_KARATSUBA_THRESHOLD here cannot be more
+   than the value in mpn/x86/p6/gmp-mparam.h.  The latter is used as a hard
+   limit in mpn/x86/p6/sqr_basecase.asm.  */
 
 
 /* 1867 MHz P6 model 13 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.3 */
+/* Generated by tuneupc, 2008-10-30, gcc 4.3 */
 
-#define MUL_TOOM22_THRESHOLD             20
-#define MUL_TOOM33_THRESHOLD             77
-#define MUL_TOOM44_THRESHOLD            182
+#define MUL_KARATSUBA_THRESHOLD          20
+#define MUL_TOOM3_THRESHOLD              77
+#define MUL_TOOM44_THRESHOLD            142
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              30
-#define SQR_TOOM3_THRESHOLD             102
+#define SQR_KARATSUBA_THRESHOLD          30
+#define SQR_TOOM3_THRESHOLD             101
 #define SQR_TOOM4_THRESHOLD             154
 
-#define MUL_FFT_TABLE  { 400, 928, 1664, 4608, 10240, 40960, 163840, 393216, 0 }
-#define MUL_FFT_MODF_THRESHOLD          496
-#define MUL_FFT_THRESHOLD              7168
-
-#define SQR_FFT_TABLE  { 400, 928, 1664, 3584, 10240, 40960, 98304, 655360, 0 }
-#define SQR_FFT_MODF_THRESHOLD          496
-#define SQR_FFT_THRESHOLD              3840
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               40
-#define MULLO_MUL_N_THRESHOLD         11138
-
-#define MULMOD_BNM1_THRESHOLD            13
-
-#define DC_DIV_QR_THRESHOLD              20
-#define DC_DIVAPPR_Q_THRESHOLD           55
-#define DC_BDIV_QR_THRESHOLD             60
-#define DC_BDIV_Q_THRESHOLD             132
-#define INV_MULMOD_BNM1_THRESHOLD        78
-#define INV_NEWTON_THRESHOLD             78
-#define INV_APPR_THRESHOLD                5
-#define BINV_NEWTON_THRESHOLD           274
-#define REDC_1_TO_REDC_N_THRESHOLD       62
-
-#define MATRIX22_STRASSEN_THRESHOLD      17
-#define HGCD_THRESHOLD                   71
-#define GCD_DC_THRESHOLD                386
-#define GCDEXT_DC_THRESHOLD             293
+#define MULLOW_BASECASE_THRESHOLD         4
+#define MULLOW_DC_THRESHOLD              38
+#define MULLOW_MUL_N_THRESHOLD          234
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 24
+#define POWM_THRESHOLD                  150
+
+#define MATRIX22_STRASSEN_THRESHOLD      23
+#define HGCD_THRESHOLD                   95
+#define GCD_DC_THRESHOLD                381
+#define GCDEXT_DC_THRESHOLD             419
 #define JACOBI_BASE_METHOD                1
 
-#define MOD_1_NORM_THRESHOLD              3
-#define MOD_1_UNNORM_THRESHOLD            5
-#define MOD_1_1_THRESHOLD                 5
-#define MOD_1_2_THRESHOLD                 7
-#define MOD_1_4_THRESHOLD                 8
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  1
+#define USE_PREINV_MOD_1                  1  /* native */
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             13
-#define GET_STR_PRECOMPUTE_THRESHOLD     20
-#define SET_STR_DC_THRESHOLD            582
-#define SET_STR_PRECOMPUTE_THRESHOLD   1076
+#define GET_STR_DC_THRESHOLD             14
+#define GET_STR_PRECOMPUTE_THRESHOLD     24
+#define SET_STR_DC_THRESHOLD            276
+#define SET_STR_PRECOMPUTE_THRESHOLD   1078
+
+#define MUL_FFT_TABLE  { 400, 928, 1664, 3584, 10240, 40960, 98304, 393216, 1572864, 0 }
+#define MUL_FFT_MODF_THRESHOLD          496
+#define MUL_FFT_THRESHOLD              7168
+
+#define SQR_FFT_TABLE  { 432, 928, 1664, 3584, 10240, 40960, 98304, 393216, 1572864, 0 }
+#define SQR_FFT_MODF_THRESHOLD          448
+#define SQR_FFT_THRESHOLD              3840
--- 1/mpn/x86/pentium/dive_1.asm
+++ 2/mpn/x86/pentium/dive_1.asm
@@ -146,7 +146,7 @@
 
 	negl	%ebx			C -size
 
-	ASSERT(e,`	C expect d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C expect d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	pushl	%eax	FRAME_pushl()
 	imull	PARAM_DIVISOR, %eax
 	cmpl	$1, %eax
--- 1/mpn/x86/pentium/gmp-mparam.h
+++ 2/mpn/x86/pentium/gmp-mparam.h
@@ -19,7 +19,7 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -31,11 +31,11 @@
 
 /* Generated by tuneup.c, 2004-02-10, gcc 2.95 */
 
-#define MUL_TOOM22_THRESHOLD             16
-#define MUL_TOOM33_THRESHOLD             90
+#define MUL_KARATSUBA_THRESHOLD          16
+#define MUL_TOOM3_THRESHOLD              90
 
 #define SQR_BASECASE_THRESHOLD            0  /* always */
-#define SQR_TOOM2_THRESHOLD              22
+#define SQR_KARATSUBA_THRESHOLD          22
 #define SQR_TOOM3_THRESHOLD             122
 
 #define DIV_SB_PREINV_THRESHOLD       MP_SIZE_T_MAX  /* never */
--- 1/mpn/x86/pentium/mmx/gmp-mparam.h
+++ 2/mpn/x86/pentium/mmx/gmp-mparam.h
@@ -19,7 +19,7 @@
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
@@ -31,18 +31,18 @@
 
 /* Generated by tuneup.c, 2009-01-06, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             16
-#define MUL_TOOM33_THRESHOLD             89
+#define MUL_KARATSUBA_THRESHOLD          16
+#define MUL_TOOM3_THRESHOLD              89
 #define MUL_TOOM44_THRESHOLD            131
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              22
+#define SQR_KARATSUBA_THRESHOLD          22
 #define SQR_TOOM3_THRESHOLD              77
 #define SQR_TOOM4_THRESHOLD             168
 
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               40
-#define MULLO_MUL_N_THRESHOLD           266
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              40
+#define MULLOW_MUL_N_THRESHOLD          266
 
 #define DIV_SB_PREINV_THRESHOLD           4
 #define DIV_DC_THRESHOLD                 43
--- 1/mpn/x86/pentium/mode1o.asm
+++ 2/mpn/x86/pentium/mode1o.asm
@@ -122,7 +122,7 @@
 	subl	%eax, %ecx		C inv = 2*inv - inv*inv*d
 	pushl	%esi		FRAME_pushl()
 
-	ASSERT(e,`	C d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	movl	%ecx, %eax
 	imull	PARAM_DIVISOR, %eax
 	cmpl	$1, %eax')
--- 1/mpn/x86/pentium4/sse2/dive_1.asm
+++ 2/mpn/x86/pentium4/sse2/dive_1.asm
@@ -126,7 +126,7 @@
 
 	psubd	%mm0, %mm5		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C expect d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C expect d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	pushl	%eax	FRAME_pushl()
 	movq	%mm6, %mm0
 	pmuludq	%mm5, %mm0
--- 1/mpn/x86/pentium4/sse2/gmp-mparam.h
+++ 2/mpn/x86/pentium4/sse2/gmp-mparam.h
@@ -18,64 +18,51 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 32
+#define BITS_PER_MP_LIMB 32
 #define BYTES_PER_MP_LIMB 4
 
 
-/* 2600 MHz Pentium 4 model 2 (bonk.sics.se) */
+/* 2600 MHz Pentium 4 model 2 */
 
-/* Generated by tuneup.c, 2009-11-28, gcc 4.0 */
+/* Generated by tuneup.c, 2009-01-06, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             31
-#define MUL_TOOM33_THRESHOLD            109
-#define MUL_TOOM44_THRESHOLD            292
+#define MUL_KARATSUBA_THRESHOLD          31
+#define MUL_TOOM3_THRESHOLD             119
+#define MUL_TOOM44_THRESHOLD            178
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              49
-#define SQR_TOOM3_THRESHOLD             171
-#define SQR_TOOM4_THRESHOLD             458
+#define SQR_KARATSUBA_THRESHOLD          49
+#define SQR_TOOM3_THRESHOLD             165
+#define SQR_TOOM4_THRESHOLD             252
+
+#define MULLOW_BASECASE_THRESHOLD        15
+#define MULLOW_DC_THRESHOLD              44
+#define MULLOW_MUL_N_THRESHOLD          363
+
+#define DIV_SB_PREINV_THRESHOLD       MP_SIZE_T_MAX  /* never */
+#define DIV_DC_THRESHOLD                 33
+#define POWM_THRESHOLD                   95
+
+#define MATRIX22_STRASSEN_THRESHOLD      23
+#define HGCD_THRESHOLD                   64
+#define GCD_DC_THRESHOLD                310
+#define GCDEXT_DC_THRESHOLD             310
+#define JACOBI_BASE_METHOD                1
 
-#define MUL_FFT_TABLE  { 528, 1184, 1664, 4608, 14336, 40960, 229376, 655360, 0 }
-#define MUL_FFT_MODF_THRESHOLD          592
-#define MUL_FFT_THRESHOLD              9216
-
-#define SQR_FFT_TABLE  { 496, 1184, 1920, 5632, 14336, 40960, 163840, 655360, 0 }
-#define SQR_FFT_MODF_THRESHOLD          512
-#define SQR_FFT_THRESHOLD              4864
-
-#define MULLO_BASECASE_THRESHOLD         12
-#define MULLO_DC_THRESHOLD               51
-#define MULLO_MUL_N_THRESHOLD         15896
-
-#define MULMOD_BNM1_THRESHOLD            19
-
-#define DC_DIV_QR_THRESHOLD              28
-#define DC_DIVAPPR_Q_THRESHOLD           73
-#define DC_BDIV_QR_THRESHOLD             54
-#define DC_BDIV_Q_THRESHOLD              88
-#define INV_MULMOD_BNM1_THRESHOLD        61
-#define INV_NEWTON_THRESHOLD            196
-#define INV_APPR_THRESHOLD                9
-#define BINV_NEWTON_THRESHOLD           375
-#define REDC_1_TO_REDC_N_THRESHOLD       65
-
-#define MATRIX22_STRASSEN_THRESHOLD      29
-#define HGCD_THRESHOLD                   68
-#define GCD_DC_THRESHOLD                283
-#define GCDEXT_DC_THRESHOLD             237
-#define JACOBI_BASE_METHOD                2
-
-#define MOD_1_NORM_THRESHOLD             29
-#define MOD_1_UNNORM_THRESHOLD        MP_SIZE_T_MAX  /* never */
-#define MOD_1_1_THRESHOLD                12
-#define MOD_1_2_THRESHOLD                17
-#define MOD_1_4_THRESHOLD                18
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  1
+#define USE_PREINV_MOD_1                  1  /* native */
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             12
-#define GET_STR_PRECOMPUTE_THRESHOLD     24
+#define GET_STR_DC_THRESHOLD             11
+#define GET_STR_PRECOMPUTE_THRESHOLD     26
 #define SET_STR_DC_THRESHOLD            118
-#define SET_STR_PRECOMPUTE_THRESHOLD    929
+#define SET_STR_PRECOMPUTE_THRESHOLD   1078
+
+#define MUL_FFT_TABLE  { 560, 928, 1920, 5632, 14336, 40960, 0 }
+#define MUL_FFT_MODF_THRESHOLD          720
+#define MUL_FFT_THRESHOLD              9216
+
+#define SQR_FFT_TABLE  { 592, 928, 1920, 4608, 14336, 40960, 0 }
+#define SQR_FFT_MODF_THRESHOLD          608
+#define SQR_FFT_THRESHOLD              5888
--- 1/mpn/x86/pentium4/sse2/mode1o.asm
+++ 2/mpn/x86/pentium4/sse2/mode1o.asm
@@ -101,7 +101,7 @@
 
 	psubd	%mm0, %mm6		C inv = 2*inv - inv*inv*d
 
-	ASSERT(e,`	C expect d*inv == 1 mod 2^GMP_LIMB_BITS
+	ASSERT(e,`	C expect d*inv == 1 mod 2^BITS_PER_MP_LIMB
 	pushl	%eax	FRAME_pushl()
 	movd	%mm6, %eax
 	imul	PARAM_DIVISOR, %eax
--- 1/mpn/x86/pentium4/sse2/popcount.asm
+++ 2/mpn/x86/pentium4/sse2/popcount.asm
@@ -41,8 +41,7 @@
 C P4 model 4 (Nocona):          8
 C K8:                           7.5
 C K10:				3.5
-C P6 core2:			3.68
-C P6 corei7:			3.15
+C P6-15:			3.68
 
 C TODO
 C  * Make a mpn_hamdist based on this.  Alignment could either be handled by
--- 1/mpn/x86/pentium4/sse2/sqr_basecase.asm
+++ 2/mpn/x86/pentium4/sse2/sqr_basecase.asm
@@ -30,7 +30,7 @@
 C    with possibly needless alignment.
 C  * Use OSP, should solve feed-in latency problems.
 C  * Address relative slowness for un<=3 for Pentium M.  The old code is there
-C    considerably faster.  (1:20/14, 2:34:32, 3:66/57)
+C    consideraly faster.  (1:20/14, 2:34:32, 3:66/57)
 
 C INPUT PARAMETERS
 C rp		sp + 4
--- 1/mpn/x86/t-zdisp2.pl
+++ 2/mpn/x86/t-zdisp2.pl
@@ -71,7 +71,7 @@
   }
 }
 
-# Ensure we're using the right SQR_TOOM2_THRESHOLD for the part of the
+# Ensure we're using the right SQR_KARATSUBA_THRESHOLD for the part of the
 # tree being processed.
 sub process_mparam {
   my $file = "$File::Find::dir/gmp-mparam.h";
@@ -79,10 +79,10 @@
     print "$file\n" if $opt{'t'};
     open MPARAM, "<$file" or die;
     while (<MPARAM>) {
-      if (/^#define SQR_TOOM2_THRESHOLD[ \t]*([0-9][0-9]*)/) {
+      if (/^#define SQR_KARATSUBA_THRESHOLD[ \t]*([0-9][0-9]*)/) {
         open KARA, ">$tempfile" or die;
-        print KARA "define(\`SQR_TOOM2_THRESHOLD',$1)\n\n";
-        print "define(\`SQR_TOOM2_THRESHOLD',$1)\n" if $opt{'t'};
+        print KARA "define(\`SQR_KARATSUBA_THRESHOLD',$1)\n\n";
+        print "define(\`SQR_KARATSUBA_THRESHOLD',$1)\n" if $opt{'t'};
         close KARA or die;
         last;
       }
--- 1/mpn/x86/x86-defs.m4
+++ 2/mpn/x86/x86-defs.m4
@@ -895,7 +895,7 @@
 dnl
 dnl  This macro is only meant for use in ASSERT()s or when testing, since
 dnl  the PIC sequence it generates will want to be done with a ret balancing
-dnl  the call on CPUs with return address branch prediction.
+dnl  the call on CPUs with return address branch predition.
 dnl
 dnl  The addl generated here has a backward reference to the label, and so
 dnl  won't suffer from the two forwards references bug in old gas (described
--- 1/mpn/x86_64/addmul_2.asm
+++ 2/mpn/x86_64/addmul_2.asm
@@ -24,8 +24,7 @@
 C K8,K9:	 2.375
 C K10:		 2.375
 C P4:		 ?
-C P6 core2:	 4.45
-C P6 corei7:	 4.35
+C P6-15:	 4.45
 
 C This code is the result of running a code generation and optimization tool
 C suite written by David Harvey and Torbjorn Granlund.
@@ -109,49 +108,49 @@
 
 	ALIGN(32)
 L(top):
-	add	w3, (rp,n,8)		C 0 21
-	adc	%rax, w0		C 1 24
+	add	w3, (rp,n,8)
+	adc	%rax, w0
 	mov	8(up,n,8), %rax
-	adc	%rdx, w1		C 3 26
+	adc	%rdx, w1
 	mov	$0, R32(w2)
 	mul	v0
-	add	%rax, w0		C 2 26
+	add	%rax, w0
 	mov	8(up,n,8), %rax
-	adc	%rdx, w1		C 4 28
-	adc	$0, R32(w2)		C 6 30
+	adc	%rdx, w1
+	adc	$0, R32(w2)
 L(am0):	mul	v1
-	add	w0, 8(rp,n,8)		C 3 27
-	adc	%rax, w1		C 6 30
-	adc	%rdx, w2		C 8 32
+	add	w0, 8(rp,n,8)
+	adc	%rax, w1
+	adc	%rdx, w2
 	mov	16(up,n,8), %rax
 	mov	$0, R32(w3)
 	mul	v0
-	add	%rax, w1		C 8
+	add	%rax, w1
 	mov	16(up,n,8), %rax
-	adc	%rdx, w2		C 10
-	adc	$0, R32(w3)		C 12
+	adc	%rdx, w2
+	adc	$0, R32(w3)
 L(am3):	mul	v1
-	add	w1, 16(rp,n,8)		C 9
-	adc	%rax, w2		C 12
+	add	w1, 16(rp,n,8)
+	adc	%rax, w2
 	mov	24(up,n,8), %rax
-	adc	%rdx, w3		C 14
+	adc	%rdx, w3
 	mul	v0
 	mov	$0, R32(w0)
-	add	%rax, w2		C 14
-	adc	%rdx, w3		C 16
+	add	%rax, w2
+	adc	%rdx, w3
 	mov	$0, R32(w1)
 	mov	24(up,n,8), %rax
-	adc	$0, R32(w0)		C 18
+	adc	$0, R32(w0)
 L(am2):	mul	v1
-	add	w2, 24(rp,n,8)		C 15
-	adc	%rax, w3		C 18
-	adc	%rdx, w0		C 20
+	add	w2, 24(rp,n,8)
+	adc	%rax, w3
+	adc	%rdx, w0
 	mov	32(up,n,8), %rax
 	mul	v0
-	add	%rax, w3		C 20
+	add	%rax, w3
 	mov	32(up,n,8), %rax
-	adc	%rdx, w0		C 22
-	adc	$0, R32(w1)		C 24
+	adc	%rdx, w0
+	adc	$0, R32(w1)
 L(am1):	mul	v1
 	add	$4, n
 	js	L(top)
--- 1/mpn/x86_64/aorrlsh_n.asm
+++ 2/mpn/x86_64/aorrlsh_n.asm
@@ -149,7 +149,7 @@
 	jnc	L(oop)
 L(end):
 	add	%ebx, %ebx
-	ADDSUBC	$0, %r15
+	adc	$0, %r15
 	mov	%r15, %rax
 	pop	%rbx
 	pop	%r15
--- 1/mpn/x86_64/aorsmul_1.asm
+++ 2/mpn/x86_64/aorsmul_1.asm
@@ -23,9 +23,8 @@
 C K8,K9:	 2.5
 C K10:		 2.5
 C P4:		14.9
-C P6 core2:	 5.09
-C P6 corei7:
-C P6 atom:	21.3
+C P6-15 (Core2): 5.09
+C P6-28 (Atom):	21.3
 
 C The inner loop of this code is the result of running a code generation and
 C optimization tool suite written by David Harvey and Torbjorn Granlund.
--- 1/mpn/x86_64/aors_n.asm
+++ 2/mpn/x86_64/aors_n.asm
@@ -23,9 +23,8 @@
 C K8,K9:	 1.5
 C K10:		 1.5
 C P4:		 ?
-C P6 core2: 	 4.9
-C P6 corei7:
-C P6 atom:	 4
+C P6-15 (Core2): 4.9
+C P6-28 (Atom):	 4
 
 C The inner loop of this code is the result of running a code generation and
 C optimization tool suite written by David Harvey and Torbjorn Granlund.
--- 1/mpn/x86_64/atom/gmp-mparam.h
+++ 2/mpn/x86_64/atom/gmp-mparam.h
@@ -1,4 +1,4 @@
-/* Intel Atom/64 gmp-mparam.h -- Compiler/machine parameter header file.
+/* Inte Atom gmp-mparam.h -- Compiler/machine parameter header file.
 
 Copyright 1991, 1993, 1994, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007,
 2008, 2009 Free Software Foundation, Inc.
@@ -18,66 +18,56 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
+/* Generated by tuneup.c, 2009-01-14, gcc 4.2 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.4 */
-
-#define MUL_TOOM22_THRESHOLD             10
-#define MUL_TOOM33_THRESHOLD             66
+#define MUL_KARATSUBA_THRESHOLD          10
+#define MUL_TOOM3_THRESHOLD              66
 #define MUL_TOOM44_THRESHOLD            118
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              16
-#define SQR_TOOM3_THRESHOLD              65
+#define SQR_KARATSUBA_THRESHOLD          18
+#define SQR_TOOM3_THRESHOLD              98
 #define SQR_TOOM4_THRESHOLD             166
 
-#define MUL_FFT_TABLE  { 272, 544, 1088, 1792, 5120, 20480, 49152, 196608, 786432, 0 }
-#define MUL_FFT_MODF_THRESHOLD          240
-#define MUL_FFT_THRESHOLD              1664
-
-#define SQR_FFT_TABLE  { 272, 544, 1088, 1792, 5120, 12288, 49152, 196608, 786432, 0 }
-#define SQR_FFT_MODF_THRESHOLD          216
-#define SQR_FFT_THRESHOLD              1408
-
-#define MULLO_BASECASE_THRESHOLD          2
-#define MULLO_DC_THRESHOLD               23
-#define MULLO_MUL_N_THRESHOLD          2350
-
-#define MULMOD_BNM1_THRESHOLD            10
-
-#define DC_DIV_QR_THRESHOLD              26
-#define DC_DIVAPPR_Q_THRESHOLD           92
-#define DC_BDIV_QR_THRESHOLD             27
-#define DC_BDIV_Q_THRESHOLD              62
-#define INV_MULMOD_BNM1_THRESHOLD        49
-#define INV_NEWTON_THRESHOLD            149
-#define INV_APPR_THRESHOLD               25
-#define BINV_NEWTON_THRESHOLD           165
-#define REDC_1_TO_REDC_2_THRESHOLD       12
-#define REDC_2_TO_REDC_N_THRESHOLD       38
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              24
+#define MULLOW_MUL_N_THRESHOLD          170
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 30
+#define POWM_THRESHOLD                   48
 
-#define MATRIX22_STRASSEN_THRESHOLD      13
+#define MATRIX22_STRASSEN_THRESHOLD      17
 #define HGCD_THRESHOLD                   86
-#define GCD_DC_THRESHOLD                198
-#define GCDEXT_DC_THRESHOLD             205
-#define JACOBI_BASE_METHOD                2
+#define GCD_DC_THRESHOLD                196
+#define GCDEXT_DC_THRESHOLD             236
+#define JACOBI_BASE_METHOD                3
 
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 9
-#define MOD_1_2_THRESHOLD                10
-#define MOD_1_4_THRESHOLD                16
+#define MOD_1_1_THRESHOLD                 8
+#define MOD_1_2_THRESHOLD                 9
+#define MOD_1_4_THRESHOLD                24
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             16
-#define GET_STR_PRECOMPUTE_THRESHOLD     27
-#define SET_STR_DC_THRESHOLD            234
-#define SET_STR_PRECOMPUTE_THRESHOLD   1105
+#define GET_STR_DC_THRESHOLD             19
+#define GET_STR_PRECOMPUTE_THRESHOLD     35
+#define SET_STR_DC_THRESHOLD            268
+#define SET_STR_PRECOMPUTE_THRESHOLD   1142
+
+#define MUL_FFT_TABLE  { 272, 544, 1088, 1792, 5120, 12288, 49152, 196608, 786432, 0 }
+#define MUL_FFT_MODF_THRESHOLD          240
+#define MUL_FFT_THRESHOLD              1408
+
+#define SQR_FFT_TABLE  { 240, 544, 1216, 2304, 5120, 12288, 49152, 196608, 786432, 0 }
+#define SQR_FFT_MODF_THRESHOLD          240
+#define SQR_FFT_THRESHOLD              1408
 
 /* These tables need to be updated.  */
 
--- 1/mpn/x86_64/bdiv_dbm1c.asm
+++ 2/mpn/x86_64/bdiv_dbm1c.asm
@@ -23,9 +23,8 @@
 C K8,K9:	 2.25
 C K10:		  ?
 C P4:		12.5
-C P6 core2: 	 4.0
-C P6 corei7: 	 3.8
-C P6 atom:	20
+C P6-15 (Core2): 4.0
+C P6-28 (Atom): 20
 
 C TODO
 C  * Do proper 4-way feed-in instead of the current epilogue
--- 1/mpn/x86_64/copyd.asm
+++ 2/mpn/x86_64/copyd.asm
@@ -24,8 +24,7 @@
 C K8,K9:	1
 C K10:		1
 C P4:		2.8
-C P6 core2:	1.2
-C P6 corei7:	1
+C P6-15:	1.2
 
 
 C INPUT PARAMETERS
--- 1/mpn/x86_64/core2/aorsmul_1.asm
+++ 2/mpn/x86_64/core2/aorsmul_1.asm
@@ -1,6 +1,6 @@
 dnl  x86-64 mpn_addmul_1 and mpn_submul_1, optimized for "Core 2".
 
-dnl  Copyright 2003, 2004, 2005, 2007, 2008, 2009 Free Software Foundation, Inc.
+dnl  Copyright 2003, 2004, 2005, 2007, 2008 Free Software Foundation, Inc.
 
 dnl  This file is part of the GNU MP Library.
 
@@ -23,8 +23,7 @@
 C K8,K9:	 4
 C K10:		 4
 C P4:		 ?
-C P6 core2:	 4.3-4.5 (fluctuating)
-C P6 corei7:	 5
+C P6-15:	 4.3-4.7 (fluctuating)
 
 C INPUT PARAMETERS
 define(`rp',	`%rdi')
@@ -47,83 +46,98 @@
 	TEXT
 	ALIGN(16)
 PROLOGUE(func)
-	push	%rbx
-	push	%rbp
-	lea	(%rdx), %rbx
-	neg	%rbx
+	push	%r15
+	push	%r12
+	push	%r13
+	lea	(%rdx), %r15
+	neg	%r15
 
 	mov	(up), %rax
-	mov	(rp), %r10
 
-	lea	-16(rp,%rdx,8), rp
+	bt	$0, %r15
+	jc	L(odd)
+
+	lea	(rp,%rdx,8), rp
 	lea	(up,%rdx,8), up
 	mul	%rcx
 
-	bt	$0, R32(%rbx)
-	jc	L(odd)
-
 	lea	(%rax), %r11
-	mov	8(up,%rbx,8), %rax
-	lea	(%rdx), %rbp
-	mul	%rcx
-	add	$2, %rbx
+	mov	8(up,%r15,8), %rax
+	mov	(rp,%r15,8), %r13
+	lea	(%rdx), %r12
+
+	add	$2, %r15
 	jns	L(n2)
 
+	mul	%rcx
 	lea	(%rax), %r8
-	mov	(up,%rbx,8), %rax
+	mov	(up,%r15,8), %rax
+	mov	-8(rp,%r15,8), %r10
 	lea	(%rdx), %r9
-	jmp	L(mid)
+	jmp	L(m)
 
-L(odd):	add	$1, %rbx
+L(odd):	lea	(rp,%rdx,8), rp
+	lea	(up,%rdx,8), up
+	mul	%rcx
+	add	$1, %r15
 	jns	L(n1)
 
-	lea	(%rax), %r8
-	mov	(up,%rbx,8), %rax
+L(gt1):	lea	(%rax), %r8
+	mov	(up,%r15,8), %rax
+	mov	-8(rp,%r15,8), %r10
 	lea	(%rdx), %r9
 	mul	%rcx
 	lea	(%rax), %r11
-	mov	8(up,%rbx,8), %rax
-	lea	(%rdx), %rbp
-	jmp	L(e)
+	mov	8(up,%r15,8), %rax
+	mov	(rp,%r15,8), %r13
+	lea	(%rdx), %r12
+	add	$2, %r15
+	jns	L(end)
 
 	ALIGN(16)
 L(top):	mul	%rcx
 	ADDSUB	%r8, %r10
 	lea	(%rax), %r8
-	mov	(up,%rbx,8), %rax
+	mov	0(up,%r15,8), %rax
 	adc	%r9, %r11
-	mov	%r10, -8(rp,%rbx,8)
-	mov	(rp,%rbx,8), %r10
+	mov	%r10, -24(rp,%r15,8)
+	mov	-8(rp,%r15,8), %r10
 	lea	(%rdx), %r9
-	adc	$0, %rbp
-L(mid):	mul	%rcx
-	ADDSUB	%r11, %r10
+	adc	$0, %r12
+L(m):	mul	%rcx
+	ADDSUB	%r11, %r13
 	lea	(%rax), %r11
-	mov	8(up,%rbx,8), %rax
-	adc	%rbp, %r8
-	mov	%r10, (rp,%rbx,8)
-	mov	8(rp,%rbx,8), %r10
-	lea	(%rdx), %rbp
+	mov	8(up,%r15,8), %rax
+	adc	%r12, %r8
+	mov	%r13, -16(rp,%r15,8)
+	mov	0(rp,%r15,8), %r13
+	lea	(%rdx), %r12
 	adc	$0, %r9
-L(e):	add	$2, %rbx
+
+	add	$2, %r15
 	js	L(top)
 
-	mul	%rcx
+L(end):	mul	%rcx
 	ADDSUB	%r8, %r10
 	adc	%r9, %r11
-	mov	%r10, -8(rp)
-	adc	$0, %rbp
-L(n2):	mov	(rp), %r10
-	ADDSUB	%r11, %r10
-	adc	%rbp, %rax
-	mov	%r10, (rp)
+	mov	%r10, -24(rp,%r15,8)
+	mov	-8(rp,%r15,8), %r10
+	adc	$0, %r12
+L(r):	ADDSUB	%r11, %r13
+	adc	%r12, %rax
+	mov	%r13, -16(rp,%r15,8)
 	adc	$0, %rdx
-L(n1):	mov	8(rp), %r10
-	ADDSUB	%rax, %r10
-	mov	%r10, 8(rp)
-	mov	R32(%rbx), R32(%rax)	C zero rax
+L(x):	ADDSUB	%rax, %r10
+	mov	%r10, -8(rp,%r15,8)
+	mov	$0, %eax
 	adc	%rdx, %rax
-	pop	%rbp
-	pop	%rbx
+L(ret):	pop	%r13
+	pop	%r12
+	pop	%r15
 	ret
+L(n2):	mul	%rcx
+	mov	-8(rp,%r15,8), %r10
+	jmp	L(r)
+L(n1):	mov	-8(rp,%r15,8), %r10
+	jmp	L(x)
 EPILOGUE()
--- 1/mpn/x86_64/core2/aors_n.asm
+++ 2/mpn/x86_64/core2/aors_n.asm
@@ -24,8 +24,7 @@
 C K8,K9:	 2.25
 C K10:		 2
 C P4:		10
-C P6 core2:	 2.05
-C P6 corei7:	 2.3
+C P6-15:	 2.05
 
 C INPUT PARAMETERS
 define(`rp',	`%rdi')
--- 1/mpn/x86_64/core2/gmp-mparam.h
+++ 2/mpn/x86_64/core2/gmp-mparam.h
@@ -1,4 +1,4 @@
-/* Core 2 gmp-mparam.h -- Compiler/machine parameter header file.
+/* "Core 2" gmp-mparam.h -- Compiler/machine parameter header file.
 
 Copyright 1991, 1993, 1994, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007,
 2008, 2009 Free Software Foundation, Inc.
@@ -18,64 +18,61 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
-/* 2133 MHz Core 2 (65nm) */
+/* 2133 MHz "Core 2" / 65nm / 4096 Kibyte cache / socket 775 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.2 */
+/* Generated by tuneup.c, 2009-01-14, gcc 4.2 */
 
-#define MUL_TOOM22_THRESHOLD             22
-#define MUL_TOOM33_THRESHOLD             65
-#define MUL_TOOM44_THRESHOLD            169
+#define MUL_KARATSUBA_THRESHOLD          18
+#define MUL_TOOM3_THRESHOLD              65
+#define MUL_TOOM44_THRESHOLD            166
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              28
-#define SQR_TOOM3_THRESHOLD              85
-#define SQR_TOOM4_THRESHOLD             142
-
-#define MUL_FFT_TABLE  { 400, 800, 1600, 2816, 7168, 20480, 81920, 196608, 0 }
-#define MUL_FFT_MODF_THRESHOLD          400
-#define MUL_FFT_THRESHOLD              4224
-
-#define SQR_FFT_TABLE  { 336, 736, 1728, 2816, 7168, 20480, 81920, 327680, 0 }
-#define SQR_FFT_MODF_THRESHOLD          368
-#define SQR_FFT_THRESHOLD              2688
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               20
-#define MULLO_MUL_N_THRESHOLD          5472
-
-#define MULMOD_BNM1_THRESHOLD            17
-
-#define DC_DIV_QR_THRESHOLD              52
-#define DC_DIVAPPR_Q_THRESHOLD          199
-#define DC_BDIV_QR_THRESHOLD             52
-#define DC_BDIV_Q_THRESHOLD             154
-#define INV_MULMOD_BNM1_THRESHOLD       108
-#define INV_NEWTON_THRESHOLD            200
-#define INV_APPR_THRESHOLD               13
-#define BINV_NEWTON_THRESHOLD           260
-#define REDC_1_TO_REDC_2_THRESHOLD       10
-#define REDC_2_TO_REDC_N_THRESHOLD       66
-
-#define MATRIX22_STRASSEN_THRESHOLD      19
-#define HGCD_THRESHOLD                  135
-#define GCD_DC_THRESHOLD                469
-#define GCDEXT_DC_THRESHOLD             361
+#define SQR_KARATSUBA_THRESHOLD          32
+#define SQR_TOOM3_THRESHOLD              97
+#define SQR_TOOM4_THRESHOLD             163
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              20
+#define MULLOW_MUL_N_THRESHOLD          232
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 60
+#define POWM_THRESHOLD                   77
+
+#define MATRIX22_STRASSEN_THRESHOLD      25
+#define HGCD_THRESHOLD                  140
+#define GCD_DC_THRESHOLD                691
+#define GCDEXT_DC_THRESHOLD             760
 #define JACOBI_BASE_METHOD                1
 
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
-#define MOD_1_1_THRESHOLD                 4
+#define MOD_1_1_THRESHOLD                 3
 #define MOD_1_2_THRESHOLD                 5
-#define MOD_1_4_THRESHOLD                15
+#define MOD_1_4_THRESHOLD                20
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             11
-#define GET_STR_PRECOMPUTE_THRESHOLD     18
-#define SET_STR_DC_THRESHOLD            552
-#define SET_STR_PRECOMPUTE_THRESHOLD   1721
+#define GET_STR_DC_THRESHOLD             10
+#define GET_STR_PRECOMPUTE_THRESHOLD     16
+#define SET_STR_DC_THRESHOLD            668
+#define SET_STR_PRECOMPUTE_THRESHOLD   2052
+
+#define MUL_FFT_TABLE  { 336, 672, 1600, 2816, 7168, 20480, 81920, 327680, 786432, 0 }
+#define MUL_FFT_MODF_THRESHOLD          352
+#define MUL_FFT_THRESHOLD              3456
+
+#define SQR_FFT_TABLE  { 336, 736, 1728, 3328, 7168, 20480, 81920, 327680, 0 }
+#define SQR_FFT_MODF_THRESHOLD          352
+#define SQR_FFT_THRESHOLD              2432
+
+/* Generated 2009-01-12, gcc 4.2 */
+
+#define MUL_FFT_TABLE2 {{1,4}, {273,5}, {545,6}, {1217,7}, {3201,8}, {6913,9}, {7681,8}, {8449,9}, {9729,8}, {10497,9}, {13825,10}, {15361,9}, {19969,10}, {23553,9}, {28161,11}, {30721,10}, {31745,9}, {34305,10}, {39937,9}, {42497,10}, {56321,11}, {63489,10}, {81409,11}, {92161,10}, {93185,11}, {96257,12}, {126977,11}, {131073,10}, {138241,11}, {167937,10}, {169473,11}, {169985,10}, {172033,11}, {195585,9}, {196097,11}, {198657,10}, {208897,11}, {217089,12}, {258049,11}, {261121,9}, {262657,10}, {275457,11}, {302081,10}, {307201,11}, {331777,12}, {389121,11}, {425985,13}, {516097,12}, {520193,11}, {598017,12}, {610305,11}, {614401,12}, {651265,11}, {653313,10}, {654337,11}, {673793,10}, {674817,11}, {677889,10}, {679937,11}, {718849,10}, {719873,12}, {782337,11}, {850945,12}, {913409,11}, {925697,13}, {1040385,12}, {1044481,11}, {1112065,12}, {1175553,11}, {1244161,12}, {1306625,11}, {1310721,12}, {1327105,11}, {1347585,12}, {1355777,11}, {1366017,12}, {1439745,13}, {1564673,12}, {1835009,14}, {1900545,12}, {1904641,14}, {2080769,13}, {2088961,12}, {2488321,13}, {2613249,12}, {2879489,13}, {2932737,12}, {2940929,13}, {3137537,12}, {3403777,13}, {3661825,12}, {3928065,14}, {4177921,13}, {4186113,12}, {4452353,13}, {4710401,12}, {4978689,13}, {5234689,12}, {5500929,13}, {5758977,14}, {6275073,13}, {7856129,15}, {8355841,14}, {8372225,13}, {9957377,14}, {MP_SIZE_T_MAX, 0}}
+
+#define SQR_FFT_TABLE2 {{1,4}, {241,5}, {545,6}, {1345,7}, {3201,8}, {6913,9}, {7681,8}, {8961,9}, {9729,8}, {10497,9}, {13825,10}, {15361,9}, {19969,10}, {23553,9}, {28161,11}, {30721,10}, {31745,9}, {34305,10}, {55297,11}, {63489,10}, {80897,11}, {94209,10}, {97281,12}, {126977,11}, {129025,9}, {130049,10}, {138753,11}, {162817,9}, {164353,11}, {170497,10}, {178177,11}, {183297,10}, {184321,11}, {194561,10}, {208897,12}, {219137,11}, {221185,12}, {258049,11}, {261121,9}, {261633,10}, {267777,9}, {268289,11}, {270337,10}, {274945,9}, {276481,10}, {278529,11}, {292865,9}, {293377,10}, {295937,9}, {296449,10}, {306177,9}, {309249,10}, {310273,11}, {328705,12}, {331777,11}, {335873,12}, {344065,11}, {346113,12}, {352257,11}, {356353,12}, {389121,11}, {395265,10}, {398337,11}, {419841,10}, {421889,11}, {423937,13}, {516097,12}, {520193,11}, {546817,10}, {550913,11}, {561153,10}, {563201,11}, {579585,10}, {585729,11}, {621569,12}, {636929,11}, {638977,12}, {651265,11}, {714753,10}, {716801,11}, {718849,12}, {782337,11}, {849921,12}, {913409,11}, {954369,13}, {1040385,12}, {1044481,11}, {1112065,12}, {1175553,11}, {1243137,12}, {1306625,11}, {1374209,12}, {1437697,13}, {1564673,12}, {1961985,14}, {2080769,13}, {2088961,12}, {2486273,13}, {2613249,12}, {2879489,13}, {3137537,12}, {3272705,13}, {3661825,12}, {3928065,14}, {4177921,13}, {4186113,12}, {4452353,13}, {4710401,12}, {4976641,13}, {5234689,12}, {5320705,13}, {5324801,12}, {5447681,13}, {5455873,12}, {5500929,13}, {5758977,14}, {6275073,13}, {6283265,12}, {6549505,13}, {7856129,15}, {8355841,14}, {8372225,13}, {9953281,14}, {MP_SIZE_T_MAX, 0}}
--- 1/mpn/x86_64/core2/lshift.asm
+++ 2/mpn/x86_64/core2/lshift.asm
@@ -24,8 +24,7 @@
 C K8,K9:	 4.25
 C K10:		 4.25
 C P4:		14.7
-C P6 core2:	 1.27
-C P6 corei7:	 1.75
+C P6-15:	 1.27
 
 
 C INPUT PARAMETERS
--- 1/mpn/x86_64/core2/rshift.asm
+++ 2/mpn/x86_64/core2/rshift.asm
@@ -24,8 +24,7 @@
 C K8,K9:	 4.25
 C K10:		 4.25
 C P4:		14.7
-C P6 core2:	 1.27
-C P6 corei7:	 1.75
+C P6-15:	 1.27
 
 
 C INPUT PARAMETERS
--- 1/mpn/x86_64/dive_1.asm
+++ 2/mpn/x86_64/dive_1.asm
@@ -24,9 +24,8 @@
 C K8,K9:	10
 C K10:		10
 C P4:		33
-C P6 core2:	13.25
-C P6 corei7:	14
-C P6 atom:	42
+C P6-15 (Core2):13.25
+C P6-28 (Atom):	42
 
 C A quick adoption of the 32-bit K7 code.
 
@@ -41,62 +40,63 @@
 	TEXT
 	ALIGN(16)
 PROLOGUE(mpn_divexact_1)
-	push	%rbx
+	pushq	%rbx
 
-	mov	%rcx, %rax
-	xor	R32(%rcx), R32(%rcx)	C shift count
-	mov	%rdx, %r8
+	movq	%rcx, %rax
+	movl	$0, %ecx		C shift count
+	movq	%rdx, %r8
 
-	bt	$0, R32(%rax)
+	btl	$0, %eax
 	jnc	L(evn)			C skip bsfq unless divisor is even
 
-L(odd):	mov	%rax, %rbx
-	shr	R32(%rax)
-	and	$127, R32(%rax)		C d/2, 7 bits
+L(odd):	movq	%rax, %rbx
+	shrl	%eax
+	andl	$127, %eax		C d/2, 7 bits
 
 ifdef(`PIC',`
-	mov	binvert_limb_table@GOTPCREL(%rip), %rdx
+	movq	binvert_limb_table@GOTPCREL(%rip), %rdx
 ',`
-	movabs	$binvert_limb_table, %rdx
+	movabsq	$binvert_limb_table, %rdx
 ')
 
-	movzbl	(%rdx,%rax), R32(%rax)	C inv 8 bits
+	movzbl	(%rax,%rdx), %eax	C inv 8 bits
 
-	mov	%rbx, %r11		C d without twos
+	movq	%rbx, %r11		C d without twos
 
-	lea	(%rax,%rax), R32(%rdx)	C 2*inv
-	imul	R32(%rax), R32(%rax)	C inv*inv
-	imul	R32(%rbx), R32(%rax)	C inv*inv*d
-	sub	R32(%rax), R32(%rdx)	C inv = 2*inv - inv*inv*d, 16 bits
+	leal	(%rax,%rax), %edx	C 2*inv
+	imull	%eax, %eax		C inv*inv
+	imull	%ebx, %eax		C inv*inv*d
+	subl	%eax, %edx		C inv = 2*inv - inv*inv*d, 16 bits
 
-	lea	(%rdx,%rdx), R32(%rax)	C 2*inv
-	imul	R32(%rdx), R32(%rdx)	C inv*inv
-	imul	R32(%rbx), R32(%rdx)	C inv*inv*d
-	sub	R32(%rdx), R32(%rax)	C inv = 2*inv - inv*inv*d, 32 bits
+	leal	(%rdx,%rdx), %eax	C 2*inv
+	imull	%edx, %edx		C inv*inv
+	imull	%ebx, %edx		C inv*inv*d
+	subl	%edx, %eax		C inv = 2*inv - inv*inv*d, 32 bits
 
-	lea	(%rax,%rax), %r10	C 2*inv
-	imul	%rax, %rax		C inv*inv
-	imul	%rbx, %rax		C inv*inv*d
-	sub	%rax, %r10		C inv = 2*inv - inv*inv*d, 64 bits
+	leaq	(%rax,%rax), %rdx	C 2*inv
+	imulq	%rax, %rax		C inv*inv
+	imulq	%rbx, %rax		C inv*inv*d
+	subq	%rax, %rdx		C inv = 2*inv - inv*inv*d, 64 bits
 
-	lea	(%rsi,%r8,8), %rsi	C up end
-	lea	-8(%rdi,%r8,8), %rdi	C rp end
-	neg	%r8			C -n
+	leaq	(%rsi,%r8,8), %rsi	C up end
+	leaq	-8(%rdi,%r8,8), %rdi	C rp end
+	negq	%r8			C -n
 
-	mov	(%rsi,%r8,8), %rax	C up[0]
+	movq	%rdx, %r10		C final inverse
+	movq	(%rsi,%r8,8), %rax	C up[0]
 
-	inc	%r8
+	incq	%r8
 	jz	L(one)
 
-	mov	(%rsi,%r8,8), %rdx	C up[1]
+	movq	(%rsi,%r8,8), %rdx	C up[1]
 
-	shrd	R8(%rcx), %rdx, %rax
+	shrdq	%cl, %rdx, %rax
 
-	xor	R32(%rbx), R32(%rbx)
-	jmp	L(ent)
+	xorl	%ebx, %ebx
+	jmp	L(entry)
 
-L(evn):	bsf	%rax, %rcx
-	shr	R8(%rcx), %rax
+L(evn):	bsfq	%rax, %rcx
+	shrq	%cl, %rax
 	jmp	L(odd)
 
 	ALIGN(8)
@@ -108,37 +108,54 @@
 	C rsi	up end
 	C rdi	rp end
 	C r8	counter, limbs, negative
-	C r10	d^(-1) mod 2^64
-	C r11	d, shifted down
 
-	mul	%r11			C carry limb in rdx	0 10
-	mov	-8(%rsi,%r8,8), %rax	C
-	mov	(%rsi,%r8,8), %r9	C
-	shrd	R8(%rcx), %r9, %rax	C
-	nop				C
-	sub	%rbx, %rax		C apply carry bit
-	setc	%bl			C
-	sub	%rdx, %rax		C apply carry limb	5
-	adc	$0, %rbx		C			6
-L(ent):	imul	%r10, %rax		C			6
-	mov	%rax, (%rdi,%r8,8)	C
-	inc	%r8			C
+	mulq	%r11			C carry limb in rdx
+
+	movq	-8(%rsi,%r8,8), %rax
+	movq	(%rsi,%r8,8), %r9
+
+	shrdq	%cl, %r9, %rax
+	nop
+
+	subq	%rbx, %rax		C apply carry bit
+	setc	%bl
+
+	subq	%rdx, %rax		C apply carry limb
+	adcq	$0, %rbx
+
+L(entry):
+	imulq	%r10, %rax
+
+	movq	%rax, (%rdi,%r8,8)
+	incq	%r8
 	jnz	L(top)
 
-	mul	%r11			C carry limb in rdx
-	mov	-8(%rsi), %rax		C up high limb
-	shr	R8(%rcx), %rax
-	sub	%rbx, %rax		C apply carry bit
-	sub	%rdx, %rax		C apply carry limb
-	imul	%r10, %rax
-	mov	%rax, (%rdi)
-	pop	%rbx
+
+	mulq	%r11			C carry limb in rdx
+
+	movq	-8(%rsi), %rax		C up high limb
+	shrq	%cl, %rax
+
+	subq	%rbx, %rax		C apply carry bit
+
+	subq	%rdx, %rax		C apply carry limb
+
+	imulq	%r10, %rax
+
+	movq	%rax, (%rdi)
+
+	popq	%rbx
 	ret
 
-L(one):	shr	R8(%rcx), %rax
-	imul	%r10, %rax
-	mov	%rax, (%rdi)
-	pop	%rbx
+
+L(one):
+	shrq	%cl, %rax
+
+	imulq	%r10, %rax
+
+	movq	%rax, (%rdi)
+
+	popq	%rbx
 	ret
 
 EPILOGUE()
--- 1/mpn/x86_64/divrem_1.asm
+++ 2/mpn/x86_64/divrem_1.asm
@@ -23,9 +23,8 @@
 C		norm	unorm	frac
 C K8		13	13	12
 C P4		44.2	44.2	42.3
-C P6 core2	25	24.5	19.3
-C P6 corei7	21.5	20.7	18
-C P6 atom	42	52	37
+C P6-15 (Core2)	24.5	24.5	19.3
+C P6-15 (Atom)	42	52	37
 
 C TODO
 C  * Compute the inverse without relying on the div instruction.
@@ -277,4 +276,4 @@
 	pop	%r12
 	pop	%r13
 	ret
-EPILOGUE()
+EPILOGUE(mpn_divrem_1)
--- 1/mpn/x86_64/divrem_2.asm
+++ 2/mpn/x86_64/divrem_2.asm
@@ -23,8 +23,7 @@
 C		norm	frac
 C K8		20	20
 C P4		73	73
-C P6 core2	37	37
-C P6 corei7	33	33
+C P6-15		37	37
 
 C TODO
 C  * Perhaps compute the inverse without relying on divq?  Could either use
@@ -138,7 +137,7 @@
 	inc	%rdi			C		7
 	xor	R32(%rdx), R32(%rdx)	C
 	cmp	%r10, %rbx		C		13
-	mov	%r8, %rax		C d0		ncp
+	mov	%r8, %rax		C d1		ncp
 	adc	$-1, %rdx		C mask		14
 	add	%rdx, %rdi		C q--		15
 	and	%rdx, %rax		C d0 or 0	15
--- 1/mpn/x86_64/gmp-mparam.h
+++ 2/mpn/x86_64/gmp-mparam.h
@@ -18,72 +18,62 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
-/* 2500 MHz Athlon64 / 512 Kibyte cache */
+/* 2200 MHz Opteron / rev A / 1024 Kibyte cache / socket 940 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.2 */
+/* Generated by tuneup.c, 2009-01-14, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             28
-#define MUL_TOOM33_THRESHOLD             81
-#define MUL_TOOM44_THRESHOLD            242
+#define MUL_KARATSUBA_THRESHOLD          28
+#define MUL_TOOM3_THRESHOLD              97
+#define MUL_TOOM44_THRESHOLD            406
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              40
-#define SQR_TOOM3_THRESHOLD             121
-#define SQR_TOOM4_THRESHOLD             532
-
-#define MUL_FFT_TABLE  { 464, 1056, 1856, 3840, 11264, 36864, 114688, 458752, 0 }
-#define MUL_FFT_MODF_THRESHOLD          688
-#define MUL_FFT_THRESHOLD              7936
-
-#define SQR_FFT_TABLE  { 496, 1120, 2240, 4352, 11264, 45056, 114688, 327680, 0 }
-#define SQR_FFT_MODF_THRESHOLD          688
-#define SQR_FFT_THRESHOLD              6272
-
-#define MULLO_BASECASE_THRESHOLD         21
-#define MULLO_DC_THRESHOLD                0  /* never mpn_mullo_basecase */
-#define MULLO_MUL_N_THRESHOLD         15585
-
-#define MULMOD_BNM1_THRESHOLD            17
-
-#define DC_DIV_QR_THRESHOLD              43
-#define DC_DIVAPPR_Q_THRESHOLD          250
-#define DC_BDIV_QR_THRESHOLD             38
-#define DC_BDIV_Q_THRESHOLD             182
-#define INV_MULMOD_BNM1_THRESHOLD       189
-#define INV_NEWTON_THRESHOLD            252
-#define INV_APPR_THRESHOLD               17
-#define BINV_NEWTON_THRESHOLD           306
-#define REDC_1_TO_REDC_2_THRESHOLD       34
-#define REDC_2_TO_REDC_N_THRESHOLD       99
-
-#define MATRIX22_STRASSEN_THRESHOLD      33
-#define HGCD_THRESHOLD                  144
-#define GCD_DC_THRESHOLD                501
-#define GCDEXT_DC_THRESHOLD             521
-#define JACOBI_BASE_METHOD                3
+#define SQR_KARATSUBA_THRESHOLD          38
+#define SQR_TOOM3_THRESHOLD             133
+#define SQR_TOOM4_THRESHOLD             547
+
+#define MULLOW_BASECASE_THRESHOLD        27
+#define MULLOW_DC_THRESHOLD              28
+#define MULLOW_MUL_N_THRESHOLD          199
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 74
+#define POWM_THRESHOLD                  146
+
+#define MATRIX22_STRASSEN_THRESHOLD      24
+#define HGCD_THRESHOLD                  143
+#define GCD_DC_THRESHOLD                529
+#define GCDEXT_DC_THRESHOLD             639
+#define JACOBI_BASE_METHOD                1
 
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
 #define MOD_1_1_THRESHOLD                 4
 #define MOD_1_2_THRESHOLD                 7
-#define MOD_1_4_THRESHOLD                15
+#define MOD_1_4_THRESHOLD                64
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             17
-#define GET_STR_PRECOMPUTE_THRESHOLD     28
-#define SET_STR_DC_THRESHOLD            266
-#define SET_STR_PRECOMPUTE_THRESHOLD   1787
+#define GET_STR_DC_THRESHOLD             18
+#define GET_STR_PRECOMPUTE_THRESHOLD     32
+#define SET_STR_DC_THRESHOLD            248
+#define SET_STR_PRECOMPUTE_THRESHOLD   2124
 
-/* These tables are older than the thresholds above.  */
+#define MUL_FFT_TABLE  { 432, 928, 2624, 3840, 11264, 36864, 147456, 327680, 0 }
+#define MUL_FFT_MODF_THRESHOLD          656
+#define MUL_FFT_THRESHOLD              7936
+
+#define SQR_FFT_TABLE  { 432, 928, 2368, 4352, 11264, 28672, 114688, 327680, 0 }
+#define SQR_FFT_MODF_THRESHOLD          560
+#define SQR_FFT_THRESHOLD              7936
 
 #define MUL_FFT_TABLE2 {{1,4}, {337,5}, {673,6}, {1729,7}, {1793,6}, {2017,7}, {5633,8}, {11009,9}, {11777,8}, {14593,9}, {15873,8}, {16897,9}, {22017,10}, {23553,9}, {29697,10}, {31745,9}, {36353,10}, {39937,9}, {44545,10}, {48129,9}, {50689,10}, {56833,11}, {63489,10}, {78337,11}, {79873,10}, {86017,11}, {88065,10}, {92161,11}, {96257,10}, {106497,11}, {129025,10}, {141313,11}, {145409,10}, {146433,11}, {161793,10}, {167937,11}, {227329,12}, {258049,11}, {326657,12}, {389121,11}, {424961,13}, {516097,12}, {520193,11}, {528385,10}, {538625,11}, {547841,10}, {552961,11}, {587777,12}, {651265,11}, {718849,10}, {719873,12}, {782337,11}, {787457,10}, {791553,11}, {796673,10}, {802817,11}, {849921,10}, {850945,12}, {913409,11}, {915457,13}, {1040385,12}, {1044481,11}, {1112065,12}, {1175553,11}, {1243137,12}, {1306625,11}, {1374209,12}, {1437697,13}, {1564673,12}, {1568769,11}, {1581057,12}, {1585153,11}, {1595393,12}, {1597441,11}, {1630209,12}, {1699841,11}, {1761281,12}, {1830913,14}, {2080769,13}, {2088961,12}, {2486273,13}, {2613249,12}, {3010561,13}, {3137537,12}, {3534849,13}, {3661825,12}, {3928065,13}, {3964929,14}, {4014081,13}, {4046849,14}, {4136961,13}, {4186113,12}, {4452353,13}, {4710401,12}, {4976641,13}, {5234689,12}, {5238785,13}, {5349377,12}, {5353473,13}, {5758977,12}, {5763073,14}, {6275073,13}, {7856129,14}, {8372225,13}, {9953281,14}, {10469377,13}, {12050433,14}, {12566529,13}, {13623297,14}, {14663681,13}, {15196161,15}, {16744449,14}, {16760833,13}, {17293313,14}, {18857985,13}, {19394561,14}, {MP_SIZE_T_MAX,0}}
-#define MUL_FFT_TABLE2_SIZE 117
 
 #define SQR_FFT_TABLE2 {{1,4}, {305,5}, {609,6}, {1601,7}, {4737,8}, {4865,7}, {5121,8}, {11009,9}, {11777,8}, {13057,9}, {13825,10}, {15361,9}, {15873,8}, {16129,9}, {22017,10}, {23553,9}, {28161,10}, {31745,9}, {36353,10}, {39937,9}, {42497,10}, {56321,11}, {63489,10}, {89601,11}, {96257,10}, {107521,12}, {126977,11}, {129025,10}, {135169,11}, {137217,10}, {139265,11}, {163841,10}, {173057,11}, {195073,9}, {196097,11}, {196609,10}, {201729,11}, {212993,12}, {217089,11}, {221185,12}, {258049,11}, {260609,10}, {261121,9}, {261633,11}, {292865,10}, {296961,11}, {299009,10}, {302081,11}, {325633,12}, {389121,11}, {392193,9}, {392705,11}, {393217,13}, {401409,11}, {404481,13}, {421889,11}, {424961,13}, {516097,12}, {520193,11}, {526337,10}, {532481,11}, {542721,10}, {543745,11}, {593921,12}, {598017,11}, {608257,12}, {610305,11}, {616449,12}, {651265,11}, {653313,10}, {687617,11}, {718849,10}, {749569,12}, {782337,11}, {784385,10}, {788481,11}, {793601,10}, {800769,11}, {802817,10}, {813057,11}, {850945,12}, {913409,11}, {917505,13}, {1040385,12}, {1044481,11}, {1113089,12}, {1175553,11}, {1243137,12}, {1309697,11}, {1347585,12}, {1351681,11}, {1368065,12}, {1437697,11}, {1503233,13}, {1564673,12}, {1568769,11}, {1628161,12}, {1839105,14}, {1851393,12}, {1884161,14}, {2080769,13}, {2088961,12}, {2488321,13}, {2613249,12}, {3010561,13}, {3137537,12}, {3403777,13}, {3661825,12}, {3928065,14}, {4177921,13}, {4186113,12}, {4452353,13}, {4710401,12}, {4976641,13}, {5234689,12}, {5500929,13}, {5758977,12}, {5763073,14}, {6275073,13}, {6283265,12}, {6549505,13}, {7856129,15}, {8011777,14}, {8060929,15}, {8355841,14}, {8372225,13}, {9953281,14}, {10469377,13}, {12050433,14}, {12566529,13}, {13623297,14}, {14663681,13}, {15196161,15}, {16744449,14}, {16760833,13}, {17293313,14}, {23052289,15}, {25133057,14}, {29343745,16}, {MP_SIZE_T_MAX,0}}
-#define SQR_FFT_TABLE2_SIZE 140
+
+#define INV_NEWTON_THRESHOLD             47
+#define BINV_NEWTON_THRESHOLD            18
--- 1/mpn/x86_64/invert_limb.asm
+++ 2/mpn/x86_64/invert_limb.asm
@@ -1,8 +1,6 @@
 dnl  AMD64 mpn_invert_limb -- Invert a normalized limb.
 
-dnl  Contributed to the GNU project by Torbjorn Granlund and Niels Möller.
-
-dnl  Copyright 2004, 2007, 2008, 2009 Free Software Foundation, Inc.
+dnl  Copyright 2004, 2007, 2008 Free Software Foundation, Inc.
 
 dnl  This file is part of the GNU MP Library.
 
@@ -23,12 +21,10 @@
 
 
 C	     cycles/limb (approx)	div
-C K8,K9:	 48			 71
-C K10:		 48			 77
-C P4:	        135			161
-C P6 core2:	 69			116
-C P6 corei7:	 55			 89
-C P6 atom:	129			191
+C K8:		 40			 71
+C P4:		141			161
+C P6-15 (Core2): 63			116
+C P6-28 (Atom): 130			191
 
 C rax rcx rdx rdi rsi r8
 
@@ -36,9 +32,9 @@
 ASM_START()
 	TEXT
 	ALIGN(16)
-PROLOGUE(mpn_invert_limb)		C			Kn	C2	Ci
-	mov	%rdi, %rax		C			 0	 0	 0
-	shr	$55, %rax		C			 1	 1	 1
+PROLOGUE(mpn_invert_limb)
+	mov	%rdi, %rax
+	shr	$55, %rax
 ifdef(`PIC',`
 ifdef(`DARWIN',`
 	mov	approx_tab@GOTPCREL(%rip), %r8
@@ -48,86 +44,78 @@
 ')',`
 	movabs	$-512+approx_tab, %r8
 ')
-	movzwl	(%r8,%rax,2), R32(%rcx)	C	%rcx = v0
-
-	C v1 = (v0 << 11) - (v0*v0*d40 >> 40) - 1
-	mov	%rdi, %rsi		C			 0	 0	 0
-	mov	R32(%rcx), R32(%rax)	C			 4	 5	 5
-	imul	R32(%rcx), R32(%rcx)	C			 4	 5	 5
-	shr	$24, %rsi		C			 1	 1	 1
-	inc	%rsi			C	%rsi = d40
-	imul	%rsi, %rcx		C			 8	10	 8
-	shr	$40, %rcx		C			12	15	11
-	sal	$11, R32(%rax)		C			 5	 6	 6
-	dec	R32(%rax)
-	sub	R32(%rcx), R32(%rax)	C	%rax = v1
-
-	C v2 = (v1 << 13) + (v1 * (2^60 - v1*d40) >> 47
-	mov	$0x1000000000000000, %rcx
-	imul	%rax, %rsi		C			14	17	13
-	sub	%rsi, %rcx
-	imul	%rax, %rcx
-	sal	$13, %rax
-	shr	$47, %rcx
-	add	%rax, %rcx		C	%rcx = v2
-
-	C v3 = (v2 << 31) + (v2 * (2^96 - v2 * d63 + (v2>>1) & mask) >> 65
-	mov	%rdi, %rsi		C			 0	 0	 0
-	shr	$1, %rsi		C d/2
-	sbb	%rax, %rax		C -d0 = -(d mod 2)
-	sub	%rax, %rsi		C d63 = ceil(d/2)
-	imul	%rcx, %rsi		C v2 * d63
-	and	%rcx, %rax		C v2 * d0
-	shr	$1, %rax		C (v2>>1) * d0
-	sub	%rsi, %rax		C (v2>>1) * d0 - v2 * d63
-	mul	%rcx
-	sal	$31, %rcx
-	shr	$1, %rdx
-	add	%rdx, %rcx		C	%rcx = v3
-
-	mov	%rdi, %rax
-	mul	%rcx
-	add	%rdi, %rax
+	movzwl	(%r8,%rax,2), R32(%rcx)
+	mov	%rdi, %rsi
+	mov	R32(%rcx), R32(%rax)
+	imul	R32(%rcx), R32(%rcx)
+	shr	$32, %rsi
+	imul	%rsi, %rcx
+	shr	$31, %rcx
+	sal	$17, %rax
+	sub	%rcx, %rax
+	mov	%rax, %r8
+	imul	%rax, %rax
+	sal	$33, %r8
+	mul	%rdi
+	neg	%rdx
+	lea	(%r8,%rdx,2), %rax
+	mov	%rax, %r8
+	mul	%rax
+	mov	%rax, %rcx
+	mov	%rdx, %rax
+	mul	%rdi
+	mov	%rax, %rsi
 	mov	%rcx, %rax
-	adc	%rdi, %rdx
-	sub	%rdx, %rax
-
+	mov	%rdx, %rcx
+	mul	%rdi
+	add	%rdx, %rsi
+	sbb	%rcx, %r8
+	shr	$62, %rsi
+	add	$1, %rsi
+	sal	$2, %r8
+	sub	%rsi, %r8
+	mov	%rdi, %rax
+	mul	%r8
+	add	%rdi, %rax		C xl += d
+	adc	%rdi, %rdx		C xh += d
+	mov	%r8, %rax
+	sub	%rdx, %rax		C return zh - xh
 	ret
 EPILOGUE()
 
 	RODATA
 	ALIGN(2)
 approx_tab:
-	.value	0x7fd,0x7f5,0x7ed,0x7e5,0x7dd,0x7d5,0x7ce,0x7c6
-	.value	0x7bf,0x7b7,0x7b0,0x7a8,0x7a1,0x79a,0x792,0x78b
-	.value	0x784,0x77d,0x776,0x76f,0x768,0x761,0x75b,0x754
-	.value	0x74d,0x747,0x740,0x739,0x733,0x72c,0x726,0x720
-	.value	0x719,0x713,0x70d,0x707,0x700,0x6fa,0x6f4,0x6ee
-	.value	0x6e8,0x6e2,0x6dc,0x6d6,0x6d1,0x6cb,0x6c5,0x6bf
-	.value	0x6ba,0x6b4,0x6ae,0x6a9,0x6a3,0x69e,0x698,0x693
-	.value	0x68d,0x688,0x683,0x67d,0x678,0x673,0x66e,0x669
-	.value	0x664,0x65e,0x659,0x654,0x64f,0x64a,0x645,0x640
-	.value	0x63c,0x637,0x632,0x62d,0x628,0x624,0x61f,0x61a
-	.value	0x616,0x611,0x60c,0x608,0x603,0x5ff,0x5fa,0x5f6
-	.value	0x5f1,0x5ed,0x5e9,0x5e4,0x5e0,0x5dc,0x5d7,0x5d3
-	.value	0x5cf,0x5cb,0x5c6,0x5c2,0x5be,0x5ba,0x5b6,0x5b2
-	.value	0x5ae,0x5aa,0x5a6,0x5a2,0x59e,0x59a,0x596,0x592
-	.value	0x58e,0x58a,0x586,0x583,0x57f,0x57b,0x577,0x574
-	.value	0x570,0x56c,0x568,0x565,0x561,0x55e,0x55a,0x556
-	.value	0x553,0x54f,0x54c,0x548,0x545,0x541,0x53e,0x53a
-	.value	0x537,0x534,0x530,0x52d,0x52a,0x526,0x523,0x520
-	.value	0x51c,0x519,0x516,0x513,0x50f,0x50c,0x509,0x506
-	.value	0x503,0x500,0x4fc,0x4f9,0x4f6,0x4f3,0x4f0,0x4ed
-	.value	0x4ea,0x4e7,0x4e4,0x4e1,0x4de,0x4db,0x4d8,0x4d5
-	.value	0x4d2,0x4cf,0x4cc,0x4ca,0x4c7,0x4c4,0x4c1,0x4be
-	.value	0x4bb,0x4b9,0x4b6,0x4b3,0x4b0,0x4ad,0x4ab,0x4a8
-	.value	0x4a5,0x4a3,0x4a0,0x49d,0x49b,0x498,0x495,0x493
-	.value	0x490,0x48d,0x48b,0x488,0x486,0x483,0x481,0x47e
-	.value	0x47c,0x479,0x477,0x474,0x472,0x46f,0x46d,0x46a
-	.value	0x468,0x465,0x463,0x461,0x45e,0x45c,0x459,0x457
-	.value	0x455,0x452,0x450,0x44e,0x44b,0x449,0x447,0x444
-	.value	0x442,0x440,0x43e,0x43b,0x439,0x437,0x435,0x432
-	.value	0x430,0x42e,0x42c,0x42a,0x428,0x425,0x423,0x421
-	.value	0x41f,0x41d,0x41b,0x419,0x417,0x414,0x412,0x410
-	.value	0x40e,0x40c,0x40a,0x408,0x406,0x404,0x402,0x400
+	.value	0xffc0,0xfec0,0xfdc0,0xfcc0,0xfbc0,0xfac0,0xfa00,0xf900
+	.value	0xf800,0xf700,0xf640,0xf540,0xf440,0xf380,0xf280,0xf180
+	.value	0xf0c0,0xefc0,0xef00,0xee00,0xed40,0xec40,0xeb80,0xeac0
+	.value	0xe9c0,0xe900,0xe840,0xe740,0xe680,0xe5c0,0xe500,0xe400
+	.value	0xe340,0xe280,0xe1c0,0xe100,0xe040,0xdf80,0xdec0,0xde00
+	.value	0xdd40,0xdc80,0xdbc0,0xdb00,0xda40,0xd980,0xd8c0,0xd800
+	.value	0xd740,0xd680,0xd600,0xd540,0xd480,0xd3c0,0xd340,0xd280
+	.value	0xd1c0,0xd140,0xd080,0xcfc0,0xcf40,0xce80,0xcdc0,0xcd40
+	.value	0xcc80,0xcc00,0xcb40,0xcac0,0xca00,0xc980,0xc8c0,0xc840
+	.value	0xc780,0xc700,0xc640,0xc5c0,0xc540,0xc480,0xc400,0xc380
+	.value	0xc2c0,0xc240,0xc1c0,0xc100,0xc080,0xc000,0xbf80,0xbec0
+	.value	0xbe40,0xbdc0,0xbd40,0xbc80,0xbc00,0xbb80,0xbb00,0xba80
+	.value	0xba00,0xb980,0xb900,0xb840,0xb7c0,0xb740,0xb6c0,0xb640
+	.value	0xb5c0,0xb540,0xb4c0,0xb440,0xb3c0,0xb340,0xb2c0,0xb240
+	.value	0xb1c0,0xb140,0xb0c0,0xb080,0xb000,0xaf80,0xaf00,0xae80
+	.value	0xae00,0xad80,0xad40,0xacc0,0xac40,0xabc0,0xab40,0xaac0
+	.value	0xaa80,0xaa00,0xa980,0xa900,0xa8c0,0xa840,0xa7c0,0xa740
+	.value	0xa700,0xa680,0xa600,0xa5c0,0xa540,0xa4c0,0xa480,0xa400
+	.value	0xa380,0xa340,0xa2c0,0xa240,0xa200,0xa180,0xa140,0xa0c0
+	.value	0xa080,0xa000,0x9f80,0x9f40,0x9ec0,0x9e80,0x9e00,0x9dc0
+	.value	0x9d40,0x9d00,0x9c80,0x9c40,0x9bc0,0x9b80,0x9b00,0x9ac0
+	.value	0x9a40,0x9a00,0x9980,0x9940,0x98c0,0x9880,0x9840,0x97c0
+	.value	0x9780,0x9700,0x96c0,0x9680,0x9600,0x95c0,0x9580,0x9500
+	.value	0x94c0,0x9440,0x9400,0x93c0,0x9340,0x9300,0x92c0,0x9240
+	.value	0x9200,0x91c0,0x9180,0x9100,0x90c0,0x9080,0x9000,0x8fc0
+	.value	0x8f80,0x8f40,0x8ec0,0x8e80,0x8e40,0x8e00,0x8d80,0x8d40
+	.value	0x8d00,0x8cc0,0x8c80,0x8c00,0x8bc0,0x8b80,0x8b40,0x8b00
+	.value	0x8a80,0x8a40,0x8a00,0x89c0,0x8980,0x8940,0x88c0,0x8880
+	.value	0x8840,0x8800,0x87c0,0x8780,0x8740,0x8700,0x8680,0x8640
+	.value	0x8600,0x85c0,0x8580,0x8540,0x8500,0x84c0,0x8480,0x8440
+	.value	0x8400,0x8380,0x8340,0x8300,0x82c0,0x8280,0x8240,0x8200
+	.value	0x81c0,0x8180,0x8140,0x8100,0x80c0,0x8080,0x8040,0x8000
 ASM_END()
--- 1/mpn/x86_64/lshift.asm
+++ 2/mpn/x86_64/lshift.asm
@@ -1,6 +1,6 @@
 dnl  AMD64 mpn_lshift -- mpn left shift.
 
-dnl  Copyright 2003, 2005, 2007, 2009 Free Software Foundation, Inc.
+dnl  Copyright 2003, 2005, 2007 Free Software Foundation, Inc.
 dnl
 dnl  This file is part of the GNU MP Library.
 dnl
@@ -38,7 +38,7 @@
 	TEXT
 	ALIGN(32)
 PROLOGUE(mpn_lshift)
-	cmp	$1, R8(%rcx)
+	cmp	$1, %cl
 	jne	L(gen)
 
 C For cnt=1 we want to work from lowest limb towards higher limbs.
@@ -77,27 +77,27 @@
 	dec	n
 	jne	L(t1)
 
-	inc	R32(%rax)
-	dec	R32(%rax)
+	inc	%eax
+	dec	%eax
 	jne	L(n00)
-	adc	R32(%rax), R32(%rax)
+	adc	%eax, %eax
 	ret
-L(e1):	test	R32(%rax), R32(%rax)	C clear cy
+L(e1):	test	%eax, %eax			C clear cy
 L(n00):	mov	(up), %r8
-	dec	R32(%rax)
+	dec	%eax
 	jne	L(n01)
 	adc	%r8, %r8
 	mov	%r8, (rp)
-L(ret):	adc	R32(%rax), R32(%rax)
+L(ret):	adc	%eax, %eax
 	ret
-L(n01):	dec	R32(%rax)
+L(n01):	dec	%eax
 	mov	8(up), %r9
 	jne	L(n10)
 	adc	%r8, %r8
 	adc	%r9, %r9
 	mov	%r8, (rp)
 	mov	%r9, 8(rp)
-	adc	R32(%rax), R32(%rax)
+	adc	%eax, %eax
 	ret
 L(n10):	mov	16(up), %r10
 	adc	%r8, %r8
@@ -106,14 +106,14 @@
 	mov	%r8, (rp)
 	mov	%r9, 8(rp)
 	mov	%r10, 16(rp)
-	adc	$-1, R32(%rax)
+	adc	$-1, %eax
 	ret
 
-L(gen):	neg	R32(%rcx)		C put rsh count in cl
+L(gen):	neg	%ecx			C put rsh count in cl
 	mov	-8(up,n,8), %rax
-	shr	R8(%rcx), %rax		C function return value
+	shr	%cl, %rax		C function return value
 
-	neg	R32(%rcx)		C put lsh count in cl
+	neg	%ecx			C put lsh count in cl
 	lea	1(n), R32(%r8)
 	and	$3, R32(%r8)
 	je	L(rlx)			C jump for n = 3, 7, 11, ...
@@ -122,10 +122,10 @@
 	jne	L(1)
 C	n = 4, 8, 12, ...
 	mov	-8(up,n,8), %r10
-	shl	R8(%rcx), %r10
-	neg	R32(%rcx)		C put rsh count in cl
+	shl	%cl, %r10
+	neg	%ecx			C put rsh count in cl
 	mov	-16(up,n,8), %r8
-	shr	R8(%rcx), %r8
+	shr	%cl, %r8
 	or	%r8, %r10
 	mov	%r10, -8(rp,n,8)
 	dec	n
@@ -135,90 +135,90 @@
 	je	L(1x)			C jump for n = 1, 5, 9, 13, ...
 C	n = 2, 6, 10, 16, ...
 	mov	-8(up,n,8), %r10
-	shl	R8(%rcx), %r10
-	neg	R32(%rcx)		C put rsh count in cl
+	shl	%cl, %r10
+	neg	%ecx			C put rsh count in cl
 	mov	-16(up,n,8), %r8
-	shr	R8(%rcx), %r8
+	shr	%cl, %r8
 	or	%r8, %r10
 	mov	%r10, -8(rp,n,8)
 	dec	n
-	neg	R32(%rcx)		C put lsh count in cl
+	neg	%ecx			C put lsh count in cl
 L(1x):
 	cmp	$1, n
 	je	L(ast)
 	mov	-8(up,n,8), %r10
-	shl	R8(%rcx), %r10
+	shl	%cl, %r10
 	mov	-16(up,n,8), %r11
-	shl	R8(%rcx), %r11
-	neg	R32(%rcx)		C put rsh count in cl
+	shl	%cl, %r11
+	neg	%ecx			C put rsh count in cl
 	mov	-16(up,n,8), %r8
 	mov	-24(up,n,8), %r9
-	shr	R8(%rcx), %r8
+	shr	%cl, %r8
 	or	%r8, %r10
-	shr	R8(%rcx), %r9
+	shr	%cl, %r9
 	or	%r9, %r11
 	mov	%r10, -8(rp,n,8)
 	mov	%r11, -16(rp,n,8)
 	sub	$2, n
 
-L(rll):	neg	R32(%rcx)		C put lsh count in cl
+L(rll):	neg	%ecx			C put lsh count in cl
 L(rlx):	mov	-8(up,n,8), %r10
-	shl	R8(%rcx), %r10
+	shl	%cl, %r10
 	mov	-16(up,n,8), %r11
-	shl	R8(%rcx), %r11
+	shl	%cl, %r11
 
 	sub	$4, n			C				      4
 	jb	L(end)			C				      2
 	ALIGN(16)
 L(top):
 	C finish stuff from lsh block
-	neg	R32(%rcx)		C put rsh count in cl
+	neg	%ecx			C put rsh count in cl
 	mov	16(up,n,8), %r8
 	mov	8(up,n,8), %r9
-	shr	R8(%rcx), %r8
+	shr	%cl, %r8
 	or	%r8, %r10
-	shr	R8(%rcx), %r9
+	shr	%cl, %r9
 	or	%r9, %r11
 	mov	%r10, 24(rp,n,8)
 	mov	%r11, 16(rp,n,8)
 	C start two new rsh
 	mov	0(up,n,8), %r8
 	mov	-8(up,n,8), %r9
-	shr	R8(%rcx), %r8
-	shr	R8(%rcx), %r9
+	shr	%cl, %r8
+	shr	%cl, %r9
 
 	C finish stuff from rsh block
-	neg	R32(%rcx)		C put lsh count in cl
+	neg	%ecx			C put lsh count in cl
 	mov	8(up,n,8), %r10
 	mov	0(up,n,8), %r11
-	shl	R8(%rcx), %r10
+	shl	%cl, %r10
 	or	%r10, %r8
-	shl	R8(%rcx), %r11
+	shl	%cl, %r11
 	or	%r11, %r9
 	mov	%r8, 8(rp,n,8)
 	mov	%r9, 0(rp,n,8)
 	C start two new lsh
 	mov	-8(up,n,8), %r10
 	mov	-16(up,n,8), %r11
-	shl	R8(%rcx), %r10
-	shl	R8(%rcx), %r11
+	shl	%cl, %r10
+	shl	%cl, %r11
 
 	sub	$4, n
 	jae	L(top)			C				      2
 L(end):
-	neg	R32(%rcx)		C put rsh count in cl
-	mov	8(up), %r8
-	shr	R8(%rcx), %r8
+	neg	%ecx			C put rsh count in cl
+	mov	16(up,n,8), %r8
+	shr	%cl, %r8
 	or	%r8, %r10
-	mov	(up), %r9
-	shr	R8(%rcx), %r9
+	mov	8(up,n,8), %r9
+	shr	%cl, %r9
 	or	%r9, %r11
-	mov	%r10, 16(rp)
-	mov	%r11, 8(rp)
+	mov	%r10, 24(rp,n,8)
+	mov	%r11, 16(rp,n,8)
 
-	neg	R32(%rcx)		C put lsh count in cl
+	neg	%ecx			C put lsh count in cl
 L(ast):	mov	(up), %r10
-	shl	R8(%rcx), %r10
+	shl	%cl, %r10
 	mov	%r10, (rp)
 	ret
 EPILOGUE()
--- 1/mpn/x86_64/mod_34lsub1.asm
+++ 2/mpn/x86_64/mod_34lsub1.asm
@@ -37,8 +37,8 @@
 
 C TODO
 C  * Apply the movzwl tricks to the x86/k7 code
-C  * Review feed-in and wind-down code.  In particular, try to avoid adc and
-C    sbb to placate Pentium4.
+C  * Review feed-in and wind-down code.  In particular, try to avoid adcq and
+C    sbbq to placate Pentium4.
 C  * More unrolling and/or index addressing could bring time to under 1 c/l
 C    for Athlon64, approaching 0.67 c/l seems possible.
 C  * There are recurrencies on the carry registers (r8, r9, r10) that might
@@ -47,14 +47,14 @@
 C  * For ultimate Athlon64 performance, a sequence like this might be best.
 C    It should reach 0.5 c/l (limited by L1 cache bandwidth).
 C
-C	add	(%rdi), %rax
-C	adc	8(%rdi), %rcx
-C	adc	16(%rdi), %rdx
-C	adc	$0, %r8
-C	add	24(%rdi), %rax
-C	adc	32(%rdi), %rcx
-C	adc	40(%rdi), %rdx
-C	adc	$0, %r8
+C	addq	(%rdi), %rax
+C	adcq	8(%rdi), %rcx
+C	adcq	16(%rdi), %rdx
+C	adcq	$0, %r8
+C	addq	24(%rdi), %rax
+C	adcq	32(%rdi), %rcx
+C	adcq	40(%rdi), %rdx
+C	adcq	$0, %r8
 C	...
 
 
--- 1/mpn/x86_64/mode1o.asm
+++ 2/mpn/x86_64/mode1o.asm
@@ -25,9 +25,8 @@
 C K8,K9:	10
 C K10:		10
 C P4:		33
-C P6 core2:	13
-C P6 corei7:	14.5
-C P6 Atom:	35
+C P6-15 (Core2):13
+C P6-28 (Atom):	35
 
 
 C mp_limb_t mpn_modexact_1_odd (mp_srcptr src, mp_size_t size,
--- 1/mpn/x86_64/mul_1.asm
+++ 2/mpn/x86_64/mul_1.asm
@@ -23,9 +23,9 @@
 C K8,K9:	 2.5
 C K10:		 2.5
 C P4:		 12.3
-C P6 core2:	 4.0
-C P6 corei7:	 3.8
-C Atom:		19.8
+C P6-15:	 4.0
+C P6-15 (Core2): 4.0
+C P6-28 (Atom):	19.8
 
 C The inner loop of this code is the result of running a code generation and
 C optimization tool suite written by David Harvey and Torbjorn Granlund.
--- 1/mpn/x86_64/mul_2.asm
+++ 2/mpn/x86_64/mul_2.asm
@@ -24,8 +24,7 @@
 C K8,K9:	 2.275
 C K10:		 2.275
 C P4:		 ?
-C P6 core2:	 4.0
-C P6 corei7:	 3.8
+C P6-15:	 4.0
 
 C This code is the result of running a code generation and optimization tool
 C suite written by David Harvey and Torbjorn Granlund.
--- 1/mpn/x86_64/mul_basecase.asm
+++ 2/mpn/x86_64/mul_basecase.asm
@@ -33,6 +33,7 @@
 C TODO
 C  * Use fewer registers.  (how??? I can't see it -- david)
 C  * Avoid some "mov $0,r" and instead use "xor r,r".
+C  * Don't align loops to a 32-byte boundaries.
 C  * Can the top of each L(addmul_outer_n) prologue be folded into the
 C    mul_1/mul_2 prologues, saving a LEA (%rip)? It would slow down the
 C    case where vn = 1 or 2; is it worth it?
--- 1/mpn/x86_64/pentium4/gmp-mparam.h
+++ 2/mpn/x86_64/pentium4/gmp-mparam.h
@@ -18,75 +18,62 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 64
+#define BITS_PER_MP_LIMB 64
 #define BYTES_PER_MP_LIMB 8
 
 /* These routines exists for all x86_64 chips, but they are slower on Pentium4
    than separate add/sub and shift.  Make sure they are not really used.  */
+#undef HAVE_NATIVE_mpn_addlsh1_n
+#undef HAVE_NATIVE_mpn_sublsh1_n
 #undef HAVE_NATIVE_mpn_rsh1add_n
 #undef HAVE_NATIVE_mpn_rsh1sub_n
 
 /* 3200 MHz Pentium / 2048 Kibyte cache / socket 775 */
 
-/* Generated by tuneup.c, 2009-11-29, gcc 4.2 */
+/* Generated by tuneup.c, 2009-01-15, gcc 3.4 */
 
-#define MUL_TOOM22_THRESHOLD             12
-#define MUL_TOOM33_THRESHOLD             81
-#define MUL_TOOM44_THRESHOLD            118
+#define MUL_KARATSUBA_THRESHOLD          12
+#define MUL_TOOM3_THRESHOLD              91
+#define MUL_TOOM44_THRESHOLD            136
 
 #define SQR_BASECASE_THRESHOLD            0  /* always (native) */
-#define SQR_TOOM2_THRESHOLD              22
-#define SQR_TOOM3_THRESHOLD              85
-#define SQR_TOOM4_THRESHOLD             234
-
-#define MUL_FFT_TABLE  { 240, 544, 1600, 2816, 7168, 20480, 49152, 196608, 786432, 0 }
-#define MUL_FFT_MODF_THRESHOLD          272
-#define MUL_FFT_THRESHOLD              2432
-
-#define SQR_FFT_TABLE  { 240, 544, 1472, 1792, 7168, 20480, 49152, 196608, 786432, 0 }
-#define SQR_FFT_MODF_THRESHOLD          240
-#define SQR_FFT_THRESHOLD              1920
-
-#define MULLO_BASECASE_THRESHOLD          0  /* always */
-#define MULLO_DC_THRESHOLD               28
-#define MULLO_MUL_N_THRESHOLD          3913
-
-#define MULMOD_BNM1_THRESHOLD            10
-
-#define DC_DIV_QR_THRESHOLD              30
-#define DC_DIVAPPR_Q_THRESHOLD           65
-#define DC_BDIV_QR_THRESHOLD             31
-#define DC_BDIV_Q_THRESHOLD              56
-#define INV_MULMOD_BNM1_THRESHOLD       108
-#define INV_NEWTON_THRESHOLD            226
-#define INV_APPR_THRESHOLD               29
-#define BINV_NEWTON_THRESHOLD           260
-#define REDC_1_TO_REDC_2_THRESHOLD       16
-#define REDC_2_TO_REDC_N_THRESHOLD       51
-
-#define MATRIX22_STRASSEN_THRESHOLD      19
-#define HGCD_THRESHOLD                  107
-#define GCD_DC_THRESHOLD                242
-#define GCDEXT_DC_THRESHOLD             246
+#define SQR_KARATSUBA_THRESHOLD          28
+#define SQR_TOOM3_THRESHOLD              97
+#define SQR_TOOM4_THRESHOLD             218
+
+#define MULLOW_BASECASE_THRESHOLD         0  /* always */
+#define MULLOW_DC_THRESHOLD              28
+#define MULLOW_MUL_N_THRESHOLD          246
+
+#define DIV_SB_PREINV_THRESHOLD           0  /* always */
+#define DIV_DC_THRESHOLD                 35
+#define POWM_THRESHOLD                   59
+
+#define MATRIX22_STRASSEN_THRESHOLD      25
+#define HGCD_THRESHOLD                  112
+#define GCD_DC_THRESHOLD                258
+#define GCDEXT_DC_THRESHOLD             311
 #define JACOBI_BASE_METHOD                1
 
 #define MOD_1_NORM_THRESHOLD              0  /* always */
 #define MOD_1_UNNORM_THRESHOLD            0  /* always */
 #define MOD_1_1_THRESHOLD                 5
-#define MOD_1_2_THRESHOLD                 8
-#define MOD_1_4_THRESHOLD                23
+#define MOD_1_2_THRESHOLD                 7
+#define MOD_1_4_THRESHOLD                28
 #define USE_PREINV_DIVREM_1               1  /* native */
-#define USE_PREINV_MOD_1                  0
+#define USE_PREINV_MOD_1                  1
 #define DIVEXACT_1_THRESHOLD              0  /* always (native) */
 #define MODEXACT_1_ODD_THRESHOLD          0  /* always (native) */
 
-#define GET_STR_DC_THRESHOLD             12
-#define GET_STR_PRECOMPUTE_THRESHOLD     25
-#define SET_STR_DC_THRESHOLD            333
-#define SET_STR_PRECOMPUTE_THRESHOLD   1330
-
-#define MUL_FFT_TABLE2 {{1,4}, {145,5}, {353,6}, {961,7}, {2689,8}, {6913,10}, {7169,9}, {7681,8}, {8449,9}, {13825,10}, {15361,9}, {19969,10}, {23553,9}, {26113,11}, {30721,10}, {31745,9}, {34561,10}, {48129,11}, {63489,10}, {81921,11}, {96257,10}, {97793,12}, {126977,11}, {133121,10}, {135169,11}, {137217,10}, {139777,9}, {141313,10}, {145409,11}, {161793,10}, {163841,11}, {165889,10}, {169985,11}, {172033,10}, {177153,11}, {206849,12}, {212993,11}, {217089,12}, {258049,11}, {358401,12}, {389121,11}, {391169,10}, {397313,11}, {410625,10}, {421889,11}, {450561,13}, {516097,12}, {520193,11}, {587777,12}, {651265,11}, {718849,12}, {790529,11}, {800769,12}, {815105,11}, {821249,12}, {833537,11}, {845825,12}, {915457,13}, {1040385,12}, {1437697,13}, {1564673,12}, {1830913,14}, {2088961,12}, {2355201,13}, {2613249,12}, {2879489,13}, {3137537,12}, {3405825,13}, {3661825,14}, {4186113,12}, {4454401,13}, {4714497,11}, {4979713,13}, {MP_SIZE_T_MAX, 0}}
-#define MUL_FFT_TABLE2_SIZE 73
-
-#define SQR_FFT_TABLE2 {{1,4}, {177,5}, {417,6}, {961,7}, {3073,8}, {6913,10}, {7169,9}, {7681,8}, {8449,9}, {13825,10}, {15361,9}, {19969,10}, {23553,9}, {26113,11}, {30721,10}, {48129,11}, {63489,10}, {65537,8}, {65793,10}, {80897,11}, {96257,12}, {126977,11}, {129025,10}, {136193,11}, {137217,10}, {146433,9}, {146945,11}, {161793,10}, {176129,11}, {195073,10}, {201729,11}, {202753,10}, {206849,11}, {219137,12}, {258049,11}, {260097,10}, {262145,11}, {270337,10}, {272385,11}, {279553,10}, {280577,11}, {286721,10}, {287745,11}, {359425,12}, {389121,11}, {423937,13}, {516097,12}, {520193,11}, {587777,12}, {651265,11}, {718849,12}, {782337,11}, {802817,12}, {806913,11}, {849921,12}, {915457,13}, {1040385,12}, {1437697,13}, {1564673,12}, {1830913,14}, {2080769,13}, {2088961,12}, {2355201,13}, {2613249,12}, {2752513,13}, {2785281,12}, {2801665,13}, {2818049,12}, {2838529,13}, {2863105,12}, {2867201,13}, {3137537,12}, {3403777,13}, {3661825,14}, {4177921,13}, {4186113,12}, {4452353,13}, {4714497,11}, {4979713,13}, {MP_SIZE_T_MAX, 0}}
-#define SQR_FFT_TABLE2_SIZE 79
+#define GET_STR_DC_THRESHOLD             15
+#define GET_STR_PRECOMPUTE_THRESHOLD     24
+#define SET_STR_DC_THRESHOLD            866
+#define SET_STR_PRECOMPUTE_THRESHOLD   1646
+
+#define MUL_FFT_TABLE  { 240, 416, 1216, 2304, 7168, 20480, 49152, 196608, 786432, 0 }
+#define MUL_FFT_MODF_THRESHOLD          256
+#define MUL_FFT_THRESHOLD              2944
+
+#define SQR_FFT_TABLE  { 208, 480, 1600, 2304, 7168, 20480, 49152, 196608, 786432, 0 }
+#define SQR_FFT_MODF_THRESHOLD          224
+#define SQR_FFT_THRESHOLD              2688
--- 1/mpn/x86_64/redc_1.asm
+++ 2/mpn/x86_64/redc_1.asm
@@ -33,11 +33,11 @@
 C    The code for 1, 2, 3, 4 should perhaps be completely register based.
 C  * Perhaps align outer loops.
 C  * The sub_n at the end leaks side-channel data.  How do we fix that?
-C  * Write mpn_add_n_sub_n computing R = A + B - C.  It should run at 2 c/l.
+C  * Write mpn_addsub_n computing R = A + B - C.  It should run at 2 c/l.
 C  * We could software pipeline the IMUL stuff, by putting it before the
 C    outer loops and before the end of the outer loops.  The last outer
 C    loop iteration would then compute an unneeded product, but it is at
-C    least not a stray read from up[], since it is at up[n].
+C    least not a stray read fro up[], since it is at up[n].
 C  * Can we combine both the add_n and sub_n into the loops, somehow?
 
 C INPUT PARAMETERS
--- 1/mpn/x86_64/rshift.asm
+++ 2/mpn/x86_64/rshift.asm
@@ -1,6 +1,6 @@
 dnl  AMD64 mpn_rshift -- mpn left shift.
 
-dnl  Copyright 2003, 2005, 2009 Free Software Foundation, Inc.
+dnl  Copyright 2003, 2005 Free Software Foundation, Inc.
 dnl
 dnl  This file is part of the GNU MP Library.
 dnl
@@ -38,10 +38,10 @@
 	TEXT
 	ALIGN(32)
 PROLOGUE(mpn_rshift)
-	neg	R32(%rcx)		C put rsh count in cl
+	neg	%ecx			C put rsh count in cl
 	mov	(up), %rax
-	shl	R8(%rcx), %rax		C function return value
-	neg	R32(%rcx)		C put lsh count in cl
+	shl	%cl, %rax		C function return value
+	neg	%ecx			C put lsh count in cl
 
 	lea	1(n), R32(%r8)
 
@@ -56,10 +56,10 @@
 	jne	L(1)
 C	n = 4, 8, 12, ...
 	mov	8(up,n,8), %r10
-	shr	R8(%rcx), %r10
-	neg	R32(%rcx)		C put rsh count in cl
+	shr	%cl, %r10
+	neg	%ecx			C put rsh count in cl
 	mov	16(up,n,8), %r8
-	shl	R8(%rcx), %r8
+	shl	%cl, %r8
 	or	%r8, %r10
 	mov	%r10, 8(rp,n,8)
 	inc	n
@@ -69,90 +69,90 @@
 	je	L(1x)			C jump for n = 1, 5, 9, 13, ...
 C	n = 2, 6, 10, 16, ...
 	mov	8(up,n,8), %r10
-	shr	R8(%rcx), %r10
-	neg	R32(%rcx)		C put rsh count in cl
+	shr	%cl, %r10
+	neg	%ecx			C put rsh count in cl
 	mov	16(up,n,8), %r8
-	shl	R8(%rcx), %r8
+	shl	%cl, %r8
 	or	%r8, %r10
 	mov	%r10, 8(rp,n,8)
 	inc	n
-	neg	R32(%rcx)		C put lsh count in cl
+	neg	%ecx			C put lsh count in cl
 L(1x):
 	cmp	$-1, n
 	je	L(ast)
 	mov	8(up,n,8), %r10
-	shr	R8(%rcx), %r10
+	shr	%cl, %r10
 	mov	16(up,n,8), %r11
-	shr	R8(%rcx), %r11
-	neg	R32(%rcx)		C put rsh count in cl
+	shr	%cl, %r11
+	neg	%ecx			C put rsh count in cl
 	mov	16(up,n,8), %r8
 	mov	24(up,n,8), %r9
-	shl	R8(%rcx), %r8
+	shl	%cl, %r8
 	or	%r8, %r10
-	shl	R8(%rcx), %r9
+	shl	%cl, %r9
 	or	%r9, %r11
 	mov	%r10, 8(rp,n,8)
 	mov	%r11, 16(rp,n,8)
 	add	$2, n
 
-L(rll):	neg	R32(%rcx)		C put lsh count in cl
+L(rll):	neg	%ecx			C put lsh count in cl
 L(rlx):	mov	8(up,n,8), %r10
-	shr	R8(%rcx), %r10
+	shr	%cl, %r10
 	mov	16(up,n,8), %r11
-	shr	R8(%rcx), %r11
+	shr	%cl, %r11
 
 	add	$4, n			C				      4
 	jb	L(end)			C				      2
 	ALIGN(16)
 L(top):
 	C finish stuff from lsh block
-	neg	R32(%rcx)		C put rsh count in cl
+	neg	%ecx			C put rsh count in cl
 	mov	-16(up,n,8), %r8
 	mov	-8(up,n,8), %r9
-	shl	R8(%rcx), %r8
+	shl	%cl, %r8
 	or	%r8, %r10
-	shl	R8(%rcx), %r9
+	shl	%cl, %r9
 	or	%r9, %r11
 	mov	%r10, -24(rp,n,8)
 	mov	%r11, -16(rp,n,8)
 	C start two new rsh
 	mov	(up,n,8), %r8
 	mov	8(up,n,8), %r9
-	shl	R8(%rcx), %r8
-	shl	R8(%rcx), %r9
+	shl	%cl, %r8
+	shl	%cl, %r9
 
 	C finish stuff from rsh block
-	neg	R32(%rcx)		C put lsh count in cl
+	neg	%ecx			C put lsh count in cl
 	mov	-8(up,n,8), %r10
 	mov	0(up,n,8), %r11
-	shr	R8(%rcx), %r10
+	shr	%cl, %r10
 	or	%r10, %r8
-	shr	R8(%rcx), %r11
+	shr	%cl, %r11
 	or	%r11, %r9
 	mov	%r8, -8(rp,n,8)
 	mov	%r9, 0(rp,n,8)
 	C start two new lsh
 	mov	8(up,n,8), %r10
 	mov	16(up,n,8), %r11
-	shr	R8(%rcx), %r10
-	shr	R8(%rcx), %r11
+	shr	%cl, %r10
+	shr	%cl, %r11
 
 	add	$4, n
 	jae	L(top)			C				      2
 L(end):
-	neg	R32(%rcx)		C put rsh count in cl
-	mov	-8(up), %r8
-	shl	R8(%rcx), %r8
+	neg	%ecx			C put rsh count in cl
+	mov	-16(up,n,8), %r8
+	shl	%cl, %r8
 	or	%r8, %r10
-	mov	(up), %r9
-	shl	R8(%rcx), %r9
+	mov	-8(up,n,8), %r9
+	shl	%cl, %r9
 	or	%r9, %r11
-	mov	%r10, -16(rp)
-	mov	%r11, -8(rp)
+	mov	%r10, -24(rp,n,8)
+	mov	%r11, -16(rp,n,8)
 
-	neg	R32(%rcx)		C put lsh count in cl
+	neg	%ecx			C put lsh count in cl
 L(ast):	mov	(up), %r10
-	shr	R8(%rcx), %r10
+	shr	%cl, %r10
 	mov	%r10, (rp)
 	ret
 EPILOGUE()
--- 1/mpn/x86_64/sqr_basecase.asm
+++ 2/mpn/x86_64/sqr_basecase.asm
@@ -25,7 +25,7 @@
 C optimization tool suite written by David Harvey and Torbjorn Granlund.
 
 C NOTES
-C   * This code only handles operands up to SQR_TOOM2_THRESHOLD_MAX.  That
+C   * This code only handles operands up to SQR_KARATSUBA_THRESHOLD_MAX.  That
 C     means we can safely use 32-bit operations for all sizes, unlike in e.g.,
 C     mpn_addmul_1.
 C   * The jump table could probably be optimized, at least for non-pic.
@@ -60,8 +60,8 @@
 C We should really trim this, for better spatial locality.  Alternatively,
 C we could grab the upper part of the stack area, leaving the lower part
 C instead of the upper part unused.
-deflit(SQR_TOOM2_THRESHOLD_MAX, 80)
-define(`STACK_ALLOC', eval(8*2*SQR_TOOM2_THRESHOLD_MAX))
+define(`SQR_KARATSUBA_THRESHOLD_MAX', 120)
+define(`STACK_ALLOC', eval(8*2*SQR_KARATSUBA_THRESHOLD_MAX))
 
 define(`n',	`%r11')
 define(`tp',	`%r12')
@@ -74,6 +74,8 @@
 define(`w2',	`%rbp')
 define(`w3',	`%r10')
 
+define(`SPECIAL_CODE_FOR_4',1)
+
 
 ASM_START()
 	TEXT
@@ -185,6 +187,7 @@
 	pop	%rbx
 	ret
 
+ifdef(`SPECIAL_CODE_FOR_4',`
 L(4):	mov	(up), %rax
 	mul	%rax
 	mov	%rax, (rp)
@@ -256,19 +259,21 @@
 	pop	%rbp
 	pop	%rbx
 	ret
-
+')
 
 L(0m4):	add	$-STACK_ALLOC, %rsp
-	lea	-24(%rsp,n,8), tp		C point tp in middle of result operand
-	mov	(up), v0
-	mov	8(up), %rax
+	lea	(%rsp,n,8), tp		C point tp in middle of result operand
 	lea	(up,n,8), up		C point up at end of input operand
 
-	lea	-4(n), i
+	lea	-1(n), i
 C Function mpn_mul_1_m3(tp, up - i, i, up[-i - 1])
-	xor	R32(j), R32(j)
-	sub	n, j
+	mov	$-1, j
+	sub	i, j
 
+	lea	-24(tp), tp		C offset FIXME
+
+	mov	(up,j,8), v0
+	mov	8(up,j,8), %rax
 	mul	v0
 	xor	R32(w2), R32(w2)
 	mov	%rax, w0
@@ -310,28 +315,31 @@
 	adc	%rdx, w1
 	mov	w2, 8(tp)
 	mov	w1, 16(tp)
-
-	lea	eval(2*8)(tp), tp	C tp += 2
-	lea	-8(up), up
+	lea	eval(24+2*8)(tp), tp	C tp += 2, undo offset FIXME
+ifdef(`SPECIAL_CODE_FOR_4',`',`
+	cmp	$3, R32(i)
+	je	L(last)
+')
 	jmp	L(dowhile)
 
-
 L(1m4):	add	$-STACK_ALLOC, %rsp
 	lea	(%rsp,n,8), tp		C point tp in middle of result operand
-	mov	(up), v0		C u0
-	mov	8(up), %rax		C u1
-	lea	8(up,n,8), up		C point up at end of input operand
+	lea	(up,n,8), up		C point up at end of input operand
 
-	lea	-3(n), i
+	lea	(n), i
 C Function mpn_mul_2s_m0(tp, up - i, i, up - i - 1)
-	lea	-3(n), j
-	neg	j
+	mov	$3, R32(j)
+	sub	i, j
 
-	mov	%rax, v1		C u1
+	lea	8(up), up		C offset FIXME
+
+	mov	-32(up,j,8), v0		C u0
+	mov	-24(up,j,8), v1		C u1
+	mov	-24(up,j,8), %rax	C u1
 	mul	v0			C u0 * u1
 	mov	%rdx, w1
 	xor	R32(w2), R32(w2)
-	mov	%rax, (%rsp)
+	mov	%rax, -24(tp,j,8)
 	jmp	L(m0)
 
 	ALIGN(16)
@@ -373,7 +381,7 @@
 	add	%rax, w3
 	mov	w2, -8(tp,j,8)
 	adc	%rdx, w0
-L(m2x):	mov	(up,j,8), %rax
+	mov	(up,j,8), %rax
 	mul	v0
 	add	%rax, w3
 	adc	%rdx, w0
@@ -389,22 +397,28 @@
 	mov	w0, -8(tp)
 	mov	w1, (tp)
 
-	lea	-16(up), up
-	lea	eval(3*8-24)(tp), tp	C tp += 3
-	jmp	L(dowhile_end)
+	lea	-8(up), up		C undo offset FIXME
+	lea	eval(3*8)(tp), tp	C tp += 3
+	add	$-2, R32(i)		C i -= 2
+	cmp	$3, R32(i)
+	je	L(last)
+	jmp	L(dowhile)
+
 
 
 L(2m4):	add	$-STACK_ALLOC, %rsp
-	lea	-24(%rsp,n,8), tp	C point tp in middle of result operand
-	mov	(up), v0
-	mov	8(up), %rax
+	lea	(%rsp,n,8), tp		C point tp in middle of result operand
 	lea	(up,n,8), up		C point up at end of input operand
 
-	lea	-4(n), i
+	lea	-1(n), i
 C Function mpn_mul_1_m1(tp, up - (i - 1), i - 1, up[-i])
-	lea	-2(n), j
-	neg	j
+	mov	$1, R32(j)
+	sub	i, j
 
+	lea	-24(tp), tp		C offset FIXME
+
+	mov	-16(up,j,8), v0
+	mov	-8(up,j,8), %rax
 	mul	v0
 	mov	%rax, w2
 	mov	(up,j,8), %rax
@@ -446,28 +460,30 @@
 	mov	w2, 8(tp)
 	mov	w1, 16(tp)
 
-	lea	eval(2*8)(tp), tp	C tp += 2
-	lea	-8(up), up
+	lea	eval(24+2*8)(tp), tp	C tp += 2, undo offset FIXME
 	jmp	L(dowhile_mid)
 
 
+
 L(3m4):	add	$-STACK_ALLOC, %rsp
 	lea	(%rsp,n,8), tp		C point tp in middle of result operand
-	mov	(up), v0		C u0
-	mov	8(up), %rax		C u1
-	lea	8(up,n,8), up		C point up at end of input operand
+	lea	(up,n,8), up		C point up at end of input operand
 
-	lea	-5(n), i
+	lea	(n), i
 C Function mpn_mul_2s_m2(tp, up - i + 1, i - 1, up - i)
-	lea	-1(n), j
-	neg	j
+	mov	$1, R32(j)
+	sub	i, j
 
-	mov	%rax, v1		C u1
-	mul	v0			C u0 * u1
+	lea	8(up), up		C offset FIXME
+
+	mov	-16(up,j,8), v0
+	mov	-8(up,j,8), v1
+	mov	-8(up,j,8), %rax
+	mul	v0			C v0 * u0
 	mov	%rdx, w3
 	xor	R32(w0), R32(w0)
 	xor	R32(w1), R32(w1)
-	mov	%rax, (%rsp)
+	mov	%rax, -8(tp,j,8)
 	jmp	L(m2)
 
 	ALIGN(16)
@@ -525,13 +541,18 @@
 	mov	w0, -8(tp)
 	mov	w1, (tp)
 
-	lea	-16(up), up
+	lea	-8(up), up		C undo offset FIXME
+	lea	eval(3*8)(tp), tp	C tp += 3
+	add	$-2, R32(i)		C i -= 2
 	jmp	L(dowhile_mid)
 
 L(dowhile):
 C Function mpn_addmul_2s_m2(tp, up - (i - 1), i - 1, up - i)
-	lea	4(i), j
-	neg	j
+	mov	$-1, j
+	sub	i, j
+
+	lea	-24(tp), tp		C offset FIXME
+	lea	-8(up), up		C offset FIXME
 
 	mov	16(up,j,8), v0
 	mov	24(up,j,8), v1
@@ -600,13 +621,18 @@
 	mov	w1, 16(tp)
 
 	lea	eval(2*8)(tp), tp	C tp += 2
-
 	add	$-2, R32(i)		C i -= 2
 
+	lea	24(tp), tp		C undo offset FIXME
+	lea	8(up), up		C undo offset FIXME
+
 L(dowhile_mid):
 C Function mpn_addmul_2s_m0(tp, up - (i - 1), i - 1, up - i)
-	lea	2(i), j
-	neg	j
+	mov	$1, R32(j)
+	sub	i, j
+
+	lea	-24(tp), tp		C offset FIXME
+	lea	-8(up), up		C offset FIXME
 
 	mov	(up,j,8), v0
 	mov	8(up,j,8), v1
@@ -673,59 +699,74 @@
 	mov	w0, 8(tp)
 	mov	w1, 16(tp)
 
-	lea	eval(2*8)(tp), tp	C tp += 2
-L(dowhile_end):
+	lea	24(tp), tp		C undo offset FIXME
+	lea	8(up), up		C undo offset FIXME
 
+	lea	eval(2*8)(tp), tp	C tp += 2
 	add	$-2, R32(i)		C i -= 2
+
+	cmp	$3, R32(i)
 	jne	L(dowhile)
 
+L(last):
+
 C Function mpn_addmul_2s_2
-	mov	-16(up), v0
-	mov	-8(up), v1
-	mov	-8(up), %rax
+	mov	-24(up), v0
+	mov	-16(up), v1
+	mov	-16(up), %rax
 	mul	v0
 	xor	R32(w3), R32(w3)
-	add	%rax, -8(tp)
+	add	%rax, -32(tp)
 	adc	%rdx, w3
 	xor	R32(w0), R32(w0)
 	xor	R32(w1), R32(w1)
-	mov	(up), %rax
+	mov	-8(up), %rax
 	mul	v0
 	add	%rax, w3
-	mov	(up), %rax
+	mov	-8(up), %rax
 	adc	%rdx, w0
 	mul	v1
-	add	w3, (tp)
+	add	w3, -24(tp)
 	adc	%rax, w0
 	adc	%rdx, w1
-	mov	w0, 8(tp)
-	mov	w1, 16(tp)
+	mov	w0, -16(tp)
+	mov	w1, -8(tp)
 
 C Function mpn_sqr_diag_addlsh1
-	lea	-4(n,n), j
+	mov	R32(n), R32(j)
+	shl	$3, n
+	sub	n, up
 
 	mov	(%rsp), %r11
 
-	lea	(rp,j,8), rp
-	lea	-8(up), up
+	bt	$0, j
+	lea	-4(j,j),j
+	jc	L(odd)
+
+L(evn):	lea	(rp,j,8), rp
+	lea	(up,j,4), up
 	lea	8(%rsp,j,8), tp
 	neg	j
-	mov	(up,j,4), %rax
-	mul	%rax
-	test	$2, R8(j)
-	jnz	L(odd)
 
-L(evn):	add	%r11, %r11
+	add	%r11, %r11
 	sbb	R32(%rbx), R32(%rbx)		C save CF
+	mov	(up,j,4), %rax
+	mul	%rax
 	add	%rdx, %r11
 	mov	%rax, (rp,j,8)
 	jmp	L(d0)
 
-L(odd):	add	%r11, %r11
+L(odd):	lea	-16(rp,j,8), rp
+	lea	-8(up,j,4), up
+	lea	-8(%rsp,j,8), tp
+	neg	j
+
+	add	%r11, %r11
 	sbb	R32(%rbp), R32(%rbp)		C save CF
+	mov	8(up,j,4), %rax
+	mul	%rax
 	add	%rdx, %r11
-	mov	%rax, (rp,j,8)
-	lea	-2(j), j
+	mov	%rax, 16(rp,j,8)
 	jmp	L(d1)
 
 	ALIGN(16)
@@ -757,24 +798,24 @@
 	add	$4, j
 	js	L(top)
 
-	mov	(up), %rax
+L(end):	mov	(up,j,4), %rax
 	mul	%rax
 	add	R32(%rbp), R32(%rbp)		C restore carry
 	adc	%rax, %r10
 	adc	%rdx, %r11
-	mov	%r10, (rp)
-	mov	%r11, 8(rp)
-	mov	(tp), %r10
+	mov	%r10, (rp,j,8)
+	mov	%r11, 8(rp,j,8)
+	mov	(tp,j,8), %r10
 	adc	%r10, %r10
 	sbb	R32(%rbp), R32(%rbp)		C save CF
 	neg	R32(%rbp)
-	mov	8(up), %rax
+	mov	8(up,j,4), %rax
 	mul	%rax
 	add	R32(%rbx), R32(%rbx)		C restore carry
 	adc	%rax, %r10
 	adc	%rbp, %rdx
-	mov	%r10, 16(rp)
-	mov	%rdx, 24(rp)
+	mov	%r10, 16(rp,j,8)
+	mov	%rdx, 24(rp,j,8)
 
 	add	$eval(8+STACK_ALLOC), %rsp
 	pop	%r14
--- 1/mpn/x86_64/sublsh1_n.asm
+++ 2/mpn/x86_64/sublsh1_n.asm
@@ -24,9 +24,7 @@
 C K8,K9:	 2.2
 C K10:		 2.2
 C P4:		12.75
-C P6 core2: 	 3.45
-C P6 corei7:	 3.45
-C P6 atom:	 ?
+C P6-15:	 3.45
 
 
 C Sometimes speed degenerates, supposedly related to that some operand
@@ -104,7 +102,7 @@
 L(ent):	jns	L(end)
 
 	ALIGN(16)
-L(top):	add	R32(%rax), R32(%rax)	C restore scy
+L(oop):	add	R32(%rax), R32(%rax)	C restore scy
 
 	mov	(vp,n,8), %r8
 L(b00):	adc	%r8, %r8
@@ -133,7 +131,7 @@
 
 	sbb	R32(%rbp), R32(%rbp)	C save acy
 	add	$4, n
-	js	L(top)
+	js	L(oop)
 
 L(end):	add	R32(%rbp), R32(%rax)
 	neg	R32(%rax)
--- 1/mpn/x86_64/x86_64-defs.m4
+++ 2/mpn/x86_64/x86_64-defs.m4
@@ -2,8 +2,8 @@
 
 dnl  m4 macros for amd64 assembler.
 
-dnl  Copyright 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2008, 2009 Free
-dnl  Software Foundation, Inc.
+dnl  Copyright 1999, 2000, 2001, 2002, 2003, 2004, 2005 Free Software
+dnl  Foundation, Inc.
 dnl
 dnl  This file is part of the GNU MP Library.
 dnl
@@ -21,32 +21,11 @@
 dnl  along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.
 
 
-dnl  Usage: CPUVEC_FUNCS_LIST
+dnl  Notes:
 dnl
-dnl  A list of the functions from gmp-impl.h x86 struct cpuvec_t, in the
-dnl  order they appear in that structure.
-
-define(CPUVEC_FUNCS_LIST,
-``add_n',
-`addmul_1',
-`copyd',
-`copyi',
-`divexact_1',
-`divexact_by3c',
-`divrem_1',
-`gcd_1',
-`lshift',
-`mod_1',
-`mod_34lsub1',
-`modexact_1c_odd',
-`mul_1',
-`mul_basecase',
-`preinv_divrem_1',
-`preinv_mod_1',
-`rshift',
-`sqr_basecase',
-`sub_n',
-`submul_1'')
+dnl  The 32-bit mode x86/x86-defs.m4 has various 32bit-isms, like the
+dnl  profiling calls, so it seems cleanest to start a fresh set of defines
+dnl  for 64-bit mode.
 
 
 dnl  Called: PROLOGUE_cpu(GSYM_PREFIX`'foo)
--- 1/mpn/z8000/gmp-mparam.h
+++ 2/mpn/z8000/gmp-mparam.h
@@ -17,5 +17,5 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
-#define GMP_LIMB_BITS 16
+#define BITS_PER_MP_LIMB 16
 #define BYTES_PER_MP_LIMB 2
--- 1/mpq/cmp_ui.c
+++ 2/mpq/cmp_ui.c
@@ -70,8 +70,8 @@
     return -num1_size;
 
   TMP_MARK;
-  tmp1_ptr = TMP_ALLOC_LIMBS (num1_size + 1);
-  tmp2_ptr = TMP_ALLOC_LIMBS (den1_size + 1);
+  tmp1_ptr = (mp_ptr) TMP_ALLOC ((num1_size + 1) * BYTES_PER_MP_LIMB);
+  tmp2_ptr = (mp_ptr) TMP_ALLOC ((den1_size + 1) * BYTES_PER_MP_LIMB);
 
   cy_limb = mpn_mul_1 (tmp1_ptr, op1->_mp_num._mp_d, num1_size,
                        (mp_limb_t) den2);
--- 1/mpq/get_d.c
+++ 2/mpq/get_d.c
@@ -39,7 +39,7 @@
 
    Enhancements:
 
-   Use the true mantissa size in the N_QLIMBS formula, to save a divide step
+   Use the true mantissa size in the N_QLIMBS formala, to save a divide step
    in nails.
 
    Examine the high limbs of num and den to see if the highest 1 bit of the
--- 1/mpq/get_str.c
+++ 2/mpq/get_str.c
@@ -37,8 +37,8 @@
          the high limbs of num and den are used.  +2 for rounding up the
          chars per bit of num and den.  +3 for sign, slash and '\0'.  */
       str_alloc = ((size_t) ((ABS (q->_mp_num._mp_size) + q->_mp_den._mp_size)
-                             * GMP_LIMB_BITS
-                             * mp_bases[ABS(base)].chars_per_bit_exactly))
+                             * BITS_PER_MP_LIMB
+                             * __mp_bases[ABS(base)].chars_per_bit_exactly))
                    + 5;
       str = (char *) (*__gmp_allocate_func) (str_alloc);
     }
--- 1/mpq/Makefile.am
+++ 2/mpq/Makefile.am
@@ -22,9 +22,9 @@
 
 noinst_LTLIBRARIES = libmpq.la
 libmpq_la_SOURCES =							\
-  abs.c aors.c canonicalize.c clear.c clears.c				\
+  abs.c aors.c canonicalize.c clear.c					\
   cmp.c cmp_si.c cmp_ui.c div.c equal.c					\
   get_d.c get_den.c get_num.c get_str.c					\
-  init.c inits.c inp_str.c inv.c md_2exp.c mul.c neg.c out_str.c	\
+  init.c inp_str.c inv.c md_2exp.c mul.c neg.c out_str.c		\
   set.c set_den.c set_num.c set_si.c set_str.c set_ui.c set_z.c set_d.c	\
   set_f.c swap.c
--- 1/mpq/set_f.c
+++ 2/mpq/set_f.c
@@ -60,7 +60,7 @@
     }
   else
     {
-      /* radix point is within or to the left of the limbs, use denominator */
+      /* radix point is within or to the left of the limbs, use demominator */
       mp_ptr     num_ptr, den_ptr;
       mp_size_t  den_size;
 
--- 1/mpq/set_si.c
+++ 2/mpq/set_si.c
@@ -1,4 +1,4 @@
-/* mpq_set_si(dest,ulong_num,ulong_den) -- Set DEST to the rational number
+/* mpq_set_si(dest,ulong_num,ulong_den) -- Set DEST to the retional number
    ULONG_NUM/ULONG_DEN.
 
 Copyright 1991, 1994, 1995, 2001, 2003 Free Software Foundation, Inc.
--- 1/mpz/aorsmul_i.c
+++ 2/mpz/aorsmul_i.c
@@ -53,7 +53,7 @@
    a chance of being faster since it involves only one set of carry
    propagations, not two.  Note that doing an addmul_1 with a
    twos-complement negative y doesn't work, because it effectively adds an
-   extra x * 2^GMP_LIMB_BITS.  */
+   extra x * 2^BITS_PER_MP_LIMB.  */
 
 REGPARM_ATTR(1) void
 mpz_aorsmul_1 (mpz_ptr w, mpz_srcptr x, mp_limb_t y, mp_size_t sub)
--- 1/mpz/divexact.c
+++ 2/mpz/divexact.c
@@ -20,11 +20,10 @@
 
 /*  Ken Weber (kweber@mat.ufrgs.br, kweber@mcs.kent.edu)
 
-    Funding for this work has been partially provided by Conselho Nacional de
-    Desenvolvimento Cienti'fico e Tecnolo'gico (CNPq) do Brazil, Grant
-    301314194-2, and was done while I was a visiting researcher in the
-    Instituto de Matema'tica at Universidade Federal do Rio Grande do Sul
-    (UFRGS).
+    Funding for this work has been partially provided by Conselho Nacional
+    de Desenvolvimento Cienti'fico e Tecnolo'gico (CNPq) do Brazil, Grant
+    301314194-2, and was done while I was a visiting reseacher in the Instituto
+    de Matema'tica at Universidade Federal do Rio Grande do Sul (UFRGS).
 
     References:
 	T. Jebelean, An algorithm for exact division, Journal of Symbolic
@@ -104,7 +103,7 @@
     {
       if (quot == den)		/*  QUOT and DEN overlap.  */
 	{
-	  tp = TMP_ALLOC_LIMBS (tsize);
+	  tp = (mp_ptr) TMP_ALLOC (tsize * BYTES_PER_MP_LIMB);
 	  MPN_COPY (tp, dp, tsize);
 	}
       else
@@ -115,7 +114,7 @@
   else
     {
       unsigned int r;
-      tp = TMP_ALLOC_LIMBS (tsize);
+      tp = (mp_ptr) TMP_ALLOC (tsize * BYTES_PER_MP_LIMB);
       count_trailing_zeros (r, dp[0]);
       mpn_rshift (tp, dp, tsize, r);
       if (dsize > tsize)
--- 1/mpz/fib_ui.c
+++ 2/mpz/fib_ui.c
@@ -78,10 +78,10 @@
       /* F[2k+1] = (2F[k]+F[k-1])*(2F[k]-F[k-1]) + 2*(-1)^k  */
       mp_size_t  xsize, ysize;
 
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
       xp[size] = mpn_lshift (xp, xp, size, 1);
       yp[size] = 0;
-      ASSERT_NOCARRY (mpn_add_n_sub_n (xp, yp, xp, yp, size+1));
+      ASSERT_NOCARRY (mpn_addsub_n (xp, yp, xp, yp, size+1));
       xsize = size + (xp[size] != 0);
       ysize = size + (yp[size] != 0);
 #else
--- 1/mpz/gcd.c
+++ 2/mpz/gcd.c
@@ -90,7 +90,7 @@
   usize -= u_zero_limbs;
   count_trailing_zeros (u_zero_bits, *up);
   tp = up;
-  up = TMP_ALLOC_LIMBS (usize);
+  up = (mp_ptr) TMP_ALLOC (usize * BYTES_PER_MP_LIMB);
   if (u_zero_bits != 0)
     {
       mpn_rshift (up, tp, usize, u_zero_bits);
@@ -105,7 +105,7 @@
   vsize -= v_zero_limbs;
   count_trailing_zeros (v_zero_bits, *vp);
   tp = vp;
-  vp = TMP_ALLOC_LIMBS (vsize);
+  vp = (mp_ptr) TMP_ALLOC (vsize * BYTES_PER_MP_LIMB);
   if (v_zero_bits != 0)
     {
       mpn_rshift (vp, tp, vsize, v_zero_bits);
@@ -135,7 +135,7 @@
     ? mpn_gcd (vp, vp, vsize, up, usize)
     : mpn_gcd (vp, up, usize, vp, vsize);
 
-  /*  Here G <-- V << (g_zero_limbs*GMP_LIMB_BITS + g_zero_bits).  */
+  /*  Here G <-- V << (g_zero_limbs*BITS_PER_MP_LIMB + g_zero_bits).  */
   gsize = vsize + g_zero_limbs;
   if (g_zero_bits != 0)
     {
--- 1/mpz/gcdext.c
+++ 2/mpz/gcdext.c
@@ -50,8 +50,8 @@
     {
       usize = asize;
       vsize = bsize;
-      up = TMP_ALLOC_LIMBS (usize + 1);
-      vp = TMP_ALLOC_LIMBS (vsize + 1);
+      up = (mp_ptr) TMP_ALLOC ((usize + 1) * BYTES_PER_MP_LIMB);
+      vp = (mp_ptr) TMP_ALLOC ((vsize + 1) * BYTES_PER_MP_LIMB);
       MPN_COPY (up, ap, usize);
       MPN_COPY (vp, bp, vsize);
       u = a;
@@ -63,8 +63,8 @@
     {
       usize = bsize;
       vsize = asize;
-      up = TMP_ALLOC_LIMBS (usize + 1);
-      vp = TMP_ALLOC_LIMBS (vsize + 1);
+      up = (mp_ptr) TMP_ALLOC ((usize + 1) * BYTES_PER_MP_LIMB);
+      vp = (mp_ptr) TMP_ALLOC ((vsize + 1) * BYTES_PER_MP_LIMB);
       MPN_COPY (up, bp, usize);
       MPN_COPY (vp, ap, vsize);
       u = b;
@@ -73,8 +73,8 @@
       tt = s;
     }
 
-  tmp_gp = TMP_ALLOC_LIMBS (usize + 1);
-  tmp_sp = TMP_ALLOC_LIMBS (usize + 1);
+  tmp_gp = (mp_ptr) TMP_ALLOC ((usize + 1) * BYTES_PER_MP_LIMB);
+  tmp_sp = (mp_ptr) TMP_ALLOC ((usize + 1) * BYTES_PER_MP_LIMB);
 
   if (vsize == 0)
     {
--- 1/mpz/gcd_ui.c
+++ 2/mpz/gcd_ui.c
@@ -1,4 +1,4 @@
-/* mpz_gcd_ui -- Calculate the greatest common divisor of two integers.
+/* mpz_gcd_ui -- Calculate the greatest common divisior of two integers.
 
 Copyright 1994, 1996, 1999, 2000, 2001, 2002, 2003, 2004 Free Software
 Foundation, Inc.
--- 1/mpz/kronsz.c
+++ 2/mpz/kronsz.c
@@ -85,7 +85,7 @@
       /* a odd, b even
 
          Establish shifted b_low with valid bit1 for ASGN and RECIP below.
-         Zero limbs stripped are accounted for, but zero bits on b_low are
+         Zero limbs stripped are acounted for, but zero bits on b_low are
          not because they remain in {b_ptr,b_abs_size} for the
          JACOBI_MOD_OR_MODEXACT_1_ODD. */
 
--- 1/mpz/Makefile.am
+++ 2/mpz/Makefile.am
@@ -27,7 +27,7 @@
   bin_ui.c bin_uiui.c cdiv_q.c \
   cdiv_q_ui.c cdiv_qr.c cdiv_qr_ui.c cdiv_r.c cdiv_r_ui.c cdiv_ui.c \
   cfdiv_q_2exp.c cfdiv_r_2exp.c \
-  clear.c clears.c clrbit.c \
+  clear.c clrbit.c \
   cmp.c cmp_d.c cmp_si.c cmp_ui.c cmpabs.c cmpabs_d.c cmpabs_ui.c \
   com.c combit.c \
   cong.c cong_2exp.c cong_ui.c \
@@ -39,7 +39,7 @@
   fits_uint.c fits_ulong.c fits_ushort.c \
   gcd.c gcd_ui.c gcdext.c get_d.c get_d_2exp.c get_si.c \
   get_str.c get_ui.c getlimbn.c hamdist.c \
-  import.c init.c init2.c inits.c inp_raw.c inp_str.c \
+  import.c init.c init2.c inp_raw.c inp_str.c \
   invert.c ior.c iset.c iset_d.c iset_si.c iset_str.c iset_ui.c \
   jacobi.c kronsz.c kronuz.c kronzs.c kronzu.c \
   lcm.c lcm_ui.c lucnum_ui.c lucnum2_ui.c millerrabin.c \
--- 1/mpz/mul.c
+++ 2/mpz/mul.c
@@ -118,7 +118,7 @@
       if (wp == up)
 	{
 	  /* W and U are identical.  Allocate temporary space for U.  */
-	  up = TMP_ALLOC_LIMBS (usize);
+	  up = (mp_ptr) TMP_ALLOC (usize * BYTES_PER_MP_LIMB);
 	  /* Is V identical too?  Keep it identical with U.  */
 	  if (wp == vp)
 	    vp = up;
@@ -128,7 +128,7 @@
       else if (wp == vp)
 	{
 	  /* W and V are identical.  Allocate temporary space for V.  */
-	  vp = TMP_ALLOC_LIMBS (vsize);
+	  vp = (mp_ptr) TMP_ALLOC (vsize * BYTES_PER_MP_LIMB);
 	  /* Copy to the temporary space.  */
 	  MPN_COPY (vp, wp, vsize);
 	}
--- 1/mpz/nextprime.c
+++ 2/mpz/nextprime.c
@@ -2,7 +2,7 @@
 
 Copyright 1999, 2000, 2001, 2008, 2009 Free Software Foundation, Inc.
 
-Contributed to the GNU project by Niels Möller and Torbjorn Granlund.
+Contributed to the GNU project by Niels Möller and Torbjörn Granlund.
 
 This file is part of the GNU MP Library.
 
--- 1/mpz/n_pow_ui.c
+++ 2/mpz/n_pow_ui.c
@@ -55,7 +55,7 @@
    The initial powering for bsize==1 into blimb or blimb:blimb_low doesn't
    form the biggest possible power of b that fits, only the biggest power of
    2 power, ie. b^(2^n).  It'd be possible to choose a bigger power, perhaps
-   using mp_bases[b].big_base for small b, and thereby get better value
+   using __mp_bases[b].big_base for small b, and thereby get better value
    from mpn_mul_1 or mpn_mul_2 in the bignum powering.  It's felt that doing
    so would be more complicated than it's worth, and could well end up being
    a slowdown for small e.  For big e on the other hand the algorithm is
--- 1/mpz/out_str.c
+++ 2/mpz/out_str.c
@@ -71,13 +71,13 @@
     }
 
   TMP_MARK;
-  str_size = ((size_t) (x_size * GMP_LIMB_BITS
-			* mp_bases[base].chars_per_bit_exactly)) + 3;
+  str_size = ((size_t) (x_size * BITS_PER_MP_LIMB
+			* __mp_bases[base].chars_per_bit_exactly)) + 3;
   str = (unsigned char *) TMP_ALLOC (str_size);
 
   /* Move the number to convert into temporary space, since mpn_get_str
      clobbers its argument + needs one extra high limb....  */
-  xp = TMP_ALLOC_LIMBS (x_size + 1);
+  xp = (mp_ptr) TMP_ALLOC ((x_size + 1) * BYTES_PER_MP_LIMB);
   MPN_COPY (xp, x->_mp_d, x_size);
 
   str_size = mpn_get_str (str, base, xp, x_size);
--- 1/mpz/perfpow.c
+++ 2/mpz/perfpow.c
@@ -1,8 +1,7 @@
 /* mpz_perfect_power_p(arg) -- Return non-zero if ARG is a perfect power,
    zero otherwise.
 
-Copyright 1998, 1999, 2000, 2001, 2005, 2008, 2009 Free Software Foundation,
-Inc.
+Copyright 1998, 1999, 2000, 2001, 2005, 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -19,11 +18,266 @@
 You should have received a copy of the GNU Lesser General Public License
 along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.  */
 
+/*
+  We are to determine if c is a perfect power, c = a ^ b.
+  Assume c is divisible by 2^n and that codd = c/2^n is odd.
+  Assume a is divisible by 2^m and that aodd = a/2^m is odd.
+  It is always true that m divides n.
+
+  * If n is prime, either 1) a is 2*aodd and b = n
+		       or 2) a = c and b = 1.
+    So for n prime, we readily have a solution.
+  * If n is factorable into the non-trivial factors p1,p2,...
+    Since m divides n, m has a subset of n's factors and b = n / m.
+*/
+
+/* This is a naive approach to recognizing perfect powers.
+   Many things can be improved.  In particular, we should use p-adic
+   arithmetic for computing possible roots.  */
+
+#include <stdio.h> /* for NULL */
 #include "gmp.h"
 #include "gmp-impl.h"
+#include "longlong.h"
+
+static unsigned long int gcd __GMP_PROTO ((unsigned long int, unsigned long int));
+static int isprime __GMP_PROTO ((unsigned long int));
+
+static const unsigned short primes[] =
+{  2,  3,  5,  7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53,
+  59, 61, 67, 71, 73, 79, 83, 89, 97,101,103,107,109,113,127,131,
+ 137,139,149,151,157,163,167,173,179,181,191,193,197,199,211,223,
+ 227,229,233,239,241,251,257,263,269,271,277,281,283,293,307,311,
+ 313,317,331,337,347,349,353,359,367,373,379,383,389,397,401,409,
+ 419,421,431,433,439,443,449,457,461,463,467,479,487,491,499,503,
+ 509,521,523,541,547,557,563,569,571,577,587,593,599,601,607,613,
+ 617,619,631,641,643,647,653,659,661,673,677,683,691,701,709,719,
+ 727,733,739,743,751,757,761,769,773,787,797,809,811,821,823,827,
+ 829,839,853,857,859,863,877,881,883,887,907,911,919,929,937,941,
+ 947,953,967,971,977,983,991,997,0
+};
+#define SMALLEST_OMITTED_PRIME 1009
+
+
+#define POW2P(a) (((a) & ((a) - 1)) == 0)
 
 int
 mpz_perfect_power_p (mpz_srcptr u)
 {
-  return mpn_perfect_power_p (PTR (u), SIZ (u));
+  unsigned long int prime;
+  unsigned long int n, n2;
+  int i;
+  unsigned long int rem;
+  mpz_t u2, q;
+  int exact;
+  mp_size_t uns;
+  mp_size_t usize = SIZ (u);
+  TMP_DECL;
+
+  if (mpz_cmpabs_ui (u, 1) <= 0)
+    return 1;			/* -1, 0, and +1 are perfect powers */
+
+  n2 = mpz_scan1 (u, 0);
+  if (n2 == 1)
+    return 0;			/* 2 divides exactly once.  */
+
+  TMP_MARK;
+
+  uns = ABS (usize) - n2 / BITS_PER_MP_LIMB;
+  MPZ_TMP_INIT (q, uns);
+  MPZ_TMP_INIT (u2, uns);
+
+  mpz_tdiv_q_2exp (u2, u, n2);
+  mpz_abs (u2, u2);
+
+  if (mpz_cmp_ui (u2, 1) == 0)
+    {
+      TMP_FREE;
+      /* factoring completed; consistent power */
+      return ! (usize < 0 && POW2P(n2));
+    }
+
+  if (isprime (n2))
+    goto n2prime;
+
+  for (i = 1; primes[i] != 0; i++)
+    {
+      prime = primes[i];
+
+      if (mpz_cmp_ui (u2, prime) < 0)
+	break;
+
+      if (mpz_divisible_ui_p (u2, prime))	/* divisible by this prime? */
+	{
+	  rem = mpz_tdiv_q_ui (q, u2, prime * prime);
+	  if (rem != 0)
+	    {
+	      TMP_FREE;
+	      return 0;		/* prime divides exactly once, reject */
+	    }
+	  mpz_swap (q, u2);
+	  for (n = 2;;)
+	    {
+	      rem = mpz_tdiv_q_ui (q, u2, prime);
+	      if (rem != 0)
+		break;
+	      mpz_swap (q, u2);
+	      n++;
+	    }
+
+	  n2 = gcd (n2, n);
+	  if (n2 == 1)
+	    {
+	      TMP_FREE;
+	      return 0;		/* we have multiplicity 1 of some factor */
+	    }
+
+	  if (mpz_cmp_ui (u2, 1) == 0)
+	    {
+	      TMP_FREE;
+	      /* factoring completed; consistent power */
+	      return ! (usize < 0 && POW2P(n2));
+	    }
+
+	  /* As soon as n2 becomes a prime number, stop factoring.
+	     Either we have u=x^n2 or u is not a perfect power.  */
+	  if (isprime (n2))
+	    goto n2prime;
+	}
+    }
+
+  if (n2 == 0)
+    {
+      /* We found no factors above; have to check all values of n.  */
+      unsigned long int nth;
+      for (nth = usize < 0 ? 3 : 2;; nth++)
+	{
+	  if (! isprime (nth))
+	    continue;
+#if 0
+	  exact = mpz_padic_root (q, u2, nth, PTH);
+	  if (exact)
+#endif
+	    exact = mpz_root (q, u2, nth);
+	  if (exact)
+	    {
+	      TMP_FREE;
+	      return 1;
+	    }
+	  if (mpz_cmp_ui (q, SMALLEST_OMITTED_PRIME) < 0)
+	    {
+	      TMP_FREE;
+	      return 0;
+	    }
+	}
+    }
+  else
+    {
+      unsigned long int nth;
+
+      if (usize < 0 && POW2P(n2))
+	{
+	  TMP_FREE;
+	  return 0;
+	}
+
+      /* We found some factors above.  We just need to consider values of n
+	 that divides n2.  */
+      for (nth = 2; nth <= n2; nth++)
+	{
+	  if (! isprime (nth))
+	    continue;
+	  if (n2 % nth != 0)
+	    continue;
+#if 0
+	  exact = mpz_padic_root (q, u2, nth, PTH);
+	  if (exact)
+#endif
+	    exact = mpz_root (q, u2, nth);
+	  if (exact)
+	    {
+	      if (! (usize < 0 && POW2P(nth)))
+		{
+		  TMP_FREE;
+		  return 1;
+		}
+	    }
+	  if (mpz_cmp_ui (q, SMALLEST_OMITTED_PRIME) < 0)
+	    {
+	      TMP_FREE;
+	      return 0;
+	    }
+	}
+
+      TMP_FREE;
+      return 0;
+    }
+
+n2prime:
+  if (usize < 0 && POW2P(n2))
+    {
+      TMP_FREE;
+      return 0;
+    }
+
+  exact = mpz_root (NULL, u2, n2);
+  TMP_FREE;
+  return exact;
+}
+
+static unsigned long int
+gcd (unsigned long int a, unsigned long int b)
+{
+  int an2, bn2, n2;
+
+  if (a == 0)
+    return b;
+  if (b == 0)
+    return a;
+
+  count_trailing_zeros (an2, a);
+  a >>= an2;
+
+  count_trailing_zeros (bn2, b);
+  b >>= bn2;
+
+  n2 = MIN (an2, bn2);
+
+  while (a != b)
+    {
+      if (a > b)
+	{
+	  a -= b;
+	  do
+	    a >>= 1;
+	  while ((a & 1) == 0);
+	}
+      else /*  b > a.  */
+	{
+	  b -= a;
+	  do
+	    b >>= 1;
+	  while ((b & 1) == 0);
+	}
+    }
+
+  return a << n2;
+}
+
+static int
+isprime (unsigned long int t)
+{
+  unsigned long int q, r, d;
+
+  if (t < 3 || (t & 1) == 0)
+    return t == 2;
+
+  for (d = 3, r = 1; r != 0; d += 2)
+    {
+      q = t / d;
+      r = t - q * d;
+      if (q < d)
+	return 1;
+    }
+  return 0;
 }
--- 1/mpz/powm.c
+++ 2/mpz/powm.c
@@ -1,8 +1,8 @@
-/* mpz_powm(res,base,exp,mod) -- Set R to (U^E) mod M.
+/* mpz_powm(res,base,exp,mod) -- Set RES to (base**exp) mod MOD.
 
-   Contributed to the GNU project by Torbjorn Granlund.
+   Contributed by Paul Zimmermann.
 
-Copyright 1991, 1993, 1994, 1996, 1997, 2000, 2001, 2002, 2005, 2008, 2009
+Copyright 1991, 1993, 1994, 1996, 1997, 2000, 2001, 2002, 2005, 2009
 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
@@ -28,28 +28,95 @@
 #include "mp.h"
 #endif
 
+/* Compute t = a mod m, a is defined by (ap,an), m is defined by (mp,mn), and
+   t is defined by (tp,mn).  */
+static void
+reduce (mp_ptr tp, mp_srcptr ap, mp_size_t an, mp_srcptr mp, mp_size_t mn)
+{
+  mp_ptr qp;
+  TMP_DECL;
 
-/* TODO
+  TMP_MARK;
+  qp = TMP_ALLOC_LIMBS (an - mn + 1);
 
- * Improve handling of buffers.  It is pretty ugly now.
+  mpn_tdiv_qr (qp, tp, 0L, ap, an, mp, mn);
 
- * For even moduli, we compute a binvert of its odd part both here and in
-   mpn_powm.  How can we avoid this recomputation?
-*/
+  TMP_FREE;
+}
+
+#if REDUCE_EXPONENT
+/* Return the group order of the ring mod m.  */
+static mp_limb_t
+phi (mp_limb_t t)
+{
+  mp_limb_t d, m, go;
+
+  go = 1;
 
-/*
-  b ^ e mod m   res
-  0   0     0    ?
-  0   e     0    ?
-  0   0     m    ?
-  0   e     m    0
-  b   0     0    ?
-  b   e     0    ?
-  b   0     m    1 mod m
-  b   e     m    b^e mod m
+  if (t % 2 == 0)
+    {
+      t = t / 2;
+      while (t % 2 == 0)
+	{
+	  go *= 2;
+	  t = t / 2;
+	}
+    }
+  for (d = 3;; d += 2)
+    {
+      m = d - 1;
+      for (;;)
+	{
+	  unsigned long int q = t / d;
+	  if (q < d)
+	    {
+	      if (t <= 1)
+		return go;
+	      if (t == d)
+		return go * m;
+	      return go * (t - 1);
+	    }
+	  if (t != q * d)
+	    break;
+	  go *= m;
+	  m = d;
+	  t = q;
+	}
+    }
+}
+#endif
+
+/* average number of calls to redc for an exponent of n bits
+   with the sliding window algorithm of base 2^k: the optimal is
+   obtained for the value of k which minimizes 2^(k-1)+n/(k+1):
+
+   n\k    4     5     6     7     8
+   128    156*  159   171   200   261
+   256    309   307*  316   343   403
+   512    617   607*  610   632   688
+   1024   1231  1204  1195* 1207  1256
+   2048   2461  2399  2366  2360* 2396
+   4096   4918  4787  4707  4665* 4670
 */
+
+
+/* Use REDC instead of usual reduction for sizes < POWM_THRESHOLD.  In REDC
+   each modular multiplication costs about 2*n^2 limbs operations, whereas
+   using usual reduction it costs 3*K(n), where K(n) is the cost of a
+   multiplication using Karatsuba, and a division is assumed to cost 2*K(n),
+   for example using Burnikel-Ziegler's algorithm. This gives a theoretical
+   threshold of a*SQR_KARATSUBA_THRESHOLD, with a=(3/2)^(1/(2-ln(3)/ln(2))) ~
+   2.66.  */
+/* For now, also disable REDC when MOD is even, as the inverse can't handle
+   that.  At some point, we might want to make the code faster for that case,
+   perhaps using CRR.  */
+
+#ifndef POWM_THRESHOLD
+#define POWM_THRESHOLD  ((8 * SQR_KARATSUBA_THRESHOLD) / 3)
+#endif
 
 #define HANDLE_NEGATIVE_EXPONENT 1
+#undef REDUCE_EXPONENT
 
 void
 #ifndef BERKELEY_MP
@@ -58,36 +125,44 @@
 pow (mpz_srcptr b, mpz_srcptr e, mpz_srcptr m, mpz_ptr r)
 #endif /* BERKELEY_MP */
 {
-  mp_size_t n, nodd, ncnt;
-  int cnt;
-  mp_ptr rp, tp;
+  mp_ptr xp, tp, qp, gp, this_gp;
   mp_srcptr bp, ep, mp;
-  mp_size_t rn, bn, es, en, itch;
+  mp_size_t bn, es, en, mn, xn;
+  mp_limb_t invm, c;
+  unsigned long int enb;
+  mp_size_t i, K, j, l, k;
+  int m_zero_cnt, e_zero_cnt;
+  int sh;
+  int use_redc;
+#if HANDLE_NEGATIVE_EXPONENT
+  mpz_t new_b;
+#endif
+#if REDUCE_EXPONENT
+  mpz_t new_e;
+#endif
   TMP_DECL;
 
-  n = ABSIZ(m);
-  if (n == 0)
-    DIVIDE_BY_ZERO;
-
   mp = PTR(m);
+  mn = ABSIZ (m);
+  if (mn == 0)
+    DIVIDE_BY_ZERO;
 
   TMP_MARK;
 
-  es = SIZ(e);
-  if (UNLIKELY (es <= 0))
+  es = SIZ (e);
+  if (es <= 0)
     {
-      mpz_t new_b;
       if (es == 0)
 	{
-	  /* b^0 mod m,  b is anything and m is non-zero.
-	     Result is 1 mod m, i.e., 1 or 0 depending on if m = 1.  */
-	  SIZ(r) = n != 1 || mp[0] != 1;
+	  /* Exponent is zero, result is 1 mod m, i.e., 1 or 0 depending on if
+	     m equals 1.  */
+	  SIZ(r) = (mn == 1 && mp[0] == 1) ? 0 : 1;
 	  PTR(r)[0] = 1;
 	  TMP_FREE;	/* we haven't really allocated anything here */
 	  return;
 	}
 #if HANDLE_NEGATIVE_EXPONENT
-      MPZ_TMP_INIT (new_b, n + 1);
+      MPZ_TMP_INIT (new_b, mn + 1);
 
       if (! mpz_invert (new_b, b, m))
 	DIVIDE_BY_ZERO;
@@ -99,179 +174,262 @@
     }
   en = es;
 
-  bn = ABSIZ(b);
-
-  if (UNLIKELY (bn == 0))
+#if REDUCE_EXPONENT
+  /* Reduce exponent by dividing it by phi(m) when m small.  */
+  if (mn == 1 && mp[0] < 0x7fffffffL && en * GMP_NUMB_BITS > 150)
     {
-      SIZ(r) = 0;
-      TMP_FREE;
-      return;
+      MPZ_TMP_INIT (new_e, 2);
+      mpz_mod_ui (new_e, e, phi (mp[0]));
+      e = new_e;
     }
+#endif
 
-  ep = PTR(e);
-
-  /* Handle (b^1 mod m) early, since mpn_pow* do not handle that case.  */
-  if (UNLIKELY (en == 1 && ep[0] == 1))
+  use_redc = mn < POWM_THRESHOLD && mp[0] % 2 != 0;
+  if (use_redc)
     {
-      rp = TMP_ALLOC_LIMBS (n);
-      bp = PTR(b);
-      if (bn >= n)
-	{
-	  mp_ptr qp = TMP_ALLOC_LIMBS (bn - n + 1);
-	  mpn_tdiv_qr (qp, rp, 0L, bp, bn, mp, n);
-	  rn = n;
-	  MPN_NORMALIZE (rp, rn);
-
-	  if (SIZ(b) < 0 && rn != 0)
-	    {
-	      mpn_sub (rp, mp, n, rp, rn);
-	      rn = n;
-	      MPN_NORMALIZE (rp, rn);
-	    }
-	}
-      else
+      /* invm = -1/m mod 2^BITS_PER_MP_LIMB, must have m odd */
+      modlimb_invert (invm, mp[0]);
+      invm = -invm;
+    }
+  else
+    {
+      /* Normalize m (i.e. make its most significant bit set) as required by
+	 division functions below.  */
+      count_leading_zeros (m_zero_cnt, mp[mn - 1]);
+      m_zero_cnt -= GMP_NAIL_BITS;
+      if (m_zero_cnt != 0)
 	{
-	  if (SIZ(b) < 0)
-	    {
-	      mpn_sub (rp, mp, n, bp, bn);
-	      rn = n;
-	      rn -= (rp[rn - 1] == 0);
-	    }
-	  else
-	    {
-	      MPN_COPY (rp, bp, bn);
-	      rn = bn;
-	    }
+	  mp_ptr new_mp;
+	  new_mp = TMP_ALLOC_LIMBS (mn);
+	  mpn_lshift (new_mp, mp, mn, m_zero_cnt);
+	  mp = new_mp;
 	}
-      goto ret;
     }
 
-  /* Remove low zero limbs from M.  This loop will terminate for correctly
-     represented mpz numbers.  */
-  ncnt = 0;
-  while (UNLIKELY (mp[0] == 0))
+  /* Determine optimal value of k, the number of exponent bits we look at
+     at a time.  */
+  count_leading_zeros (e_zero_cnt, PTR(e)[en - 1]);
+  e_zero_cnt -= GMP_NAIL_BITS;
+  enb = en * GMP_NUMB_BITS - e_zero_cnt; /* number of bits of exponent */
+  k = 1;
+  K = 2;
+  while (2 * enb > K * (2 + k * (3 + k)))
     {
-      mp++;
-      ncnt++;
-    }
-  nodd = n - ncnt;
-  cnt = 0;
-  if (mp[0] % 2 == 0)
-    {
-      mp_ptr new = TMP_ALLOC_LIMBS (nodd);
-      count_trailing_zeros (cnt, mp[0]);
-      mpn_rshift (new, mp, nodd, cnt);
-      nodd -= new[nodd - 1] == 0;
-      mp = new;
-      ncnt++;
+      k++;
+      K *= 2;
+      if (k == 10)			/* cap allocation */
+	break;
     }
 
-  if (ncnt != 0)
+  tp = TMP_ALLOC_LIMBS (2 * mn);
+  qp = TMP_ALLOC_LIMBS (mn + 1);
+
+  gp = __GMP_ALLOCATE_FUNC_LIMBS (K / 2 * mn);
+
+  /* Compute x*R^n where R=2^BITS_PER_MP_LIMB.  */
+  bn = ABSIZ (b);
+  bp = PTR(b);
+  /* Handle |b| >= m by computing b mod m.  FIXME: It is not strictly necessary
+     for speed or correctness to do this when b and m have the same number of
+     limbs, perhaps remove mpn_cmp call.  */
+  if (bn > mn || (bn == mn && mpn_cmp (bp, mp, mn) >= 0))
     {
-      /* rp needs n, mpn_powlo needs 4n, the 2 mpn_binvert might need more */
-      mp_size_t n_largest_binvert = MAX (ncnt, nodd);
-      mp_size_t itch_binvert = mpn_binvert_itch (n_largest_binvert);
-      itch = 3 * n + MAX (itch_binvert, 2 * n);
+      /* Reduce possibly huge base while moving it to gp[0].  Use a function
+	 call to reduce, since we don't want the quotient allocation to
+	 live until function return.  */
+      if (use_redc)
+	{
+	  reduce (tp + mn, bp, bn, mp, mn);	/* b mod m */
+	  MPN_ZERO (tp, mn);
+	  mpn_tdiv_qr (qp, gp, 0L, tp, 2 * mn, mp, mn); /* unnormnalized! */
+	}
+      else
+	{
+	  reduce (gp, bp, bn, mp, mn);
+	}
     }
   else
     {
-      mp_size_t itch_binvert = mpn_binvert_itch (nodd);
-      itch = n + MAX (itch_binvert, 2 * n);
+      /* |b| < m.  We pad out operands to become mn limbs,  which simplifies
+	 the rest of the function, but slows things down when |b| << m.  */
+      if (use_redc)
+	{
+	  MPN_ZERO (tp, mn);
+	  MPN_COPY (tp + mn, bp, bn);
+	  MPN_ZERO (tp + mn + bn, mn - bn);
+	  mpn_tdiv_qr (qp, gp, 0L, tp, 2 * mn, mp, mn);
+	}
+      else
+	{
+	  MPN_COPY (gp, bp, bn);
+	  MPN_ZERO (gp + bn, mn - bn);
+	}
     }
-  tp = TMP_ALLOC_LIMBS (itch);
 
-  rp = tp;  tp += n;
-
-  bp = PTR(b);
-  mpn_powm (rp, bp, bn, ep, en, mp, nodd, tp);
+  /* Compute xx^i for odd g < 2^i.  */
 
-  rn = n;
-
-  if (ncnt != 0)
+  xp = TMP_ALLOC_LIMBS (mn);
+  mpn_sqr_n (tp, gp, mn);
+  if (use_redc)
+    mpn_redc_1 (xp, tp, mp, mn, invm);		/* xx = x^2*R^n */
+  else
+    mpn_tdiv_qr (qp, xp, 0L, tp, 2 * mn, mp, mn);
+  this_gp = gp;
+  for (i = 1; i < K / 2; i++)
     {
-      mp_ptr r2, xp, yp, odd_inv_2exp;
-      unsigned long t;
-      int bcnt;
+      mpn_mul_n (tp, this_gp, xp, mn);
+      this_gp += mn;
+      if (use_redc)
+	mpn_redc_1 (this_gp, tp, mp, mn, invm);	/* g[i] = x^(2i+1)*R^n */
+      else
+	mpn_tdiv_qr (qp, this_gp, 0L, tp, 2 * mn, mp, mn);
+    }
 
-      if (bn < ncnt)
+  /* Start the real stuff.  */
+  ep = PTR (e);
+  i = en - 1;				/* current index */
+  c = ep[i];				/* current limb */
+  sh = GMP_NUMB_BITS - e_zero_cnt;	/* significant bits in ep[i] */
+  sh -= k;				/* index of lower bit of ep[i] to take into account */
+  if (sh < 0)
+    {					/* k-sh extra bits are needed */
+      if (i > 0)
 	{
-	  mp_ptr new = TMP_ALLOC_LIMBS (ncnt);
-	  MPN_COPY (new, bp, bn);
-	  MPN_ZERO (new + bn, ncnt - bn);
-	  bp = new;
+	  i--;
+	  c <<= (-sh);
+	  sh += GMP_NUMB_BITS;
+	  c |= ep[i] >> sh;
 	}
+    }
+  else
+    c >>= sh;
+
+  for (j = 0; c % 2 == 0; j++)
+    c >>= 1;
 
-      r2 = tp;
+  MPN_COPY (xp, gp + mn * (c >> 1), mn);
+  while (--j >= 0)
+    {
+      mpn_sqr_n (tp, xp, mn);
+      if (use_redc)
+	mpn_redc_1 (xp, tp, mp, mn, invm);
+      else
+	mpn_tdiv_qr (qp, xp, 0L, tp, 2 * mn, mp, mn);
+    }
 
-      if (bp[0] % 2 == 0)
+  while (i > 0 || sh > 0)
+    {
+      c = ep[i];
+      l = k;				/* number of bits treated */
+      sh -= l;
+      if (sh < 0)
 	{
-	  if (en > 1)
+	  if (i > 0)
 	    {
-	      MPN_ZERO (r2, ncnt);
-	      goto zero;
+	      i--;
+	      c <<= (-sh);
+	      sh += GMP_NUMB_BITS;
+	      c |= ep[i] >> sh;
 	    }
-
-	  ASSERT (en == 1);
-	  t = (ncnt - (cnt != 0)) * GMP_NUMB_BITS + cnt;
-
-	  /* Count number of low zero bits in B, up to 3.  */
-	  bcnt = (0x1213 >> ((bp[0] & 7) << 1)) & 0x3;
-	  /* Note that ep[0] * bcnt might overflow, but that just results
-	     in a missed optimization.  */
-	  if (ep[0] * bcnt >= t)
+	  else
 	    {
-	      MPN_ZERO (r2, ncnt);
-	      goto zero;
+	      l += sh;			/* last chunk of bits from e; l < k */
 	    }
 	}
+      else
+	c >>= sh;
+      c &= ((mp_limb_t) 1 << l) - 1;
 
-      mpn_powlo (r2, bp, ep, en, ncnt, tp + ncnt);
-
-    zero:
-      if (nodd < ncnt)
+      /* This while loop implements the sliding window improvement--loop while
+	 the most significant bit of c is zero, squaring xx as we go. */
+      while ((c >> (l - 1)) == 0 && (i > 0 || sh > 0))
 	{
-	  mp_ptr new = TMP_ALLOC_LIMBS (ncnt);
-	  MPN_COPY (new, mp, nodd);
-	  MPN_ZERO (new + nodd, ncnt - nodd);
-	  mp = new;
+	  mpn_sqr_n (tp, xp, mn);
+	  if (use_redc)
+	    mpn_redc_1 (xp, tp, mp, mn, invm);
+	  else
+	    mpn_tdiv_qr (qp, xp, 0L, tp, 2 * mn, mp, mn);
+	  if (sh != 0)
+	    {
+	      sh--;
+	      c = (c << 1) + ((ep[i] >> sh) & 1);
+	    }
+	  else
+	    {
+	      i--;
+	      sh = GMP_NUMB_BITS - 1;
+	      c = (c << 1) + (ep[i] >> sh);
+	    }
 	}
 
-      odd_inv_2exp = tp + n;
-      mpn_binvert (odd_inv_2exp, mp, ncnt, tp + 2 * n);
-
-      mpn_sub (r2, r2, ncnt, rp, nodd > ncnt ? ncnt : nodd);
-
-      xp = tp + 2 * n;
-      mpn_mullo_n (xp, odd_inv_2exp, r2, ncnt);
-
-      if (cnt != 0)
-	xp[ncnt - 1] &= (CNST_LIMB(1) << cnt) - 1;
+      /* Replace xx by xx^(2^l)*x^c.  */
+      if (c != 0)
+	{
+	  for (j = 0; c % 2 == 0; j++)
+	    c >>= 1;
 
-      yp = tp;
-      if (ncnt > nodd)
-	mpn_mul (yp, xp, ncnt, mp, nodd);
+	  /* c0 = c * 2^j, i.e. xx^(2^l)*x^c = (A^(2^(l - j))*c)^(2^j) */
+	  l -= j;
+	  while (--l >= 0)
+	    {
+	      mpn_sqr_n (tp, xp, mn);
+	      if (use_redc)
+		mpn_redc_1 (xp, tp, mp, mn, invm);
+	      else
+		mpn_tdiv_qr (qp, xp, 0L, tp, 2 * mn, mp, mn);
+	    }
+	  mpn_mul_n (tp, xp, gp + mn * (c >> 1), mn);
+	  if (use_redc)
+	    mpn_redc_1 (xp, tp, mp, mn, invm);
+	  else
+	    mpn_tdiv_qr (qp, xp, 0L, tp, 2 * mn, mp, mn);
+	}
       else
-	mpn_mul (yp, mp, nodd, xp, ncnt);
-
-      mpn_add (rp, yp, n, rp, nodd);
-
-      ASSERT (nodd + ncnt >= n);
-      ASSERT (nodd + ncnt <= n + 1);
+	j = l;				/* case c=0 */
+      while (--j >= 0)
+	{
+	  mpn_sqr_n (tp, xp, mn);
+	  if (use_redc)
+	    mpn_redc_1 (xp, tp, mp, mn, invm);
+	  else
+	    mpn_tdiv_qr (qp, xp, 0L, tp, 2 * mn, mp, mn);
+	}
     }
 
-  MPN_NORMALIZE (rp, rn);
-
-  if ((ep[0] & 1) && SIZ(b) < 0 && rn != 0)
+  if (use_redc)
     {
-      mpn_sub (rp, PTR(m), n, rp, rn);
-      rn = n;
-      MPN_NORMALIZE (rp, rn);
+      /* Convert back xx to xx/R^n.  */
+      MPN_COPY (tp, xp, mn);
+      MPN_ZERO (tp + mn, mn);
+      mpn_redc_1 (xp, tp, mp, mn, invm);
+      if (mpn_cmp (xp, mp, mn) >= 0)
+	mpn_sub_n (xp, xp, mp, mn);
     }
+  else
+    {
+      if (m_zero_cnt != 0)
+	{
+	  mp_limb_t cy;
+	  cy = mpn_lshift (tp, xp, mn, m_zero_cnt);
+	  tp[mn] = cy;
+	  mpn_tdiv_qr (qp, xp, 0L, tp, mn + (cy != 0), mp, mn);
+	  mpn_rshift (xp, xp, mn, m_zero_cnt);
+	}
+    }
+  xn = mn;
+  MPN_NORMALIZE (xp, xn);
 
- ret:
-  MPZ_REALLOC (r, rn);
-  SIZ(r) = rn;
-  MPN_COPY (PTR(r), rp, rn);
+  if ((ep[0] & 1) && SIZ(b) < 0 && xn != 0)
+    {
+      mp = PTR(m);			/* want original, unnormalized m */
+      mpn_sub (xp, mp, mn, xp, xn);
+      xn = mn;
+      MPN_NORMALIZE (xp, xn);
+    }
+  MPZ_REALLOC (r, xn);
+  SIZ (r) = xn;
+  MPN_COPY (PTR(r), xp, xn);
 
+  __GMP_FREE_FUNC_LIMBS (gp, K / 2 * mn);
   TMP_FREE;
 }
--- 1/mpz/powm_ui.c
+++ 2/mpz/powm_ui.c
@@ -109,7 +109,7 @@
   e = el;
   count_leading_zeros (c, e);
   e = (e << c) << 1;		/* shift the exp bits to the left, lose msb */
-  c = GMP_LIMB_BITS - 1 - c;
+  c = BITS_PER_MP_LIMB - 1 - c;
 
   /* Main loop. */
 
--- 1/mpz/pprime_p.c
+++ 2/mpz/pprime_p.c
@@ -70,19 +70,19 @@
   r = mpn_mod_1 (PTR(n), (mp_size_t) SIZ(n), (mp_limb_t) PP);
 #endif
   if (r % 3 == 0
-#if GMP_LIMB_BITS >= 4
+#if BITS_PER_MP_LIMB >= 4
       || r % 5 == 0
 #endif
-#if GMP_LIMB_BITS >= 8
+#if BITS_PER_MP_LIMB >= 8
       || r % 7 == 0
 #endif
-#if GMP_LIMB_BITS >= 16
+#if BITS_PER_MP_LIMB >= 16
       || r % 11 == 0 || r % 13 == 0
 #endif
-#if GMP_LIMB_BITS >= 32
+#if BITS_PER_MP_LIMB >= 32
       || r % 17 == 0 || r % 19 == 0 || r % 23 == 0 || r % 29 == 0
 #endif
-#if GMP_LIMB_BITS >= 64
+#if BITS_PER_MP_LIMB >= 64
       || r % 31 == 0 || r % 37 == 0 || r % 41 == 0 || r % 43 == 0
       || r % 47 == 0 || r % 53 == 0
 #endif
--- 1/mpz/sqrt.c
+++ 2/mpz/sqrt.c
@@ -67,7 +67,7 @@
       if (root_ptr == op_ptr)
 	{
 	  /* ROOT and OP are identical.  Allocate temporary space for OP.  */
-	  op_ptr = TMP_ALLOC_LIMBS (op_size);
+	  op_ptr = (mp_ptr) TMP_ALLOC (op_size * BYTES_PER_MP_LIMB);
 	  /* Copy to the temporary space.  Hack: Avoid temporary variable
 	     by using ROOT_PTR.  */
 	  MPN_COPY (op_ptr, root_ptr, op_size);
--- 1/mpz/sqrtrem.c
+++ 2/mpz/sqrtrem.c
@@ -79,7 +79,7 @@
       if (root_ptr == op_ptr)
 	{
 	  /* ROOT and OP are identical.  Allocate temporary space for OP.  */
-	  op_ptr = TMP_ALLOC_LIMBS (op_size);
+	  op_ptr = (mp_ptr) TMP_ALLOC (op_size * BYTES_PER_MP_LIMB);
 	  /* Copy to the temporary space.  Hack: Avoid temporary variable
 	     by using ROOT_PTR.  */
 	  MPN_COPY (op_ptr, root_ptr, op_size);
--- 1/mpz/tdiv_q.c
+++ 2/mpz/tdiv_q.c
@@ -49,7 +49,7 @@
 
   TMP_MARK;
   qp = PTR (quot);
-  rp = TMP_ALLOC_LIMBS (dl);
+  rp = (mp_ptr) TMP_ALLOC (dl * BYTES_PER_MP_LIMB);
   np = PTR (num);
   dp = PTR (den);
 
@@ -61,7 +61,7 @@
   if (dp == qp)
     {
       mp_ptr tp;
-      tp = TMP_ALLOC_LIMBS (dl);
+      tp = (mp_ptr) TMP_ALLOC (dl * BYTES_PER_MP_LIMB);
       MPN_COPY (tp, dp, dl);
       dp = tp;
     }
@@ -69,7 +69,7 @@
   if (np == qp)
     {
       mp_ptr tp;
-      tp = TMP_ALLOC_LIMBS (nl);
+      tp = (mp_ptr) TMP_ALLOC (nl * BYTES_PER_MP_LIMB);
       MPN_COPY (tp, np, nl);
       np = tp;
     }
--- 1/mpz/tdiv_qr.c
+++ 2/mpz/tdiv_qr.c
@@ -81,7 +81,7 @@
   if (dp == rp || dp == qp)
     {
       mp_ptr tp;
-      tp = TMP_ALLOC_LIMBS (dl);
+      tp = (mp_ptr) TMP_ALLOC (dl * BYTES_PER_MP_LIMB);
       MPN_COPY (tp, dp, dl);
       dp = tp;
     }
@@ -90,7 +90,7 @@
   if (np == rp || np == qp)
     {
       mp_ptr tp;
-      tp = TMP_ALLOC_LIMBS (nl);
+      tp = (mp_ptr) TMP_ALLOC (nl * BYTES_PER_MP_LIMB);
       MPN_COPY (tp, np, nl);
       np = tp;
     }
--- 1/mpz/tdiv_r.c
+++ 2/mpz/tdiv_r.c
@@ -54,7 +54,7 @@
     }
 
   TMP_MARK;
-  qp = TMP_ALLOC_LIMBS (ql);
+  qp = (mp_ptr) TMP_ALLOC (ql * BYTES_PER_MP_LIMB);
   rp = PTR (rem);
   np = PTR (num);
   dp = PTR (den);
@@ -67,7 +67,7 @@
   if (dp == rp)
     {
       mp_ptr tp;
-      tp = TMP_ALLOC_LIMBS (dl);
+      tp = (mp_ptr) TMP_ALLOC (dl * BYTES_PER_MP_LIMB);
       MPN_COPY (tp, dp, dl);
       dp = tp;
     }
@@ -75,7 +75,7 @@
   if (np == rp)
     {
       mp_ptr tp;
-      tp = TMP_ALLOC_LIMBS (nl);
+      tp = (mp_ptr) TMP_ALLOC (nl * BYTES_PER_MP_LIMB);
       MPN_COPY (tp, np, nl);
       np = tp;
     }
--- 1/NEWS
+++ 2/NEWS
@@ -1,20 +1,26 @@
 Copyright 1996, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008,
-2009 Free Software Foundation, Inc.
+2009, 2010 Free Software Foundation, Inc.
 
-This file is part of the GNU MP Library.
+Verbatim copying and distribution of this entire article is permitted
+in any medium, provided this notice is preserved.
 
-The GNU MP Library is free software; you can redistribute it and/or modify
-it under the terms of the GNU Lesser General Public License as published by
-the Free Software Foundation; either version 3 of the License, or (at your
-option) any later version.
-
-The GNU MP Library is distributed in the hope that it will be useful, but
-WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
-or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
-License for more details.
 
-You should have received a copy of the GNU Lesser General Public License
-along with the GNU MP Library.  If not, see http://www.gnu.org/licenses/.
+Changes between GMP version 4.3.1 and 4.3.2
+
+  Bugs:
+  * Fixed bug in mpf_eq.
+  * Fixed overflow issues in mpz_set_str, mpz_inp_str, mpf_set_str, and
+    mpf_get_str.
+  * Avoid unbounded stack allocation for unbalanced multiplication.
+  * Fixed bug in FFT multiplication.
+
+  Speedups:
+  * None, except that proper processor recognition helps affected processors.
+
+  Features:
+  * Recognise more "Core 2" processor variants.
+  * The cofactors of mpz_gcdext and mpn_gcdext are now more strictly
+    normalised, returning to how GMP 4.2 worked.
 
 
 Changes between GMP version 4.3.0 and 4.3.1
@@ -433,11 +439,3 @@
 * mpz division functions round differently.
 * mpz mod functions now really compute mod.
 * mpz_powm and mpz_powm_ui now really use mod for reduction.
-
-
-
-----------------
-Local variables:
-mode: text
-fill-column: 76
-End:
--- 1/printf/doprntf.c
+++ 2/printf/doprntf.c
@@ -62,7 +62,7 @@
   int         fraczeros, fraclen, preczeros;
   char        *s, *free_ptr;
   mp_exp_t    exp;
-  char        exponent[GMP_LIMB_BITS + 10];
+  char        exponent[BITS_PER_MP_LIMB + 10];
   const char  *showbase;
   int         retval = 0;
 
@@ -91,7 +91,7 @@
 	   digit and subtract that from prec.  In either case add 2 so the
 	   round to nearest can be applied accurately.  */
 	ndigits = prec + 2
-	  + EXP(f) * (mp_bases[ABS(p->base)].chars_per_limb + (EXP(f)>=0));
+	  + EXP(f) * (__mp_bases[ABS(p->base)].chars_per_limb + (EXP(f)>=0));
 	ndigits = MAX (ndigits, 1);
 	break;
 
--- 1/printf/snprntffuns.c
+++ 2/printf/snprntffuns.c
@@ -45,7 +45,7 @@
    no indication how big the output would have been.  It's necessary to
    re-run to determine that size.
 
-   "size-1" would mean success from a C99 vsnprintf, and the re-run is
+   "size-1" would mean sucess from a C99 vsnprintf, and the re-run is
    unnecessary in this case, but we don't bother to try to detect what sort
    of vsnprintf we've got.  size-1 should occur rarely in normal
    circumstances.
--- 1/randlc2x.c
+++ 2/randlc2x.c
@@ -92,11 +92,11 @@
     {
       mp_size_t tmp = an + seedn;
       ta = tn + 1;
-      tp = TMP_ALLOC_LIMBS (ta);
+      tp = (mp_ptr) TMP_ALLOC (ta * BYTES_PER_MP_LIMB);
       MPN_ZERO (&tp[tmp], ta - tmp); /* mpn_mul won't zero it out.  */
     }
   else
-    tp = TMP_ALLOC_LIMBS (ta);
+    tp = (mp_ptr) TMP_ALLOC (ta * BYTES_PER_MP_LIMB);
 
   /* t = a * seed.  NOTE: an is always > 0; see initialization.  */
   ASSERT (seedn >= an && an > 0);
@@ -155,7 +155,7 @@
   chunk_nbits = p->_mp_m2exp / 2;
   tn = BITS_TO_LIMBS (chunk_nbits);
 
-  tp = TMP_ALLOC_LIMBS (tn);
+  tp = (mp_ptr) TMP_ALLOC (tn * BYTES_PER_MP_LIMB);
 
   rbitpos = 0;
   while (rbitpos + chunk_nbits <= nbits)
--- 1/scanf/doscan.c
+++ 2/scanf/doscan.c
@@ -70,7 +70,7 @@
        It's necessary to parse up the format string to recognise the GMP
        extra types F, Q and Z.  Other types and conversions are passed
        across to the standard sscanf or fscanf via funs->scan, for ease of
-       implementation.  This is essential in the case of something like glibc
+       implemenation.  This is essential in the case of something like glibc
        %p where the pointer format isn't actually documented.
 
        Because funs->scan doesn't get the whole input it can't put the right
--- 1/tests/amd64check.c
+++ 2/tests/amd64check.c
@@ -25,18 +25,18 @@
 
 /* Vector if constants and register values.  We use one vector to allow access
    via a base pointer, very beneficial for the PIC-enabled amd64call.asm.  */
-mp_limb_t calling_conventions_values[23] =
+long calling_conventions_values[23] =
 {
-  CNST_LIMB(0x1234567887654321),	/* want_rbx */
-  CNST_LIMB(0x89ABCDEFFEDCBA98),	/* want_rbp */
-  CNST_LIMB(0xDEADBEEFBADECAFE),	/* want_r12 */
-  CNST_LIMB(0xFFEEDDCCBBAA9988),	/* want_r13 */
-  CNST_LIMB(0x0011223344556677),	/* want_r14 */
-  CNST_LIMB(0x1234432156788765),	/* want_r15 */
+  0x1234567887654321L,		/* want_rbx */
+  0x89ABCDEFFEDCBA98L,		/* want_rbp */
+  0xDEADBEEFBADECAFEL,		/* want_r12 */
+  0xFFEEDDCCBBAA9988L,		/* want_r13 */
+  0x0011223344556677L,		/* want_r14 */
+  0x1234432156788765L,		/* want_r15 */
 
-  CNST_LIMB(0xFEEDABBACAAFBEED),	/* JUNK_RAX */
-  CNST_LIMB(0xAB78DE89FF5125BB),	/* JUNK_R10 */
-  CNST_LIMB(0x1238901890189031)		/* JUNK_R11 */
+  0xFEEDABBACAAFBEED,		/* JUNK_RAX */
+  0xAB78DE89FF5125BB,		/* JUNK_R10 */
+  0x1238901890189031		/* JUNK_R11 */
 
   /* rest of array used for dynamic values.  */
 };
--- 1/tests/devel/anymul_1.c
+++ 2/tests/devel/anymul_1.c
@@ -147,7 +147,7 @@
       cyc = ((double) t * CLOCK) / (TIMES * size * 1000.0);
       printf (funcname ":    %5ldms (%.3f cycles/limb) [%.2f Gb/s]\n",
 	      t, cyc,
-	      CLOCK/cyc*GMP_LIMB_BITS*GMP_LIMB_BITS/1e9);
+	      CLOCK/cyc*BITS_PER_MP_LIMB*BITS_PER_MP_LIMB/1e9);
 #endif
 
 #ifndef NOCHECK
@@ -237,7 +237,7 @@
     {
 #ifdef _LONG_LONG_LIMB
       printf ("%0*lX%0*lX", (int) (sizeof(mp_limb_t)),
-	      (unsigned long) (p[i] >> (GMP_LIMB_BITS/2)),
+	      (unsigned long) (p[i] >> (BITS_PER_MP_LIMB/2)),
               (int) (sizeof(mp_limb_t)), (unsigned long) (p[i]));
 #else
       printf ("%0*lX", (int) (2 * sizeof(mp_limb_t)), p[i]);
--- 1/tests/devel/aors_n.c
+++ 2/tests/devel/aors_n.c
@@ -214,7 +214,7 @@
     {
 #ifdef _LONG_LONG_LIMB
       printf ("%0*lX%0*lX", (int) (sizeof(mp_limb_t)),
-	      (unsigned long) (p[i] >> (GMP_LIMB_BITS/2)),
+	      (unsigned long) (p[i] >> (BITS_PER_MP_LIMB/2)),
               (int) (sizeof(mp_limb_t)), (unsigned long) (p[i]));
 #else
       printf ("%0*lX", (int) (2 * sizeof(mp_limb_t)), p[i]);
--- 1/tests/devel/copy.c
+++ 2/tests/devel/copy.c
@@ -178,7 +178,7 @@
     {
 #ifdef _LONG_LONG_LIMB
       printf ("%0*lX%0*lX", (int) (sizeof(mp_limb_t)),
-	      (unsigned long) (p[i] >> (GMP_LIMB_BITS/2)),
+	      (unsigned long) (p[i] >> (BITS_PER_MP_LIMB/2)),
               (int) (sizeof(mp_limb_t)), (unsigned long) (p[i]));
 #else
       printf ("%0*lX", (int) (2 * sizeof(mp_limb_t)), p[i]);
--- 1/tests/devel/divmod_1.c
+++ 2/tests/devel/divmod_1.c
@@ -141,7 +141,7 @@
       cyc = ((double) t * CLOCK) / (TIMES * nn * 1000.0);
       printf ("mpn_divrem_1 int:    %5ldms (%.3f cycles/limb) [%.2f Gb/s]\n",
 	      t, cyc,
-	      CLOCK/cyc*GMP_LIMB_BITS*GMP_LIMB_BITS/1e9);
+	      CLOCK/cyc*BITS_PER_MP_LIMB*BITS_PER_MP_LIMB/1e9);
       t0 = cputime();
       for (i = 0; i < TIMES; i++)
 	mpn_divrem_1 (dx + 1, fn, np, 0, dlimb);
@@ -149,7 +149,7 @@
       cyc = ((double) t * CLOCK) / (TIMES * fn * 1000.0);
       printf ("mpn_divrem_1 frac:   %5ldms (%.3f cycles/limb) [%.2f Gb/s]\n",
 	      t, cyc,
-	      CLOCK/cyc*GMP_LIMB_BITS*GMP_LIMB_BITS/1e9);
+	      CLOCK/cyc*BITS_PER_MP_LIMB*BITS_PER_MP_LIMB/1e9);
 #endif
 
       retx = refmpn_divrem_1 (dx + 1, fn, np, nn, dlimb);
@@ -186,7 +186,7 @@
     {
 #ifdef _LONG_LONG_LIMB
       printf ("%0*lX%0*lX", (int) (sizeof(mp_limb_t)),
-	      (unsigned long) (p[i] >> (GMP_LIMB_BITS/2)),
+	      (unsigned long) (p[i] >> (BITS_PER_MP_LIMB/2)),
               (int) (sizeof(mp_limb_t)), (unsigned long) (p[i]));
 #else
       printf ("%0*lX", (int) (2 * sizeof(mp_limb_t)), p[i]);
--- 1/tests/devel/divrem.c
+++ 2/tests/devel/divrem.c
@@ -88,7 +88,7 @@
 
       mpn_random2 (nptr, nsize);
       mpn_random2 (dptr, dsize);
-      dptr[dsize - 1] |= (mp_limb_t) 1 << (GMP_LIMB_BITS - 1);
+      dptr[dsize - 1] |= (mp_limb_t) 1 << (BITS_PER_MP_LIMB - 1);
 
       MPN_COPY (rptr, nptr, nsize);
       qlimb = mpn_divrem (qptr, (mp_size_t) 0, rptr, nsize, dptr, dsize);
--- 1/tests/devel/logops_n.c
+++ 2/tests/devel/logops_n.c
@@ -217,7 +217,7 @@
     {
 #ifdef _LONG_LONG_LIMB
       printf ("%0*lX%0*lX", (int) (sizeof(mp_limb_t)),
-	      (unsigned long) (p[i] >> (GMP_LIMB_BITS/2)),
+	      (unsigned long) (p[i] >> (BITS_PER_MP_LIMB/2)),
               (int) (sizeof(mp_limb_t)), (unsigned long) (p[i]));
 #else
       printf ("%0*lX", (int) (2 * sizeof(mp_limb_t)), p[i]);
--- 1/tests/devel/Makefile.am
+++ 2/tests/devel/Makefile.am
@@ -21,7 +21,7 @@
 INCLUDES = -I$(top_srcdir) -I$(top_srcdir)/tests
 LDADD = $(top_builddir)/tests/libtests.la $(top_builddir)/libgmp.la
 
-# add_n_sub_n add_n_sub_n_2 not yet built since mpn_add_n_sub_n doesn't yet exist
+# addsub_n addsub_n_2 not yet built since mpn_addsub_n doesn't yet exist
 #
 EXTRA_PROGRAMS = \
   aors_n anymul_1 copy divmod_1 divrem shift logops_n tst-addsub try
--- 1/tests/devel/shift.c
+++ 2/tests/devel/shift.c
@@ -194,7 +194,7 @@
     {
 #ifdef _LONG_LONG_LIMB
       printf ("%0*lX%0*lX", (int) (sizeof(mp_limb_t)),
-	      (unsigned long) (p[i] >> (GMP_LIMB_BITS/2)),
+	      (unsigned long) (p[i] >> (BITS_PER_MP_LIMB/2)),
 	      (int) (sizeof(mp_limb_t)), (unsigned long) (p[i]));
 #else
       printf ("%0*lX", (int) (2 * sizeof(mp_limb_t)), p[i]);
--- 1/tests/devel/try.c
+++ 2/tests/devel/try.c
@@ -3,7 +3,7 @@
    THIS IS A TEST PROGRAM USED ONLY FOR DEVELOPMENT.  IT'S ALMOST CERTAIN TO
    BE SUBJECT TO INCOMPATIBLE CHANGES IN FUTURE VERSIONS OF GMP.
 
-Copyright 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2008, 2009 Free Software
+Copyright 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2008 Free Software
 Foundation, Inc.
 
 This file is part of the GNU MP Library.
@@ -96,7 +96,7 @@
    stuff common to all functions, but the exceptions get messy.
 
    When there's no overlap, run with both src>dst and src<dst.  A subtle
-   calling-conventions violation occurred in a P6 copy which depended on the
+   calling-conventions violation occured in a P6 copy which depended on the
    relative location of src and dst.
 
    multiplier_N is more or less a third source region for the addmul_N
@@ -220,7 +220,7 @@
 
 #define MAX3(x,y,z)   (MAX (x, MAX (y, z)))
 
-#if GMP_LIMB_BITS == 32
+#if BITS_PER_MP_LIMB == 32
 #define DEADVAL  CNST_LIMB(0xDEADBEEF)
 #else
 #define DEADVAL  CNST_LIMB(0xDEADBEEFBADDCAFE)
@@ -351,12 +351,10 @@
 
 #define DATA_NON_ZERO         1
 #define DATA_GCD              2
-#define DATA_SRC0_ODD         3
-#define DATA_SRC0_HIGHBIT     4
-#define DATA_SRC1_ODD         5
-#define DATA_SRC1_HIGHBIT     6
-#define DATA_MULTIPLE_DIVISOR 7
-#define DATA_UDIV_QRNND       8
+#define DATA_SRC1_ODD         3
+#define DATA_SRC1_HIGHBIT     4
+#define DATA_MULTIPLE_DIVISOR 5
+#define DATA_UDIV_QRNND       6
   char  data;
 
 /* Default is allow full overlap. */
@@ -592,13 +590,7 @@
 #define TYPE_COM_N            28
 
 #define TYPE_ADDLSH1_N        30
-#define TYPE_ADDLSH2_N        48
-#define TYPE_ADDLSH_N         49
 #define TYPE_SUBLSH1_N        31
-#define TYPE_SUBLSH_N        130
-#define TYPE_RSBLSH1_N        34
-#define TYPE_RSBLSH2_N        46
-#define TYPE_RSBLSH_N         47
 #define TYPE_RSH1ADD_N        32
 #define TYPE_RSH1SUB_N        33
 
@@ -620,9 +612,6 @@
 #define TYPE_MODEXACT_1_ODD   53
 #define TYPE_MODEXACT_1C_ODD  54
 
-#define TYPE_INVERT           55
-#define TYPE_BINVERT          56
-
 #define TYPE_GCD              60
 #define TYPE_GCD_1            61
 #define TYPE_GCD_FINDA        62
@@ -647,9 +636,8 @@
 #define TYPE_SQR              82
 #define TYPE_UMUL_PPMM        83
 #define TYPE_UMUL_PPMM_R      84
-#define TYPE_MULLO_N          85
 
-#define TYPE_SBPI1_DIV_QR     90
+#define TYPE_SB_DIVREM_MN     90
 #define TYPE_TDIV_QR          91
 
 #define TYPE_SQRTREM          100
@@ -858,12 +846,12 @@
   p->dst[1] = 1;
   p->src[0] = 1;
   p->src[1] = 1;
-  REFERENCE (refmpn_add_n_sub_n);
+  REFERENCE (refmpn_addsub_n);
 
   p = &param[TYPE_ADDSUB_NC];
   COPY (TYPE_ADDSUB_N);
   p->carry = CARRY_4;
-  REFERENCE (refmpn_add_n_sub_nc);
+  REFERENCE (refmpn_addsub_nc);
 
 
   p = &param[TYPE_COPY];
@@ -897,35 +885,10 @@
   COPY (TYPE_ADD_N);
   REFERENCE (refmpn_addlsh1_n);
 
-  p = &param[TYPE_ADDLSH2_N];
-  COPY (TYPE_ADD_N);
-  REFERENCE (refmpn_addlsh2_n);
-
-  p = &param[TYPE_ADDLSH_N];
-  COPY (TYPE_ADD_N);
-  p->shift = 1;
-  REFERENCE (refmpn_addlsh_n);
-
   p = &param[TYPE_SUBLSH1_N];
   COPY (TYPE_ADD_N);
   REFERENCE (refmpn_sublsh1_n);
 
-  p = &param[TYPE_SUBLSH_N];
-  COPY (TYPE_ADDLSH_N);
-  REFERENCE (refmpn_sublsh_n);
-
-  p = &param[TYPE_RSBLSH1_N];
-  COPY (TYPE_ADD_N);
-  REFERENCE (refmpn_rsblsh1_n);
-
-  p = &param[TYPE_RSBLSH2_N];
-  COPY (TYPE_ADD_N);
-  REFERENCE (refmpn_rsblsh2_n);
-
-  p = &param[TYPE_RSBLSH_N];
-  COPY (TYPE_ADDLSH_N);
-  REFERENCE (refmpn_rsblsh_n);
-
   p = &param[TYPE_RSH1ADD_N];
   COPY (TYPE_ADD_N);
   REFERENCE (refmpn_rsh1add_n);
@@ -1101,11 +1064,6 @@
   p->src[1] = 1;
   REFERENCE (refmpn_mul_n);
 
-  p = &param[TYPE_MULLO_N];
-  COPY (TYPE_MUL_N);
-  p->dst_size[0] = 0;
-  REFERENCE (refmpn_mullo_n);
-
   p = &param[TYPE_MUL_MN];
   COPY (TYPE_MUL_N);
   p->size2 = 1;
@@ -1149,7 +1107,7 @@
   REFERENCE (refmpn_hamdist);
 
 
-  p = &param[TYPE_SBPI1_DIV_QR];
+  p = &param[TYPE_SB_DIVREM_MN];
   p->retval = 1;
   p->dst[0] = 1;
   p->dst[1] = 1;
@@ -1159,7 +1117,7 @@
   p->size2 = 1;
   p->dst_size[0] = SIZE_DIFF;
   p->overlap = OVERLAP_NONE;
-  REFERENCE (refmpn_sb_div_qr);
+  REFERENCE (refmpn_sb_divrem_mn);
 
   p = &param[TYPE_TDIV_QR];
   p->dst[0] = 1;
@@ -1199,20 +1157,6 @@
   p->overlap = OVERLAP_NONE;
   REFERENCE (refmpn_get_str);
 
-  p = &param[TYPE_BINVERT];
-  p->dst[0] = 1;
-  p->src[0] = 1;
-  p->data = DATA_SRC0_ODD;
-  p->overlap = OVERLAP_NONE;
-  REFERENCE (refmpn_binvert);
-
-  p = &param[TYPE_INVERT];
-  p->dst[0] = 1;
-  p->src[0] = 1;
-  p->data = DATA_SRC0_HIGHBIT;
-  p->overlap = OVERLAP_NONE;
-  REFERENCE (refmpn_invert);
-
 #ifdef EXTRA_PARAM_INIT
   EXTRA_PARAM_INIT
 #endif
@@ -1301,66 +1245,43 @@
 }
 
 void
-mpn_toom22_mul_fun (mp_ptr dst, mp_srcptr src1, mp_srcptr src2, mp_size_t size)
-{
-  mp_ptr  tspace;
-  TMP_DECL;
-  TMP_MARK;
-  tspace = TMP_ALLOC_LIMBS (mpn_toom22_mul_itch (size, size));
-  mpn_toom22_mul (dst, src1, size, src2, size, tspace);
-  TMP_FREE;
-}
-void
-mpn_toom2_sqr_fun (mp_ptr dst, mp_srcptr src, mp_size_t size)
-{
-  mp_ptr tspace;
-  TMP_DECL;
-  TMP_MARK;
-  tspace = TMP_ALLOC_LIMBS (mpn_toom2_sqr_itch (size));
-  mpn_toom2_sqr (dst, src, size, tspace);
-  TMP_FREE;
-}
-void
-mpn_toom33_mul_fun (mp_ptr dst, mp_srcptr src1, mp_srcptr src2, mp_size_t size)
+mpn_kara_mul_n_fun (mp_ptr dst, mp_srcptr src1, mp_srcptr src2, mp_size_t size)
 {
   mp_ptr  tspace;
   TMP_DECL;
   TMP_MARK;
-  tspace = TMP_ALLOC_LIMBS (mpn_toom33_mul_itch (size, size));
-  mpn_toom33_mul (dst, src1, size, src2, size, tspace);
-  TMP_FREE;
+  tspace = TMP_ALLOC_LIMBS (MPN_KARA_MUL_N_TSIZE (size));
+  mpn_kara_mul_n (dst, src1, src2, size, tspace);
 }
 void
-mpn_toom3_sqr_fun (mp_ptr dst, mp_srcptr src, mp_size_t size)
+mpn_kara_sqr_n_fun (mp_ptr dst, mp_srcptr src, mp_size_t size)
 {
   mp_ptr tspace;
   TMP_DECL;
   TMP_MARK;
-  tspace = TMP_ALLOC_LIMBS (mpn_toom3_sqr_itch (size));
-  mpn_toom3_sqr (dst, src, size, tspace);
+  tspace = TMP_ALLOC_LIMBS (MPN_KARA_SQR_N_TSIZE (size));
+  mpn_kara_sqr_n (dst, src, size, tspace);
   TMP_FREE;
 }
 void
-mpn_toom44_mul_fun (mp_ptr dst, mp_srcptr src1, mp_srcptr src2, mp_size_t size)
+mpn_toom3_mul_n_fun (mp_ptr dst, mp_srcptr src1, mp_srcptr src2, mp_size_t size)
 {
   mp_ptr  tspace;
   TMP_DECL;
   TMP_MARK;
-  tspace = TMP_ALLOC_LIMBS (mpn_toom44_mul_itch (size, size));
-  mpn_toom44_mul (dst, src1, size, src2, size, tspace);
-  TMP_FREE;
+  tspace = TMP_ALLOC_LIMBS (MPN_TOOM3_MUL_N_TSIZE (size));
+  mpn_toom3_mul_n (dst, src1, src2, size, tspace);
 }
 void
-mpn_toom4_sqr_fun (mp_ptr dst, mp_srcptr src, mp_size_t size)
+mpn_toom3_sqr_n_fun (mp_ptr dst, mp_srcptr src, mp_size_t size)
 {
   mp_ptr tspace;
   TMP_DECL;
   TMP_MARK;
-  tspace = TMP_ALLOC_LIMBS (mpn_toom4_sqr_itch (size));
-  mpn_toom4_sqr (dst, src, size, tspace);
+  tspace = TMP_ALLOC_LIMBS (MPN_TOOM3_SQR_N_TSIZE (size));
+  mpn_toom3_sqr_n (dst, src, size, tspace);
   TMP_FREE;
 }
-
 mp_limb_t
 umul_ppmm_fun (mp_limb_t *lowptr, mp_limb_t m1, mp_limb_t m2)
 {
@@ -1403,11 +1324,11 @@
   { TRY(mpn_sub_nc),    TYPE_SUB_NC },
 #endif
 
-#if HAVE_NATIVE_mpn_add_n_sub_n
-  { TRY(mpn_add_n_sub_n),  TYPE_ADDSUB_N  },
+#if HAVE_NATIVE_mpn_addsub_n
+  { TRY(mpn_addsub_n),  TYPE_ADDSUB_N  },
 #endif
-#if HAVE_NATIVE_mpn_add_n_sub_nc
-  { TRY(mpn_add_n_sub_nc), TYPE_ADDSUB_NC },
+#if HAVE_NATIVE_mpn_addsub_nc
+  { TRY(mpn_addsub_nc), TYPE_ADDSUB_NC },
 #endif
 
   { TRY(mpn_addmul_1),  TYPE_ADDMUL_1  },
@@ -1462,27 +1383,9 @@
 #if HAVE_NATIVE_mpn_addlsh1_n
   { TRY(mpn_addlsh1_n), TYPE_ADDLSH1_N },
 #endif
-#if HAVE_NATIVE_mpn_addlsh2_n
-  { TRY(mpn_addlsh2_n), TYPE_ADDLSH2_N },
-#endif
-#if HAVE_NATIVE_mpn_addlsh_n
-  { TRY(mpn_addlsh_n), TYPE_ADDLSH_N },
-#endif
 #if HAVE_NATIVE_mpn_sublsh1_n
   { TRY(mpn_sublsh1_n), TYPE_SUBLSH1_N },
 #endif
-#if HAVE_NATIVE_mpn_sublsh_n
-  { TRY(mpn_sublsh_n), TYPE_SUBLSH_N },
-#endif
-#if HAVE_NATIVE_mpn_rsblsh1_n
-  { TRY(mpn_rsblsh1_n), TYPE_RSBLSH1_N },
-#endif
-#if HAVE_NATIVE_mpn_rsblsh2_n
-  { TRY(mpn_rsblsh2_n), TYPE_RSBLSH2_N },
-#endif
-#if HAVE_NATIVE_mpn_rsblsh_n
-  { TRY(mpn_rsblsh_n), TYPE_RSBLSH_N },
-#endif
 #if HAVE_NATIVE_mpn_rsh1add_n
   { TRY(mpn_rsh1add_n), TYPE_RSH1ADD_N },
 #endif
@@ -1533,7 +1436,7 @@
   { TRY(mpn_modexact_1c_odd),       TYPE_MODEXACT_1C_ODD },
 
 
-  { TRY(mpn_sbpi1_div_qr), TYPE_SBPI1_DIV_QR, 3},
+  { TRY(mpn_sb_divrem_mn), TYPE_SB_DIVREM_MN, 3},
   { TRY(mpn_tdiv_qr),      TYPE_TDIV_QR },
 
   { TRY(mpn_mul_1),      TYPE_MUL_1 },
@@ -1555,8 +1458,7 @@
 
 
   { TRY(mpn_mul_basecase), TYPE_MUL_MN },
-  { TRY(mpn_mullo_basecase), TYPE_MULLO_N },
-#if SQR_TOOM2_THRESHOLD > 0
+#if SQR_KARATSUBA_THRESHOLD > 0
   { TRY(mpn_sqr_basecase), TYPE_SQR },
 #endif
 
@@ -1572,12 +1474,10 @@
   { TRY(mpn_umul_ppmm_r),  TYPE_UMUL_PPMM_R, 2 },
 #endif
 
-  { TRY_FUNFUN(mpn_toom22_mul),  TYPE_MUL_N,  MPN_TOOM22_MUL_MINSIZE },
-  { TRY_FUNFUN(mpn_toom2_sqr),   TYPE_SQR,    MPN_TOOM2_SQR_MINSIZE },
-  { TRY_FUNFUN(mpn_toom33_mul),  TYPE_MUL_N,  MPN_TOOM33_MUL_MINSIZE },
-  { TRY_FUNFUN(mpn_toom3_sqr),   TYPE_SQR,    MPN_TOOM3_SQR_MINSIZE },
-  { TRY_FUNFUN(mpn_toom44_mul),  TYPE_MUL_N,  MPN_TOOM44_MUL_MINSIZE },
-  { TRY_FUNFUN(mpn_toom4_sqr),   TYPE_SQR,    MPN_TOOM4_SQR_MINSIZE },
+  { TRY_FUNFUN(mpn_kara_mul_n),  TYPE_MUL_N, MPN_KARA_MUL_N_MINSIZE },
+  { TRY_FUNFUN(mpn_kara_sqr_n),  TYPE_SQR,   MPN_KARA_SQR_N_MINSIZE },
+  { TRY_FUNFUN(mpn_toom3_mul_n), TYPE_MUL_N, MPN_TOOM3_MUL_N_MINSIZE },
+  { TRY_FUNFUN(mpn_toom3_sqr_n), TYPE_SQR,   MPN_TOOM3_SQR_N_MINSIZE },
 
   { TRY(mpn_gcd_1),        TYPE_GCD_1            },
   { TRY(mpn_gcd),          TYPE_GCD              },
@@ -1596,9 +1496,6 @@
 
   { TRY(mpn_get_str),    TYPE_GET_STR },
 
-  { TRY(mpn_binvert),    TYPE_BINVERT },
-  { TRY(mpn_invert),     TYPE_INVERT  },
-
 #ifdef EXTRA_ROUTINES
   EXTRA_ROUTINES
 #endif
@@ -1892,13 +1789,13 @@
 	    byte_tracen ("   d[%d]", i, e->d[i].p, d[i].size);
 	  else
 	    mpn_tracen ("   d[%d]", i, e->d[i].p, d[i].size);
-	  printf ("        located %p\n", (void *) (e->d[i].p));
+	  printf ("        located %p\n", e->d[i].p);
 	}
     }
 
   for (i = 0; i < NUM_SOURCES; i++)
     if (tr->src[i])
-      printf ("   s[%d] located %p\n", i, (void *)  (e->s[i].p));
+      printf ("   s[%d] located %p\n", i, e->s[i].p);
 }
 
 
@@ -2045,21 +1942,12 @@
   case TYPE_ADD_N:
   case TYPE_SUB_N:
   case TYPE_ADDLSH1_N:
-  case TYPE_ADDLSH2_N:
   case TYPE_SUBLSH1_N:
-  case TYPE_RSBLSH1_N:
-  case TYPE_RSBLSH2_N:
   case TYPE_RSH1ADD_N:
   case TYPE_RSH1SUB_N:
     e->retval = CALLING_CONVENTIONS (function)
       (e->d[0].p, e->s[0].p, e->s[1].p, size);
     break;
-  case TYPE_ADDLSH_N:
-  case TYPE_SUBLSH_N:
-  case TYPE_RSBLSH_N:
-    e->retval = CALLING_CONVENTIONS (function)
-      (e->d[0].p, e->s[0].p, e->s[1].p, size, shift);
-    break;
   case TYPE_ADD_NC:
   case TYPE_SUB_NC:
     e->retval = CALLING_CONVENTIONS (function)
@@ -2192,18 +2080,13 @@
       (e->s[0].p[1], e->s[0].p[0], divisor, e->d[0].p);
     break;
 
-  case TYPE_SBPI1_DIV_QR:
-    {
-      gmp_pi1_t dinv;
-      invert_pi1 (dinv, e->s[1].p[size2-1], e->s[1].p[size2-2]); /* FIXME: use refinvert_pi1 */
-      refmpn_copyi (e->d[1].p, e->s[0].p, size);        /* dividend */
-      refmpn_fill (e->d[0].p, size-size2, 0x98765432);  /* quotient */
-      e->retval = CALLING_CONVENTIONS (function)
-	(e->d[0].p, e->d[1].p, size, e->s[1].p, size2, dinv.inv32);
-      refmpn_zero (e->d[1].p+size2, size-size2);    /* excess over remainder */
-    }
+  case TYPE_SB_DIVREM_MN:
+    refmpn_copyi (e->d[1].p, e->s[0].p, size);        /* dividend */
+    refmpn_fill (e->d[0].p, size-size2, 0x98765432);  /* quotient */
+    e->retval = CALLING_CONVENTIONS (function)
+      (e->d[0].p, e->d[1].p, size, e->s[1].p, size2);
+    refmpn_zero (e->d[1].p+size2, size-size2);    /* excess over remainder */
     break;
-
   case TYPE_TDIV_QR:
     CALLING_CONVENTIONS (function) (e->d[0].p, e->d[1].p, 0,
 				    e->s[0].p, size, e->s[1].p, size2);
@@ -2295,7 +2178,6 @@
       (e->d[0].p, e->s[0].p, size, e->s[1].p, size2);
     break;
   case TYPE_MUL_N:
-  case TYPE_MULLO_N:
     CALLING_CONVENTIONS (function) (e->d[0].p, e->s[0].p, e->s[1].p, size);
     break;
   case TYPE_SQR:
@@ -2363,28 +2245,7 @@
 	  e->retval = CALLING_CONVENTIONS (function) (dst, base,
 						      e->d[1].p, size);
 	}
-      refmpn_zero (e->d[1].p, size);  /* clobbered or unused */
-    }
-    break;
-
- case TYPE_INVERT:
-    {
-      mp_ptr scratch;
-      TMP_DECL;
-      TMP_MARK;
-      scratch = TMP_ALLOC_LIMBS (mpn_invert_itch (size));
-      CALLING_CONVENTIONS (function) (e->d[0].p, e->s[0].p, size, scratch);
-      TMP_FREE;
-    }
-    break;
-  case TYPE_BINVERT:
-    {
-      mp_ptr scratch;
-      TMP_DECL;
-      TMP_MARK;
-      scratch = TMP_ALLOC_LIMBS (mpn_binvert_itch (size));
-      CALLING_CONVENTIONS (function) (e->d[0].p, e->s[0].p, size, scratch);
-      TMP_FREE;
+      refmpn_zero (e->d[1].p, size);  /* cloberred or unused */
     }
     break;
 
@@ -2616,11 +2477,6 @@
 	s[i].p[0] |= 1;
 	break;
 
-      case DATA_SRC0_ODD:
-	if (i == 0)
-	  s[i].p[0] |= 1;
-	break;
-
       case DATA_SRC1_ODD:
 	if (i == 1)
 	  s[i].p[0] |= 1;
@@ -2636,13 +2492,6 @@
 	  }
 	break;
 
-      case DATA_SRC0_HIGHBIT:
-       if (i == 0)
-         {
-           s[i].p[size-1] |= GMP_NUMB_HIGHBIT;
-         }
-       break;
-
       case DATA_UDIV_QRNND:
 	s[i].p[1] %= divisor;
 	break;
@@ -2902,8 +2751,8 @@
       {
 	malloc_region (&s[i].region, 2*option_lastsize+ALIGNMENTS-1);
 	printf ("s[%d] %p to %p (0x%lX bytes)\n",
-		i, (void *) (s[i].region.ptr),
-		(void *) (s[i].region.ptr + s[i].region.size),
+		i, s[i].region.ptr,
+		s[i].region.ptr + s[i].region.size,
 		(long) s[i].region.size * BYTES_PER_MP_LIMB);
       }
 
@@ -2912,8 +2761,8 @@
       {                                                                 \
 	malloc_region (&e.d[i].region, 2*option_lastsize+ALIGNMENTS-1); \
 	printf ("%s d[%d] %p to %p (0x%lX bytes)\n",                    \
-		es, i, (void *) (e.d[i].region.ptr),			\
-		(void *)  (e.d[i].region.ptr + e.d[i].region.size),	\
+		es, i, e.d[i].region.ptr,                               \
+		e.d[i].region.ptr + e.d[i].region.size,                 \
 		(long) e.d[i].region.size * BYTES_PER_MP_LIMB);         \
       }
 
--- 1/tests/memory.c
+++ 2/tests/memory.c
@@ -88,12 +88,12 @@
   tests_memory_list = h;
 
   rptr = __gmp_default_allocate (size + 2 * sizeof (mp_limb_t));
-  ptr = (void *) ((gmp_intptr_t) rptr + sizeof (mp_limb_t));
+  ptr = (void *) ((long) rptr + sizeof (mp_limb_t));
 
-  *((mp_limb_t *) ((gmp_intptr_t) ptr - sizeof (mp_limb_t)))
+  *((mp_limb_t *) ((long) ptr - sizeof (mp_limb_t)))
     = PATTERN1 - ((mp_limb_t) ptr);
   PATTERN2_var = PATTERN2 - ((mp_limb_t) ptr);
-  memcpy ((void *) ((gmp_intptr_t) ptr + size), &PATTERN2_var, sizeof (mp_limb_t));
+  memcpy ((void *) ((long) ptr + size), &PATTERN2_var, sizeof (mp_limb_t));
 
   h->size = size;
   h->ptr = ptr;
@@ -109,16 +109,16 @@
 
   if (new_size == 0)
     {
-      fprintf (stderr, "tests_reallocate(): attempt to reallocate %p to 0 bytes\n",
-	       ptr);
+      fprintf (stderr, "tests_reallocate(): attempt to reallocate 0x%lX to 0 bytes\n",
+	       (unsigned long) ptr);
       abort ();
     }
 
   hp = tests_memory_find (ptr);
   if (hp == NULL)
     {
-      fprintf (stderr, "tests_reallocate(): attempt to reallocate bad pointer %p\n",
-	       ptr);
+      fprintf (stderr, "tests_reallocate(): attempt to reallocate bad pointer 0x%lX\n",
+	       (unsigned long) ptr);
       abort ();
     }
   h = *hp;
@@ -130,28 +130,28 @@
       abort ();
     }
 
-  if (*((mp_limb_t *) ((gmp_intptr_t) ptr - sizeof (mp_limb_t)))
+  if (*((mp_limb_t *) ((long) ptr - sizeof (mp_limb_t)))
       != PATTERN1 - ((mp_limb_t) ptr))
     {
       fprintf (stderr, "in realloc: redzone clobbered before block\n");
       abort ();
     }
   PATTERN2_var = PATTERN2 - ((mp_limb_t) ptr);
-  if (memcmp ((void *) ((gmp_intptr_t) ptr + h->size), &PATTERN2_var, sizeof (mp_limb_t)))
+  if (memcmp ((void *) ((long) ptr + h->size), &PATTERN2_var, sizeof (mp_limb_t)))
     {
       fprintf (stderr, "in realloc: redzone clobbered after block\n");
       abort ();
     }
 
-  rptr = __gmp_default_reallocate ((void *) ((gmp_intptr_t) ptr - sizeof (mp_limb_t)),
+  rptr = __gmp_default_reallocate ((void *) ((long) ptr - sizeof (mp_limb_t)),
 				 old_size + 2 * sizeof (mp_limb_t),
 				 new_size + 2 * sizeof (mp_limb_t));
-  ptr = (void *) ((gmp_intptr_t) rptr + sizeof (mp_limb_t));
+  ptr = (void *) ((long) rptr + sizeof (mp_limb_t));
 
-  *((mp_limb_t *) ((gmp_intptr_t) ptr - sizeof (mp_limb_t)))
+  *((mp_limb_t *) ((long) ptr - sizeof (mp_limb_t)))
     = PATTERN1 - ((mp_limb_t) ptr);
   PATTERN2_var = PATTERN2 - ((mp_limb_t) ptr);
-  memcpy ((void *) ((gmp_intptr_t) ptr + new_size), &PATTERN2_var, sizeof (mp_limb_t));
+  memcpy ((void *) ((long) ptr + new_size), &PATTERN2_var, sizeof (mp_limb_t));
 
   h->size = new_size;
   h->ptr = ptr;
@@ -164,8 +164,8 @@
   struct header  **hp = tests_memory_find (ptr);
   if (hp == NULL)
     {
-      fprintf (stderr, "tests_free(): attempt to free bad pointer %p\n",
-	       ptr);
+      fprintf (stderr, "tests_free(): attempt to free bad pointer 0x%lX\n",
+	       (unsigned long) ptr);
       abort ();
     }
   return hp;
@@ -180,20 +180,20 @@
 
   *hp = h->next;  /* unlink */
 
-  if (*((mp_limb_t *) ((gmp_intptr_t) ptr - sizeof (mp_limb_t)))
+  if (*((mp_limb_t *) ((long) ptr - sizeof (mp_limb_t)))
       != PATTERN1 - ((mp_limb_t) ptr))
     {
       fprintf (stderr, "in free: redzone clobbered before block\n");
       abort ();
     }
   PATTERN2_var = PATTERN2 - ((mp_limb_t) ptr);
-  if (memcmp ((void *) ((gmp_intptr_t) ptr + h->size), &PATTERN2_var, sizeof (mp_limb_t)))
+  if (memcmp ((void *) ((long) ptr + h->size), &PATTERN2_var, sizeof (mp_limb_t)))
     {
       fprintf (stderr, "in free: redzone clobbered after block\n");
       abort ();
     }
 
-  __gmp_default_free ((void *) ((gmp_intptr_t) ptr - sizeof(mp_limb_t)),
+  __gmp_default_free ((void *) ((long) ptr - sizeof(mp_limb_t)),
 		      h->size + 2 * sizeof (mp_limb_t));
   __gmp_default_free (h, sizeof (*h));
 }
--- 1/tests/misc.c
+++ 2/tests/misc.c
@@ -132,8 +132,8 @@
 void *
 align_pointer (void *p, size_t align)
 {
-  gmp_intptr_t d;
-  d = ((gmp_intptr_t) p) & (align-1);
+  unsigned long  d;
+  d = ((unsigned long) p) & (align-1);
   d = (d != 0 ? align-d : 0);
   return (void *) (((char *) p) + d);
 }
@@ -370,11 +370,11 @@
 {
 #if GMP_NAIL_BITS == 0
   mp_limb_t  n;
-  _gmp_rand (&n, RANDS, GMP_LIMB_BITS);
+  _gmp_rand (&n, RANDS, BITS_PER_MP_LIMB);
   return n;
 #else
   mp_limb_t n[2];
-  _gmp_rand (n, RANDS, GMP_LIMB_BITS);
+  _gmp_rand (n, RANDS, BITS_PER_MP_LIMB);
   return n[0] + (n[1] << GMP_NUMB_BITS);
 #endif
 }
--- 1/tests/mpbsd/t-mtox.c
+++ 2/tests/mpbsd/t-mtox.c
@@ -38,7 +38,7 @@
 
   for (i = 0; i < 1000; i++)
     {
-      mpz_erandomb (z, rands, 6 * GMP_LIMB_BITS);
+      mpz_erandomb (z, rands, 6 * BITS_PER_MP_LIMB);
       got = mtox (z);
       want = mpz_get_str (NULL, 16, z);
       if (strcmp (got, want) != 0)
--- 1/tests/mpf/Makefile.am
+++ 2/tests/mpf/Makefile.am
@@ -25,7 +25,7 @@
 check_PROGRAMS = t-add t-sub t-conv t-sqrt t-sqrt_ui t-muldiv t-dm2exp reuse \
   t-cmp_d t-cmp_si t-div t-fits t-get_d t-get_d_2exp \
   t-get_si t-get_ui t-gsprec t-inp_str t-int_p t-mul_ui \
-  t-set t-set_q t-set_si t-set_ui t-trunc t-ui_div t-eq
+  t-set t-set_q t-set_si t-set_ui t-trunc t-ui_div
 TESTS = $(check_PROGRAMS)
 
 $(top_builddir)/tests/libtests.la:
--- 1/tests/mpf/t-conv.c
+++ 2/tests/mpf/t-conv.c
@@ -45,7 +45,7 @@
   mp_exp_t bexp;
   long size, exp;
   int base;
-  char buf[SIZE * GMP_LIMB_BITS + 5];
+  char buf[SIZE * BITS_PER_MP_LIMB + 5];
 
   tests_start ();
 
--- 1/tests/mpf/t-muldiv.c
+++ 2/tests/mpf/t-muldiv.c
@@ -36,7 +36,7 @@
   int reps = 10000;
   int i;
   mpf_t u, v, w, x;
-  mp_size_t bprec = SIZE * GMP_LIMB_BITS;
+  mp_size_t bprec = SIZE * BITS_PER_MP_LIMB;
   mpf_t rerr, limit_rerr;
   unsigned long ulimb, vlimb;
   int single_flag;
--- 1/tests/mpn/Makefile.am
+++ 2/tests/mpn/Makefile.am
@@ -22,12 +22,7 @@
 LDADD = $(top_builddir)/tests/libtests.la $(top_builddir)/libgmp.la
 
 check_PROGRAMS = t-asmtype t-aors_1 t-divrem_1 t-fat t-get_d \
-  t-instrument t-iord_u t-mp_bases t-perfsqr t-scan \
-  t-toom22 t-toom32 t-toom33 t-toom42 t-toom43 t-toom44 \
-  t-toom52 t-toom53 t-toom62 \
-  t-bdiv t-hgcd t-matrix22 t-mullo t-mulmod_bnm1
-
-EXTRA_DIST = toom-shared.h
+  t-instrument t-iord_u t-mp_bases t-perfsqr t-scan t-hgcd t-matrix22
 
 TESTS = $(check_PROGRAMS)
 
--- 1/tests/mpq/t-get_str.c
+++ 2/tests/mpq/t-get_str.c
@@ -65,8 +65,8 @@
     {
       printf ("mpq_get_str wrong return value (passing non-NULL)\n");
       printf ("  base %d\n", base);
-      printf ("  got  %p\n", ret);
-      printf ("  want %p\n", want);
+      printf ("  got  0x%lX\n", (unsigned long) ret);
+      printf ("  want 0x%lX\n", (unsigned long) want);
       abort ();
     }
   if (strcmp (str, want) != 0)
--- 1/tests/mpz/bit.c
+++ 2/tests/mpz/bit.c
@@ -207,7 +207,7 @@
 	    {
 	      mpz_set_si (x, (long) initial);
 
-	      bit = (unsigned long) limb*GMP_LIMB_BITS + offset;
+	      bit = (unsigned long) limb*BITS_PER_MP_LIMB + offset;
 
 	      mpz_clrbit (x, bit);
 	      MPZ_CHECK_FORMAT (x);
--- 1/tests/mpz/t-aorsmul.c
+++ 2/tests/mpz/t-aorsmul.c
@@ -388,14 +388,14 @@
 
   for (i = 0; i < reps; i++)
     {
-      mpz_errandomb (w, rands, 5*GMP_LIMB_BITS);
-      mpz_errandomb (x, rands, 5*GMP_LIMB_BITS);
-      mpz_errandomb (y, rands, 5*GMP_LIMB_BITS);
+      mpz_errandomb (w, rands, 5*BITS_PER_MP_LIMB);
+      mpz_errandomb (x, rands, 5*BITS_PER_MP_LIMB);
+      mpz_errandomb (y, rands, 5*BITS_PER_MP_LIMB);
       check_all (w, x, y);
       check_all_inplace (w, y);
 
-      mpz_errandomb (w, rands, 5*GMP_LIMB_BITS);
-      mpz_errandomb (x, rands, 5*GMP_LIMB_BITS);
+      mpz_errandomb (w, rands, 5*BITS_PER_MP_LIMB);
+      mpz_errandomb (x, rands, 5*BITS_PER_MP_LIMB);
       mpz_errandomb (y, rands, BITS_PER_ULONG);
       check_all (w, x, y);
       check_all_inplace (w, y);
--- 1/tests/mpz/t-cong_2exp.c
+++ 2/tests/mpz/t-cong_2exp.c
@@ -124,12 +124,12 @@
 
   for (i = 0; i < reps; i++)
     {
-      mpz_errandomb (a, rands, 8*GMP_LIMB_BITS);
-      mpz_errandomb (c, rands, 8*GMP_LIMB_BITS);
-      d = urandom() % (8*GMP_LIMB_BITS);
+      mpz_errandomb (a, rands, 8*BITS_PER_MP_LIMB);
+      mpz_errandomb (c, rands, 8*BITS_PER_MP_LIMB);
+      d = urandom() % (8*BITS_PER_MP_LIMB);
 
-      mpz_mul_2exp (a, a, urandom() % (2*GMP_LIMB_BITS));
-      mpz_mul_2exp (c, c, urandom() % (2*GMP_LIMB_BITS));
+      mpz_mul_2exp (a, a, urandom() % (2*BITS_PER_MP_LIMB));
+      mpz_mul_2exp (c, c, urandom() % (2*BITS_PER_MP_LIMB));
 
       mpz_negrandom (a, rands);
       mpz_negrandom (c, rands);
--- 1/tests/mpz/t-cong.c
+++ 2/tests/mpz/t-cong.c
@@ -146,11 +146,11 @@
 
   for (i = 0; i < reps; i++)
     {
-      mpz_errandomb (a, rands, 8*GMP_LIMB_BITS);
+      mpz_errandomb (a, rands, 8*BITS_PER_MP_LIMB);
       MPZ_CHECK_FORMAT (a);
-      mpz_errandomb (c, rands, 8*GMP_LIMB_BITS);
+      mpz_errandomb (c, rands, 8*BITS_PER_MP_LIMB);
       MPZ_CHECK_FORMAT (c);
-      mpz_errandomb_nonzero (d, rands, 8*GMP_LIMB_BITS);
+      mpz_errandomb_nonzero (d, rands, 8*BITS_PER_MP_LIMB);
 
       mpz_negrandom (a, rands);
       MPZ_CHECK_FORMAT (a);
@@ -171,7 +171,7 @@
 
       if (! mpz_pow2abs_p (d))
         {
-          refmpz_combit (a, urandom() % (8*GMP_LIMB_BITS));
+          refmpz_combit (a, urandom() % (8*BITS_PER_MP_LIMB));
           check_one (a, c, d, 0);
         }
     }
--- 1/tests/mpz/t-divis.c
+++ 2/tests/mpz/t-divis.c
@@ -1,6 +1,7 @@
-/* test mpz_divisible_p and mpz_divisible_ui_p
+/* test mpz_divisible_p and mpz_divisible_ui_p */
 
-Copyright 2001, 2009 Free Software Foundation, Inc.
+/*
+Copyright 2001 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -124,8 +125,8 @@
 
   for (i = 0; i < reps; i++)
     {
-      mpz_erandomb (a, rands, 1 << 19);
-      mpz_erandomb_nonzero (d, rands, 1 << 18);
+      mpz_erandomb (a, rands, 512);
+      mpz_erandomb_nonzero (d, rands, 512);
 
       mpz_fdiv_r (r, a, d);
 
@@ -151,7 +152,7 @@
 int
 main (int argc, char *argv[])
 {
-  int  reps = 100;
+  int  reps = 1000;
 
   tests_start ();
 
--- 1/tests/mpz/t-fac_ui.c
+++ 2/tests/mpz/t-fac_ui.c
@@ -65,7 +65,7 @@
           abort ();
         }
 
-      mpz_mul_ui (f, f, n+1);  /* (n+1)! = n! * (n+1) */
+      mpz_mul_ui (f, f, n+1);  /* (n+1)! = n! * n */
     }
 
   mpz_clear (f);
--- 1/tests/mpz/t-fib_ui.c
+++ 2/tests/mpz/t-fib_ui.c
@@ -73,7 +73,7 @@
 main (int argc, char *argv[])
 {
   unsigned long  n;
-  unsigned long  limit = 100 * GMP_LIMB_BITS;
+  unsigned long  limit = 100 * BITS_PER_MP_LIMB;
   mpz_t          want_fn, want_fn1, got_fn, got_fn1;
 
   tests_start ();
--- 1/tests/mpz/t-fits.c
+++ 2/tests/mpz/t-fits.c
@@ -84,7 +84,7 @@
   EXPECT (mpz_fits_sshort_p, 1);
 
   mpz_set_ui (z, 1L);
-  mpz_mul_2exp (z, z, 5L*GMP_LIMB_BITS);
+  mpz_mul_2exp (z, z, 5L*BITS_PER_MP_LIMB);
   expr = "2^(5*BPML)";
   EXPECT (mpz_fits_ulong_p, 0);
   EXPECT (mpz_fits_uint_p, 0);
--- 1/tests/mpz/t-gcd.c
+++ 2/tests/mpz/t-gcd.c
@@ -136,7 +136,7 @@
   mpz_add (op1, op1, op2);
   mpz_mul_ui (op2, op2, 2);
   one_test (op1, op2, NULL, -1);
-  
+
 #if 0
   mpz_set_str (op1, "4da8e405e0d2f70d6d679d3de08a5100a81ec2cff40f97b313ae75e1183f1df2b244e194ebb02a4ece50d943640a301f0f6cc7f539117b783c3f3a3f91649f8a00d2e1444d52722810562bce02fccdbbc8fe3276646e306e723dd3b", 16);
   mpz_set_str (op2, "76429e12e4fdd8929d89c21657097fbac09d1dc08cf7f1323a34e78ca34226e1a7a29b86fee0fa7fe2cc2a183d46d50df1fe7029590974ad7da77605f35f902cb8b9b8d22dd881eaae5919675d49a337145a029c3b33fc2b0", 16);
--- 1/tests/mpz/t-io_raw.c
+++ 2/tests/mpz/t-io_raw.c
@@ -233,7 +233,7 @@
 
   for (i = 0; i < 500; i++)
     {
-      mpz_erandomb (want, rands, 10*GMP_LIMB_BITS);
+      mpz_erandomb (want, rands, 10*BITS_PER_MP_LIMB);
       mpz_negrandom (want, rands);
 
       fp = fopen_wplusb_or_die (FILENAME);
--- 1/tests/mpz/t-lucnum_ui.c
+++ 2/tests/mpz/t-lucnum_ui.c
@@ -35,7 +35,7 @@
 check_sequence (int argc, char *argv[])
 {
   unsigned long  n;
-  unsigned long  limit = 100 * GMP_LIMB_BITS;
+  unsigned long  limit = 100 * BITS_PER_MP_LIMB;
   mpz_t          want_ln, want_ln1, got_ln, got_ln1;
 
   if (argc > 1 && argv[1][0] == 'x')
--- 1/tests/mpz/t-mul.c
+++ 2/tests/mpz/t-mul.c
@@ -27,7 +27,8 @@
 #include "tests.h"
 
 void debug_mp __GMP_PROTO ((mpz_t));
-static void refmpz_mul __GMP_PROTO ((mpz_t, const mpz_t, const mpz_t));
+static void ref_mpn_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
+static void ref_mpz_mul __GMP_PROTO ((mpz_t, const mpz_t, const mpz_t));
 void dump_abort __GMP_PROTO ((int, char *, mpz_t, mpz_t, mpz_t, mpz_t));
 
 #define FFT_MIN_BITSIZE 100000
@@ -38,13 +39,15 @@
 one (int i, mpz_t multiplicand, mpz_t multiplier)
 {
   mpz_t product, ref_product;
+  mpz_t quotient;
 
   mpz_init (product);
   mpz_init (ref_product);
+  mpz_init (quotient);
 
   /* Test plain multiplication comparing results against reference code.  */
   mpz_mul (product, multiplier, multiplicand);
-  refmpz_mul (ref_product, multiplier, multiplicand);
+  ref_mpz_mul (ref_product, multiplier, multiplicand);
   if (mpz_cmp (product, ref_product))
     dump_abort (i, "incorrect plain product",
 		multiplier, multiplicand, product, ref_product);
@@ -59,6 +62,7 @@
 
   mpz_clear (product);
   mpz_clear (ref_product);
+  mpz_clear (quotient);
 }
 
 int
@@ -137,7 +141,7 @@
 }
 
 static void
-refmpz_mul (mpz_t w, const mpz_t u, const mpz_t v)
+ref_mpz_mul (mpz_t w, const mpz_t u, const mpz_t v)
 {
   mp_size_t usize = u->_mp_size;
   mp_size_t vsize = v->_mp_size;
@@ -165,9 +169,9 @@
   wp = __GMP_ALLOCATE_FUNC_LIMBS (talloc);
 
   if (usize > vsize)
-    refmpn_mul (wp, up, usize, vp, vsize);
+    ref_mpn_mul (wp, up, usize, vp, vsize);
   else
-    refmpn_mul (wp, vp, vsize, up, usize);
+    ref_mpn_mul (wp, vp, vsize, up, usize);
   wsize = usize + vsize;
   wsize -= wp[wsize - 1] == 0;
   MPZ_REALLOC (w, wsize);
@@ -177,6 +181,121 @@
   __GMP_FREE_FUNC_LIMBS (wp, talloc);
 }
 
+static void mul_basecase __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
+
+#define TOOM3_THRESHOLD (MAX (MUL_TOOM3_THRESHOLD, SQR_TOOM3_THRESHOLD))
+#define FFT_THRESHOLD (MAX (MUL_FFT_THRESHOLD, SQR_FFT_THRESHOLD))
+
+static void
+ref_mpn_mul (mp_ptr wp, mp_srcptr up, mp_size_t un, mp_srcptr vp, mp_size_t vn)
+{
+  mp_ptr tp;
+  mp_size_t tn;
+  mp_limb_t cy;
+
+  if (vn < TOOM3_THRESHOLD)
+    {
+      /* In the mpn_mul_basecase and mpn_kara_mul_n range, use our own
+	 mul_basecase.  */
+      if (vn != 0)
+	mul_basecase (wp, up, un, vp, vn);
+      else
+	MPN_ZERO (wp, un);
+      return;
+    }
+
+  if (vn < FFT_THRESHOLD)
+    {
+      /* In the mpn_toom3_mul_n range, use mpn_kara_mul_n.  */
+      tn = 2 * vn + MPN_KARA_MUL_N_TSIZE (vn);
+      tp = __GMP_ALLOCATE_FUNC_LIMBS (tn);
+      mpn_kara_mul_n (tp, up, vp, vn, tp + 2 * vn);
+    }
+  else
+    {
+      /* Finally, for the largest operands, use mpn_toom3_mul_n.  */
+      /* The "- 63 + 255" tweaks the allocation to allow for huge operands.
+	 See the definition of this macro in gmp-impl.h to understand this.  */
+      tn = 2 * vn + MPN_TOOM3_MUL_N_TSIZE (vn) - 63 + 255;
+      tp = __GMP_ALLOCATE_FUNC_LIMBS (tn);
+      mpn_toom3_mul_n (tp, up, vp, vn, tp + 2 * vn);
+    }
+
+  if (un != vn)
+    {
+      if (un - vn < vn)
+	ref_mpn_mul (wp + vn, vp, vn, up + vn, un - vn);
+      else
+	ref_mpn_mul (wp + vn, up + vn, un - vn, vp, vn);
+
+      MPN_COPY (wp, tp, vn);
+      cy = mpn_add_n (wp + vn, wp + vn, tp + vn, vn);
+      mpn_incr_u (wp + 2 * vn, cy);
+    }
+  else
+    {
+      MPN_COPY (wp, tp, 2 * vn);
+    }
+
+  __GMP_FREE_FUNC_LIMBS (tp, tn);
+}
+
+static void
+mul_basecase (mp_ptr wp, mp_srcptr up, mp_size_t un, mp_srcptr vp, mp_size_t vn)
+{
+  mp_size_t i, j;
+  mp_limb_t prod_low, prod_high;
+  mp_limb_t cy_dig;
+  mp_limb_t v_limb;
+
+  /* Multiply by the first limb in V separately, as the result can
+     be stored (not added) to PROD.  We also avoid a loop for zeroing.  */
+  v_limb = vp[0];
+  cy_dig = 0;
+  for (j = un; j > 0; j--)
+    {
+      mp_limb_t u_limb, w_limb;
+      u_limb = *up++;
+      umul_ppmm (prod_high, prod_low, u_limb, v_limb << GMP_NAIL_BITS);
+      add_ssaaaa (cy_dig, w_limb, prod_high, prod_low, 0, cy_dig << GMP_NAIL_BITS);
+      *wp++ = w_limb >> GMP_NAIL_BITS;
+    }
+
+  *wp++ = cy_dig;
+  wp -= un;
+  up -= un;
+
+  /* For each iteration in the outer loop, multiply one limb from
+     U with one limb from V, and add it to PROD.  */
+  for (i = 1; i < vn; i++)
+    {
+      v_limb = vp[i];
+      cy_dig = 0;
+
+      for (j = un; j > 0; j--)
+	{
+	  mp_limb_t u_limb, w_limb;
+	  u_limb = *up++;
+	  umul_ppmm (prod_high, prod_low, u_limb, v_limb << GMP_NAIL_BITS);
+	  w_limb = *wp;
+	  add_ssaaaa (prod_high, prod_low, prod_high, prod_low, 0, w_limb << GMP_NAIL_BITS);
+	  prod_low >>= GMP_NAIL_BITS;
+	  prod_low += cy_dig;
+#if GMP_NAIL_BITS == 0
+	  cy_dig = prod_high + (prod_low < cy_dig);
+#else
+	  cy_dig = prod_high;
+	  cy_dig += prod_low >> GMP_NUMB_BITS;
+#endif
+	  *wp++ = prod_low & GMP_NUMB_MASK;
+	}
+
+      *wp++ = cy_dig;
+      wp -= un;
+      up -= un;
+    }
+}
+
 void
 dump_abort (int i, char *s,
             mpz_t op1, mpz_t op2, mpz_t product, mpz_t ref_product)
--- 1/tests/mpz/t-perfpow.c
+++ 2/tests/mpz/t-perfpow.c
@@ -1,8 +1,6 @@
 /* Test mpz_perfect_power_p.
 
-   Contributed to the GNU project by Torbjorn Granlund and Martin Boij.
-
-Copyright 2008, 2009 Free Software Foundation, Inc.
+Copyright 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -102,124 +100,44 @@
   mpz_clear (x);
 }
 
-#define NRP 15
-
 void
-check_random (int reps)
+check_random (int n_tests)
 {
-  mpz_t n, np, temp, primes[NRP];
-  int i, j, k, unique, destroy, res;
-  unsigned long int nrprimes, primebits, g, exp[NRP], e;
+  int test;
+  mpz_t bs, op1, op2, x;
   gmp_randstate_ptr rands;
+  unsigned long ex;
 
   rands = RANDS;
 
-  mpz_init (n);
-  mpz_init (np);
-  mpz_init (temp);
-
-  for (i = 0; i < NRP; i++)
-    mpz_init (primes[i]);
+  mpz_init (bs);
+  mpz_init (op1);
+  mpz_init (op2);
+  mpz_init (x);
 
-  for (i = 0; i < reps; i++)
+  for (test = 0; test < n_tests; test++)
     {
-      mpz_urandomb (np, rands, 32);
-      nrprimes = mpz_get_ui (np) % NRP + 1; /* 1-NRP unique primes */
-
-      mpz_urandomb (np, rands, 32);
-      g = mpz_get_ui (np) % 32 + 2; /* gcd 2-33 */
+      mpz_urandomb (bs, rands, 32);
+      mpz_rrandomb (op1, rands, mpz_get_ui (bs) % 32);
+      if (test & 1)
+	mpz_neg (op1, op1);
+      mpz_rrandomb (op2, rands, (mpz_get_ui (bs) >> 5) % 8);
 
-      for (j = 0; j < nrprimes;)
-	{
-	  mpz_urandomb (np, rands, 32);
-	  primebits = mpz_get_ui (np) % 100 + 3; /* 3-102 bit primes */
-	  mpz_urandomb (primes[j], rands, primebits);
-	  mpz_nextprime (primes[j], primes[j]);
-	  unique = 1;
-	  for (k = 0; k < j; k++)
-	    {
-	      if (mpz_cmp (primes[j], primes[k]) == 0)
-		{
-		  unique = 0;
-		  break;
-		}
-	    }
-	  if (unique)
-	    {
-	      mpz_urandomb (np, rands, 32);
-	      e = 371 / (10 * primebits) + mpz_get_ui (np) % 11 + 1; /* Magic constants */
-	      exp[j++] = g * e;
-	    }
-	}
+      ex = mpz_get_ui (op2) + 2;
+      mpz_pow_ui (x, op1, ex);
 
-      if (nrprimes > 1)
+      if (! mpz_perfect_power_p (x))
 	{
-	  /* Destroy d exponents, d in [1, nrprimes - 1] */
-	  if (nrprimes == 2)
-	    {
-	      destroy = 1;
-	    }
-	  else
-	    {
-	      mpz_urandomb (np, rands, 32);
-	      destroy = mpz_get_ui (np) % (nrprimes - 2) + 1;
-	    }
-
-	  g = exp[destroy];
-	  for (k = destroy + 1; k < nrprimes; k++)
-	    g = mpn_gcd_1 (&g, 1, exp[k]);
-
-	  for (j = 0; j < destroy; j++)
-	    {
-	      mpz_urandomb (np, rands, 32);
-	      e = mpz_get_ui (np) % 50 + 1;
-	      while (mpn_gcd_1 (&g, 1, e) > 1)
-		e++;
-
-	      exp[j] = e;
-	    }
-	}
-
-      /* Compute n */
-      mpz_pow_ui (n, primes[0], exp[0]);
-      for (j = 1; j < nrprimes; j++)
-	{
-	  mpz_pow_ui (temp, primes[j], exp[j]);
-	  mpz_mul (n, n, temp);
-	}
-
-      res = mpz_perfect_power_p (n);
-
-      if (nrprimes == 1)
-	{
-	if (res == 0 && exp[0] > 1)
-	  {
-	    printf("n is a perfect power, perfpow_p disagrees\n");
-	    gmp_printf("n = %Zu\nprimes[0] = %Zu\nexp[0] = %lu\n", n, primes[0], exp[0]);
-	    abort ();
-	  }
-	else if (res == 1 && exp[0] == 1)
-	  {
-	    gmp_printf("n = %Zu\n", n);
-	    printf("n is now a prime number, but perfpow_p still believes n is a perfect power\n");
-	    abort ();
-	  }
-	}
-      else
-	{
-	  if (res == 1)
-	    {
-	      gmp_printf("n = %Zu\nn was destroyed, but perfpow_p still believes n is a perfect power\n", n);
-	      abort ();
-	    }
+	  gmp_fprintf (stderr, "mpz_perfect_power_p rejects perfect power %Zd^%ld\n", op1, ex);
+	  gmp_fprintf (stderr, "fault operand: %Zx\n", x);
+	  abort ();
 	}
     }
 
-  mpz_clear (n);
-  mpz_clear (np);
-  mpz_clear (temp);
-  for (i = 0; i < NRP; i++)
-    mpz_clear (primes[i]);
+  mpz_clear (bs);
+  mpz_clear (op1);
+  mpz_clear (op2);
+  mpz_clear (x);
 }
 
 int
@@ -232,7 +150,7 @@
 
   check_tests ();
 
-  n_tests = 1000;
+  n_tests = 100000;
   if (argc == 2)
     n_tests = atoi (argv[1]);
   check_random (n_tests);
--- 1/tests/mpz/t-popcount.c
+++ 2/tests/mpz/t-popcount.c
@@ -32,7 +32,7 @@
   unsigned long  i, got;
 
   mpz_init (n);
-  for (i = 0; i < 5 * GMP_LIMB_BITS; i++)
+  for (i = 0; i < 5 * BITS_PER_MP_LIMB; i++)
     {
       mpz_setbit (n, i);
       got = mpz_popcount (n);
--- 1/tests/mpz/t-root.c
+++ 2/tests/mpz/t-root.c
@@ -98,7 +98,7 @@
   mpz_t root1;
   mp_size_t x2_size;
   int i;
-  int reps = 20000;
+  int reps = 5000;
   unsigned long nth;
   gmp_randstate_ptr rands;
   mpz_t bs;
--- 1/tests/mpz/t-set_f.c
+++ 2/tests/mpz/t-set_f.c
@@ -28,7 +28,7 @@
 check_one (mpz_srcptr z)
 {
   static const int shift[] = {
-    0, 1, GMP_LIMB_BITS, 2*GMP_LIMB_BITS, 5*GMP_LIMB_BITS
+    0, 1, BITS_PER_MP_LIMB, 2*BITS_PER_MP_LIMB, 5*BITS_PER_MP_LIMB
   };
 
   int    sh, shneg, neg;
@@ -101,10 +101,10 @@
   mpz_set_si (z, 123L);
   check_one (z);
 
-  mpz_rrandomb (z, RANDS, 2*GMP_LIMB_BITS);
+  mpz_rrandomb (z, RANDS, 2*BITS_PER_MP_LIMB);
   check_one (z);
 
-  mpz_rrandomb (z, RANDS, 5*GMP_LIMB_BITS);
+  mpz_rrandomb (z, RANDS, 5*BITS_PER_MP_LIMB);
   check_one (z);
 
   mpz_clear (z);
--- 1/tests/rand/findlc.c
+++ 2/tests/rand/findlc.c
@@ -47,7 +47,7 @@
     signal (SIGSEGV, SIG_DFL);
 }
 
-/* Input is a modulus (m).  We shall find multiplier (a) and adder (c)
+/* Input is a modulus (m).  We shall find multiplyer (a) and adder (c)
    conforming to the rules found in the first comment block in file
    mpz/urandom.c.
 
--- 1/tests/rand/gen.c
+++ 2/tests/rand/gen.c
@@ -277,7 +277,7 @@
     case RFUNC_mpf_urandomb:
 #if 0
       /* Don't init a too small generator.  */
-      size = PREC (f1) * GMP_LIMB_BITS;
+      size = PREC (f1) * BITS_PER_MP_LIMB;
       /* Fall through.  */
 #endif
     case RFUNC_mpz_urandomb:
--- 1/tests/rand/statlib.c
+++ 2/tests/rand/statlib.c
@@ -298,7 +298,7 @@
 
    X[] must not contain numbers outside the range 0 <= X <= IMAX.
 
-   Return value is number of observations actually used, after
+   Return value is number of observations actally used, after
    discarding entries out of range.
 
    Since X[] contains integers between zero and IMAX, inclusive, we
--- 1/tests/refmpf.c
+++ 2/tests/refmpf.c
@@ -40,7 +40,7 @@
   if (SIZ (u) == 0)
     {
       size = ABSIZ (v);
-      wt = TMP_ALLOC_LIMBS (size + 1);
+      wt = (mp_ptr) TMP_ALLOC ((size+1) * BYTES_PER_MP_LIMB);
       MPN_COPY (wt, PTR (v), size);
       exp = EXP (v);
       neg = SIZ (v) < 0;
@@ -49,7 +49,7 @@
   if (SIZ (v) == 0)
     {
       size = ABSIZ (u);
-      wt = TMP_ALLOC_LIMBS (size + 1);
+      wt = (mp_ptr) TMP_ALLOC ((size+1) * BYTES_PER_MP_LIMB);
       MPN_COPY (wt, PTR (u), size);
       exp = EXP (u);
       neg = SIZ (u) < 0;
@@ -70,9 +70,9 @@
   hi = MAX (EXP (u), EXP (v));
   lo = MIN (EXP (u) - ABSIZ (u), EXP (v) - ABSIZ (v));
   size = hi - lo;
-  ut = TMP_ALLOC_LIMBS (size + 1);
-  vt = TMP_ALLOC_LIMBS (size + 1);
-  wt = TMP_ALLOC_LIMBS (size + 1);
+  ut = (mp_ptr) TMP_ALLOC ((size + 1) * BYTES_PER_MP_LIMB);
+  vt = (mp_ptr) TMP_ALLOC ((size + 1) * BYTES_PER_MP_LIMB);
+  wt = (mp_ptr) TMP_ALLOC ((size + 1) * BYTES_PER_MP_LIMB);
   MPN_ZERO (ut, size);
   MPN_ZERO (vt, size);
   {int off;
@@ -211,7 +211,7 @@
   if (SIZ (u) == 0)
     {
       size = ABSIZ (v);
-      wt = TMP_ALLOC_LIMBS (size + 1);
+      wt = (mp_ptr) TMP_ALLOC ((size+1) * BYTES_PER_MP_LIMB);
       MPN_COPY (wt, PTR (v), size);
       exp = EXP (v);
       neg = SIZ (v) > 0;
@@ -220,7 +220,7 @@
   if (SIZ (v) == 0)
     {
       size = ABSIZ (u);
-      wt = TMP_ALLOC_LIMBS (size + 1);
+      wt = (mp_ptr) TMP_ALLOC ((size+1) * BYTES_PER_MP_LIMB);
       MPN_COPY (wt, PTR (u), size);
       exp = EXP (u);
       neg = SIZ (u) < 0;
@@ -243,9 +243,9 @@
   hi = MAX (EXP (u), EXP (v));
   lo = MIN (EXP (u) - ABSIZ (u), EXP (v) - ABSIZ (v));
   size = hi - lo;
-  ut = TMP_ALLOC_LIMBS (size + 1);
-  vt = TMP_ALLOC_LIMBS (size + 1);
-  wt = TMP_ALLOC_LIMBS (size + 1);
+  ut = (mp_ptr) TMP_ALLOC ((size + 1) * BYTES_PER_MP_LIMB);
+  vt = (mp_ptr) TMP_ALLOC ((size + 1) * BYTES_PER_MP_LIMB);
+  wt = (mp_ptr) TMP_ALLOC ((size + 1) * BYTES_PER_MP_LIMB);
   MPN_ZERO (ut, size);
   MPN_ZERO (vt, size);
   {int off;
--- 1/tests/refmpn.c
+++ 2/tests/refmpn.c
@@ -2,7 +2,7 @@
    of the normal gmp code.  Speed isn't a consideration.
 
 Copyright 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006,
-2007, 2008, 2009 Free Software Foundation, Inc.
+2007, 2008 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -184,7 +184,7 @@
 mp_limb_t
 refmpn_msbone (mp_limb_t x)
 {
-  mp_limb_t  n = (mp_limb_t) 1 << (GMP_LIMB_BITS-1);
+  mp_limb_t  n = (mp_limb_t) 1 << (BITS_PER_MP_LIMB-1);
 
   while (n != 0)
     {
@@ -611,91 +611,40 @@
 }
 
 mp_limb_t
-refmpn_addlsh_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp,
-		 mp_size_t n, unsigned int s)
+refmpn_addlsh1_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
 {
   mp_limb_t cy;
   mp_ptr tp;
 
   ASSERT (refmpn_overlap_fullonly_two_p (rp, up, vp, n));
   ASSERT (n >= 1);
-  ASSERT (0 < s && s < GMP_NUMB_BITS);
   ASSERT_MPN (up, n);
   ASSERT_MPN (vp, n);
 
   tp = refmpn_malloc_limbs (n);
-  cy  = refmpn_lshift (tp, vp, n, s);
+  cy  = refmpn_lshift (tp, vp, n, 1);
   cy += refmpn_add_n (rp, up, tp, n);
   free (tp);
   return cy;
 }
 mp_limb_t
-refmpn_addlsh1_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
-{
-  return refmpn_addlsh_n (rp, up, vp, n, 1);
-}
-mp_limb_t
-refmpn_addlsh2_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
-{
-  return refmpn_addlsh_n (rp, up, vp, n, 2);
-}
-
-mp_limb_t
-refmpn_sublsh_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp,
-		 mp_size_t n, unsigned int s)
+refmpn_sublsh1_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
 {
   mp_limb_t cy;
   mp_ptr tp;
 
   ASSERT (refmpn_overlap_fullonly_two_p (rp, up, vp, n));
   ASSERT (n >= 1);
-  ASSERT (0 < s && s < GMP_NUMB_BITS);
   ASSERT_MPN (up, n);
   ASSERT_MPN (vp, n);
 
   tp = refmpn_malloc_limbs (n);
-  cy  = mpn_lshift (tp, vp, n, s);
+  cy  = mpn_lshift (tp, vp, n, 1);
   cy += mpn_sub_n (rp, up, tp, n);
   free (tp);
   return cy;
 }
 mp_limb_t
-refmpn_sublsh1_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
-{
-  return refmpn_sublsh_n (rp, up, vp, n, 1);
-}
-
-mp_limb_signed_t
-refmpn_rsblsh_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp,
-		 mp_size_t n, unsigned int s)
-{
-  mp_limb_signed_t cy;
-  mp_ptr tp;
-
-  ASSERT (refmpn_overlap_fullonly_two_p (rp, up, vp, n));
-  ASSERT (n >= 1);
-  ASSERT (0 < s && s < GMP_NUMB_BITS);
-  ASSERT_MPN (up, n);
-  ASSERT_MPN (vp, n);
-
-  tp = refmpn_malloc_limbs (n);
-  cy  = mpn_lshift (tp, vp, n, s);
-  cy -= mpn_sub_n (rp, tp, up, n);
-  free (tp);
-  return cy;
-}
-mp_limb_signed_t
-refmpn_rsblsh1_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
-{
-  return refmpn_rsblsh_n (rp, up, vp, n, 1);
-}
-mp_limb_signed_t
-refmpn_rsblsh2_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
-{
-  return refmpn_rsblsh_n (rp, up, vp, n, 2);
-}
-
-mp_limb_t
 refmpn_rsh1add_n (mp_ptr rp, mp_srcptr up, mp_srcptr vp, mp_size_t n)
 {
   mp_limb_t cya, cys;
@@ -769,10 +718,10 @@
 }
 
 
-#define SHIFTHIGH(x) ((x) << GMP_LIMB_BITS/2)
-#define SHIFTLOW(x)  ((x) >> GMP_LIMB_BITS/2)
+#define SHIFTHIGH(x) ((x) << BITS_PER_MP_LIMB/2)
+#define SHIFTLOW(x)  ((x) >> BITS_PER_MP_LIMB/2)
 
-#define LOWMASK   (((mp_limb_t) 1 << GMP_LIMB_BITS/2)-1)
+#define LOWMASK   (((mp_limb_t) 1 << BITS_PER_MP_LIMB/2)-1)
 #define HIGHMASK  SHIFTHIGH(LOWMASK)
 
 #define LOWPART(x)   ((x) & LOWMASK)
@@ -987,7 +936,7 @@
 }
 
 mp_limb_t
-refmpn_add_n_sub_nc (mp_ptr r1p, mp_ptr r2p,
+refmpn_addsub_nc (mp_ptr r1p, mp_ptr r2p,
 		  mp_srcptr s1p, mp_srcptr s2p, mp_size_t size,
 		  mp_limb_t carry)
 {
@@ -1012,15 +961,15 @@
 }
 
 mp_limb_t
-refmpn_add_n_sub_n (mp_ptr r1p, mp_ptr r2p,
+refmpn_addsub_n (mp_ptr r1p, mp_ptr r2p,
 		 mp_srcptr s1p, mp_srcptr s2p, mp_size_t size)
 {
-  return refmpn_add_n_sub_nc (r1p, r2p, s1p, s2p, size, CNST_LIMB(0));
+  return refmpn_addsub_nc (r1p, r2p, s1p, s2p, size, CNST_LIMB(0));
 }
 
 
 /* Right shift hi,lo and return the low limb of the result.
-   Note a shift by GMP_LIMB_BITS isn't assumed to work (doesn't on x86). */
+   Note a shift by BITS_PER_MP_LIMB isn't assumed to work (doesn't on x86). */
 mp_limb_t
 rshift_make (mp_limb_t hi, mp_limb_t lo, unsigned shift)
 {
@@ -1032,7 +981,7 @@
 }
 
 /* Left shift hi,lo and return the high limb of the result.
-   Note a shift by GMP_LIMB_BITS isn't assumed to work (doesn't on x86). */
+   Note a shift by BITS_PER_MP_LIMB isn't assumed to work (doesn't on x86). */
 mp_limb_t
 lshift_make (mp_limb_t hi, mp_limb_t lo, unsigned shift)
 {
@@ -1315,14 +1264,14 @@
   tp = refmpn_malloc_limbs (nn + qxn);
   refmpn_zero (tp, qxn);
   refmpn_copyi (tp + qxn, np, nn);
-  qh = refmpn_sb_div_qr (qp, tp, nn + qxn, dp, 2);
+  qh = refmpn_sb_divrem_mn (qp, tp, nn + qxn, dp, 2);
   refmpn_copyi (np, tp, 2);
   free (tp);
   return qh;
 }
 
 /* Inverse is floor((b*(b-d)-1) / d), per division by invariant integers
-   paper, figure 8.1 m', where b=2^GMP_LIMB_BITS.  Note that -d-1 < d
+   paper, figure 8.1 m', where b=2^BITS_PER_MP_LIMB.  Note that -d-1 < d
    since d has the high bit set. */
 
 mp_limb_t
@@ -1333,48 +1282,6 @@
   return refmpn_udiv_qrnnd (&r, -d-1, MP_LIMB_T_MAX, d);
 }
 
-void
-refmpn_invert (mp_ptr rp, mp_srcptr up, mp_size_t n, mp_ptr scratch)
-{
-  mp_ptr qp, tp;
-  TMP_DECL;
-  TMP_MARK;
-
-  tp = TMP_ALLOC_LIMBS (2 * n);
-  qp = TMP_ALLOC_LIMBS (n + 1);
-
-  MPN_ZERO (tp, 2 * n);  mpn_sub_1 (tp, tp, 2 * n, 1);
-
-  refmpn_tdiv_qr (qp, rp, 0, tp, 2 * n, up, n);
-  refmpn_copyi (rp, qp, n);
-
-  TMP_FREE;
-}
-
-void
-refmpn_binvert (mp_ptr rp, mp_srcptr up, mp_size_t n, mp_ptr scratch)
-{
-  mp_ptr tp;
-  mp_limb_t binv;
-  TMP_DECL;
-  TMP_MARK;
-
-  /* We use the library mpn_sbpi1_bdiv_q here, which isn't kosher in testing
-     code.  To make up for it, we check that the inverse is correct using a
-     multiply.  */
-
-  tp = TMP_ALLOC_LIMBS (2 * n);
-
-  MPN_ZERO (tp, n);
-  tp[0] = 1;
-  binvert_limb (binv, up[0]);
-  mpn_sbpi1_bdiv_q (rp, tp, n, up, n, -binv);
-
-  refmpn_mul_n (tp, rp, up, n);
-  ASSERT_ALWAYS (tp[0] == 1 && mpn_zero_p (tp + 1, n - 1));
-
-  TMP_FREE;
-}
 
 /* The aim is to produce a dst quotient and return a remainder c, satisfying
    c*b^n + src-i == 3*dst, where i is the incoming carry.
@@ -1450,91 +1357,16 @@
     prodp[usize+i] = refmpn_addmul_1 (prodp+i, up, usize, vp[i]);
 }
 
-#define TOOM3_THRESHOLD (MAX (MUL_TOOM33_THRESHOLD, SQR_TOOM3_THRESHOLD))
-#define TOOM4_THRESHOLD (MAX (MUL_TOOM44_THRESHOLD, SQR_TOOM4_THRESHOLD))
-#if WANT_FFT
-#define FFT_THRESHOLD (MAX (MUL_FFT_THRESHOLD, SQR_FFT_THRESHOLD))
-#else
-#define FFT_THRESHOLD MP_SIZE_T_MAX /* don't use toom44 here */
-#endif
-
-void
-refmpn_mul (mp_ptr wp, mp_srcptr up, mp_size_t un, mp_srcptr vp, mp_size_t vn)
-{
-  mp_ptr tp;
-  mp_size_t tn;
-  mp_limb_t cy;
-
-  if (vn < TOOM3_THRESHOLD)
-    {
-      /* In the mpn_mul_basecase and mpn_kara_mul_n range, use our own
-	 mul_basecase.  */
-      if (vn != 0)
-	refmpn_mul_basecase (wp, up, un, vp, vn);
-      else
-	MPN_ZERO (wp, un);
-      return;
-    }
-
-  if (vn < TOOM4_THRESHOLD)
-    {
-      /* In the mpn_toom33_mul range, use mpn_toom22_mul.  */
-      tn = 2 * vn + mpn_toom22_mul_itch (vn, vn);
-      tp = refmpn_malloc_limbs (tn);
-      mpn_toom22_mul (tp, up, vn, vp, vn, tp + 2 * vn);
-    }
-  else if (vn < FFT_THRESHOLD)
-    {
-      /* In the mpn_toom44_mul range, use mpn_toom33_mul.  */
-      tn = 2 * vn + mpn_toom33_mul_itch (vn, vn);
-      tp = refmpn_malloc_limbs (tn);
-      mpn_toom33_mul (tp, up, vn, vp, vn, tp + 2 * vn);
-    }
-  else
-    {
-      /* Finally, for the largest operands, use mpn_toom44_mul.  */
-      tn = 2 * vn + mpn_toom44_mul_itch (vn, vn);
-      tp = refmpn_malloc_limbs (tn);
-      mpn_toom44_mul (tp, up, vn, vp, vn, tp + 2 * vn);
-    }
-
-  if (un != vn)
-    {
-      if (un - vn < vn)
-	refmpn_mul (wp + vn, vp, vn, up + vn, un - vn);
-      else
-	refmpn_mul (wp + vn, up + vn, un - vn, vp, vn);
-
-      MPN_COPY (wp, tp, vn);
-      cy = refmpn_add (wp + vn, wp + vn, un, tp + vn, vn);
-    }
-  else
-    {
-      MPN_COPY (wp, tp, 2 * vn);
-    }
-
-  free (tp);
-}
-
 void
 refmpn_mul_n (mp_ptr prodp, mp_srcptr up, mp_srcptr vp, mp_size_t size)
 {
-  refmpn_mul (prodp, up, size, vp, size);
-}
-
-void
-refmpn_mullo_n (mp_ptr prodp, mp_srcptr up, mp_srcptr vp, mp_size_t size)
-{
-  mp_ptr tp = refmpn_malloc_limbs (2*size);
-  refmpn_mul (tp, up, size, vp, size);
-  refmpn_copyi (prodp, tp, size);
-  free (tp);
+  refmpn_mul_basecase (prodp, up, size, vp, size);
 }
 
 void
 refmpn_sqr (mp_ptr dst, mp_srcptr src, mp_size_t size)
 {
-  refmpn_mul (dst, src, size, src, size);
+  refmpn_mul_basecase (dst, src, size, src, size);
 }
 
 /* Allowing usize<vsize, usize==0 or vsize==0. */
@@ -1563,9 +1395,9 @@
     }
 
   if (usize >= vsize)
-    refmpn_mul (prodp, up, usize, vp, vsize);
+    refmpn_mul_basecase (prodp, up, usize, vp, vsize);
   else
-    refmpn_mul (prodp, vp, vsize, up, usize);
+    refmpn_mul_basecase (prodp, vp, vsize, up, usize);
 }
 
 
@@ -1797,12 +1629,12 @@
 
 
 
-/* Similar to the old mpn/generic/sb_divrem_mn.c, but somewhat simplified, in
+/* Similar to mpn/generic/sb_divrem_mn.c, but somewhat simplified, in
    particular the trial quotient is allowed to be 2 too big. */
 mp_limb_t
-refmpn_sb_div_qr (mp_ptr qp,
-		  mp_ptr np, mp_size_t nsize,
-		  mp_srcptr dp, mp_size_t dsize)
+refmpn_sb_divrem_mn (mp_ptr qp,
+		     mp_ptr np, mp_size_t nsize,
+		     mp_srcptr dp, mp_size_t dsize)
 {
   mp_limb_t  retval = 0;
   mp_size_t  i;
@@ -1875,7 +1707,7 @@
   return retval;
 }
 
-/* Similar to the old mpn/generic/sb_divrem_mn.c, but somewhat simplified, in
+/* Similar to mpn/generic/sb_divrem_mn.c, but somewhat simplified, in
    particular the trial quotient is allowed to be 2 too big. */
 void
 refmpn_tdiv_qr (mp_ptr qp, mp_ptr rp, mp_size_t qxn,
@@ -1902,7 +1734,7 @@
       n2p[nsize] = refmpn_lshift_or_copy (n2p, np, nsize, norm);
       ASSERT_NOCARRY (refmpn_lshift_or_copy (d2p, dp, dsize, norm));
 
-      refmpn_sb_div_qr (qp, n2p, nsize+1, d2p, dsize);
+      refmpn_sb_divrem_mn (qp, n2p, nsize+1, d2p, dsize);
       refmpn_rshift_or_copy (rp, n2p, dsize, norm);
 
       /* ASSERT (refmpn_zero_p (tp+dsize, nsize-dsize)); */
@@ -1940,7 +1772,7 @@
 
   ASSERT (size >= 0);
   ASSERT (base >= 2);
-  ASSERT (base < numberof (mp_bases));
+  ASSERT (base < numberof (__mp_bases));
   ASSERT (size == 0 || src[size-1] != 0);
   ASSERT_MPN (src, size);
 
@@ -2006,10 +1838,10 @@
    can probably be removed when those normal routines are reliable, though
    perhaps something independent would still be useful at times.  */
 
-#if GMP_LIMB_BITS == 32
+#if BITS_PER_MP_LIMB == 32
 #define RAND_A  CNST_LIMB(0x29CF535)
 #endif
-#if GMP_LIMB_BITS == 64
+#if BITS_PER_MP_LIMB == 64
 #define RAND_A  CNST_LIMB(0xBAECD515DAF0B49D)
 #endif
 
@@ -2019,13 +1851,13 @@
 refmpn_random_half (void)
 {
   refmpn_random_seed = refmpn_random_seed * RAND_A + 1;
-  return (refmpn_random_seed >> GMP_LIMB_BITS/2);
+  return (refmpn_random_seed >> BITS_PER_MP_LIMB/2);
 }
 
 mp_limb_t
 refmpn_random_limb (void)
 {
-  return ((refmpn_random_half () << (GMP_LIMB_BITS/2))
+  return ((refmpn_random_half () << (BITS_PER_MP_LIMB/2))
 	   | refmpn_random_half ()) & GMP_NUMB_MASK;
 }
 
--- 1/tests/t-constants.c
+++ 2/tests/t-constants.c
@@ -209,9 +209,10 @@
   int  error = 0;
 
   CHECK_INT (BYTES_PER_MP_LIMB, (int) sizeof(mp_limb_t));
-  CHECK_INT (mp_bits_per_limb, GMP_LIMB_BITS);
+  CHECK_INT (mp_bits_per_limb, BITS_PER_MP_LIMB);
+  CHECK_INT (__GMP_BITS_PER_MP_LIMB, BITS_PER_MP_LIMB);
 
-  CHECK_BITS (GMP_LIMB_BITS, mp_limb_t);
+  CHECK_BITS (BITS_PER_MP_LIMB, mp_limb_t);
   CHECK_BITS (BITS_PER_ULONG, unsigned long);
 
   CHECK_HIGHBIT (GMP_LIMB_HIGHBIT, mp_limb_t,      LL("0x%lX","0x%llX"));
--- 1/tests/t-count_zeros.c
+++ 2/tests/t-count_zeros.c
@@ -63,9 +63,9 @@
   check_clz (COUNT_LEADING_ZEROS_0, CNST_LIMB(0));
 #endif
 
-  for (i=0; i < GMP_LIMB_BITS; i++)
+  for (i=0; i < BITS_PER_MP_LIMB; i++)
     {
-      check_clz (i, CNST_LIMB(1) << (GMP_LIMB_BITS-1-i));
+      check_clz (i, CNST_LIMB(1) << (BITS_PER_MP_LIMB-1-i));
       check_ctz (i, CNST_LIMB(1) << i);
 
       check_ctz (i, MP_LIMB_T_MAX << i);
--- 1/tests/tests.h
+++ 2/tests/tests.h
@@ -1,7 +1,6 @@
 /* Tests support prototypes etc.
 
-Copyright 2000, 2001, 2002, 2003, 2004, 2008, 2009 Free Software Foundation,
-Inc.
+Copyright 2000, 2001, 2002, 2003, 2004 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -183,10 +182,6 @@
                                  mp_size_t size, mp_limb_t carry));
 mp_limb_t refmpn_addlsh1_n __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_srcptr yp,
                                 mp_size_t size));
-mp_limb_t refmpn_addlsh2_n __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_srcptr yp,
-                                mp_size_t size));
-mp_limb_t refmpn_addlsh_n __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_srcptr yp,
-                                mp_size_t size, unsigned int));
 mp_limb_t refmpn_addmul_1 __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_size_t size,
                                    mp_limb_t multiplier));
 mp_limb_t refmpn_addmul_1c __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_size_t size,
@@ -206,10 +201,10 @@
 mp_limb_t refmpn_addmul_8 __GMP_PROTO ((mp_ptr dst, mp_srcptr src,
                                         mp_size_t size, mp_srcptr mult));
 
-mp_limb_t refmpn_add_n_sub_n __GMP_PROTO ((mp_ptr r1p, mp_ptr r2p,
+mp_limb_t refmpn_addsub_n __GMP_PROTO ((mp_ptr r1p, mp_ptr r2p,
                                    mp_srcptr s1p, mp_srcptr s2p,
                                    mp_size_t size));
-mp_limb_t refmpn_add_n_sub_nc __GMP_PROTO ((mp_ptr r1p, mp_ptr r2p,
+mp_limb_t refmpn_addsub_nc __GMP_PROTO ((mp_ptr r1p, mp_ptr r2p,
                                     mp_srcptr s1p, mp_srcptr s2p,
                                     mp_size_t size, mp_limb_t carry));
 
@@ -309,14 +304,11 @@
 void refmpn_mul_basecase __GMP_PROTO ((mp_ptr prodp,
                                   mp_srcptr up, mp_size_t usize,
                                   mp_srcptr vp, mp_size_t vsize));
-void refmpn_mullo_n __GMP_PROTO ((mp_ptr prodp,
-				  mp_srcptr up, mp_srcptr vp, mp_size_t vsize));
 void refmpn_mul_any __GMP_PROTO ((mp_ptr prodp,
                              mp_srcptr up, mp_size_t usize,
                              mp_srcptr vp, mp_size_t vsize));
 void refmpn_mul_n __GMP_PROTO ((mp_ptr prodp, mp_srcptr up, mp_srcptr vp,
                            mp_size_t size));
-void refmpn_mul __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_srcptr, mp_size_t));
 
 void refmpn_nand_n __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_srcptr yp,
                             mp_size_t size));
@@ -351,9 +343,9 @@
                                                   mp_srcptr xp, mp_size_t size,
                                                   unsigned shift));
 
-mp_limb_t refmpn_sb_div_qr __GMP_PROTO ((mp_ptr,
-					 mp_ptr, mp_size_t,
-					 mp_srcptr, mp_size_t));
+mp_limb_t refmpn_sb_divrem_mn __GMP_PROTO ((mp_ptr qp,
+                                       mp_ptr np, mp_size_t nsize,
+                                       mp_srcptr dp, mp_size_t dsize));
 unsigned long refmpn_scan0 __GMP_PROTO ((mp_srcptr, unsigned long));
 unsigned long refmpn_scan1 __GMP_PROTO ((mp_srcptr, unsigned long));
 void refmpn_setbit __GMP_PROTO ((mp_ptr, unsigned long));
@@ -374,17 +366,11 @@
                                  mp_size_t size, mp_limb_t carry));
 mp_limb_t refmpn_sublsh1_n __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_srcptr yp,
                                 mp_size_t size));
-mp_limb_t refmpn_sublsh_n __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_srcptr yp,
-                                mp_size_t size, unsigned int));
 mp_limb_t refmpn_submul_1 __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_size_t size,
                                    mp_limb_t multiplier));
 mp_limb_t refmpn_submul_1c __GMP_PROTO ((mp_ptr wp, mp_srcptr xp, mp_size_t size,
                                     mp_limb_t multiplier, mp_limb_t carry));
 
-mp_limb_signed_t refmpn_rsblsh1_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-mp_limb_signed_t refmpn_rsblsh2_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t));
-mp_limb_signed_t refmpn_rsblsh_n __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, unsigned int));
-
 void refmpn_tdiv_qr __GMP_PROTO ((mp_ptr qp, mp_ptr rp, mp_size_t qxn,
                              mp_ptr np, mp_size_t nsize,
                              mp_srcptr dp, mp_size_t dsize));
@@ -404,9 +390,6 @@
 void refmpn_zero_extend __GMP_PROTO ((mp_ptr, mp_size_t, mp_size_t));
 int refmpn_zero_p __GMP_PROTO ((mp_srcptr ptr, mp_size_t size));
 
-void refmpn_binvert __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
-void refmpn_invert __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
-
 
 void refmpq_add __GMP_PROTO ((mpq_ptr w, mpq_srcptr x, mpq_srcptr y));
 void refmpq_sub __GMP_PROTO ((mpq_ptr w, mpq_srcptr x, mpq_srcptr y));
--- 1/tune/common.c
+++ 2/tune/common.c
@@ -1,7 +1,7 @@
 /* Shared speed subroutines.
 
-Copyright 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2008, 2009
-Free Software Foundation, Inc.
+Copyright 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006 Free Software
+Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -693,18 +693,6 @@
   SPEED_ROUTINE_MPN_BDIV_DBM1C (mpn_bdiv_dbm1c);
 }
 
-double
-speed_mpn_bdiv_q_1 (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_BDIV_Q_1 (mpn_bdiv_q_1);
-}
-
-double
-speed_mpn_bdiv_q_1_pi1 (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_BDIV_Q_1_PI1 (mpn_bdiv_q_1_pi1);
-}
-
 #if HAVE_NATIVE_mpn_modexact_1_odd
 double
 speed_mpn_modexact_1_odd (struct speed_params *s)
@@ -726,98 +714,52 @@
   SPEED_ROUTINE_MPN_DC_TDIV_QR (mpn_tdiv_qr);
 }
 double
-speed_mpn_dcpi1_div_qr_n (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_DC_DIVREM_N (mpn_dcpi1_div_qr_n);
-}
-
-double
-speed_mpz_mod (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPZ_MOD (mpz_mod);
-}
-
-double
-speed_mpn_sbpi1_div_qr (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_PI1_DIV (mpn_sbpi1_div_qr, inv.inv32);
-}
-double
-speed_mpn_dcpi1_div_qr (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_PI1_DIV (mpn_dcpi1_div_qr, &inv);
-}
-double
-speed_mpn_sbpi1_divappr_q (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_PI1_DIV (mpn_sbpi1_divappr_q, inv.inv32);
-}
-double
-speed_mpn_dcpi1_divappr_q (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_PI1_DIV (mpn_dcpi1_divappr_q, &inv);
-}
-
-double
-speed_mpn_sbpi1_bdiv_qr (struct speed_params *s)
+speed_mpn_dc_divrem_n (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_PI1_BDIV_QR (mpn_sbpi1_bdiv_qr);
+  SPEED_ROUTINE_MPN_DC_DIVREM_N (mpn_dc_divrem_n);
 }
 double
-speed_mpn_dcpi1_bdiv_qr (struct speed_params *s)
+speed_mpn_dc_divrem_sb (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_PI1_BDIV_QR (mpn_dcpi1_bdiv_qr);
+  SPEED_ROUTINE_MPN_DC_DIVREM_SB (mpn_sb_divrem_mn);
 }
 double
-speed_mpn_sbpi1_bdiv_q (struct speed_params *s)
+speed_mpn_dc_divrem_sb_div (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_PI1_BDIV_Q (mpn_sbpi1_bdiv_q);
+  SPEED_ROUTINE_MPN_DC_DIVREM_SB (mpn_sb_divrem_mn_div);
 }
 double
-speed_mpn_dcpi1_bdiv_q (struct speed_params *s)
+speed_mpn_dc_divrem_sb_inv (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_PI1_BDIV_Q (mpn_dcpi1_bdiv_q);
+  SPEED_ROUTINE_MPN_DC_DIVREM_SB (mpn_sb_divrem_mn_inv);
 }
 
 double
-speed_mpn_binvert (struct speed_params *s)
+speed_mpn_sb_divrem_m3 (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_BINVERT (mpn_binvert, mpn_binvert_itch);
+  SPEED_ROUTINE_MPN_SB_DIVREM_M3 (mpn_sb_divrem_mn);
 }
-
 double
-speed_mpn_invert (struct speed_params *s)
+speed_mpn_sb_divrem_m3_div (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_INVERT (mpn_invert, mpn_invert_itch);
+  SPEED_ROUTINE_MPN_SB_DIVREM_M3 (mpn_sb_divrem_mn_div);
 }
-
 double
-speed_mpn_invertappr (struct speed_params *s)
+speed_mpn_sb_divrem_m3_inv (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_INVERTAPPR (mpn_invertappr, mpn_invertappr_itch);
+  SPEED_ROUTINE_MPN_SB_DIVREM_M3 (mpn_sb_divrem_mn_inv);
 }
 
 double
-speed_mpn_ni_invertappr (struct speed_params *s)
+speed_mpz_mod (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_INVERTAPPR (mpn_ni_invertappr, mpn_invertappr_itch);
+  SPEED_ROUTINE_MPZ_MOD (mpz_mod);
 }
-
 double
 speed_mpn_redc_1 (struct speed_params *s)
 {
   SPEED_ROUTINE_REDC_1 (mpn_redc_1);
 }
-double
-speed_mpn_redc_2 (struct speed_params *s)
-{
-  SPEED_ROUTINE_REDC_2 (mpn_redc_2);
-}
-double
-speed_mpn_redc_n (struct speed_params *s)
-{
-  SPEED_ROUTINE_REDC_N (mpn_redc_n);
-}
 
 
 double
@@ -843,11 +785,11 @@
 SPEED_ROUTINE_MPN_BINARY_N (mpn_sub_n);
 }
 
-#if HAVE_NATIVE_mpn_add_n_sub_n
+#if HAVE_NATIVE_mpn_addsub_n
 double
-speed_mpn_add_n_sub_n (struct speed_params *s)
+speed_mpn_addsub_n (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_ADDSUB_N_CALL (mpn_add_n_sub_n (ap, sp, s->xp, s->yp, s->size));
+  SPEED_ROUTINE_MPN_ADDSUB_N_CALL (mpn_addsub_n (ap, sp, s->xp, s->yp, s->size));
 }
 #endif
 
@@ -865,34 +807,6 @@
   SPEED_ROUTINE_MPN_BINARY_N (mpn_sublsh1_n);
 }
 #endif
-#if HAVE_NATIVE_mpn_rsblsh1_n
-double
-speed_mpn_rsblsh1_n (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_BINARY_N (mpn_rsblsh1_n);
-}
-#endif
-#if HAVE_NATIVE_mpn_addlsh2_n
-double
-speed_mpn_addlsh2_n (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_BINARY_N (mpn_addlsh2_n);
-}
-#endif
-#if HAVE_NATIVE_mpn_sublsh2_n
-double
-speed_mpn_sublsh2_n (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_BINARY_N (mpn_sublsh2_n);
-}
-#endif
-#if HAVE_NATIVE_mpn_rsblsh2_n
-double
-speed_mpn_rsblsh2_n (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_BINARY_N (mpn_rsblsh2_n);
-}
-#endif
 #if HAVE_NATIVE_mpn_rsh1add_n
 double
 speed_mpn_rsh1add_n (struct speed_params *s)
@@ -994,45 +908,25 @@
 #endif
 
 double
-speed_mpn_toom2_sqr (struct speed_params *s)
+speed_mpn_kara_mul_n (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_TOOM2_SQR (mpn_toom2_sqr);
+  SPEED_ROUTINE_MPN_KARA_MUL_N (mpn_kara_mul_n);
 }
 double
-speed_mpn_toom3_sqr (struct speed_params *s)
+speed_mpn_kara_sqr_n (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_TOOM3_SQR (mpn_toom3_sqr);
-}
-double
-speed_mpn_toom4_sqr (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_TOOM4_SQR (mpn_toom4_sqr);
-}
-double
-speed_mpn_toom22_mul (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_TOOM22_MUL_N (mpn_toom22_mul);
-}
-double
-speed_mpn_toom33_mul (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_TOOM33_MUL_N (mpn_toom33_mul);
-}
-double
-speed_mpn_toom44_mul (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_TOOM44_MUL_N (mpn_toom44_mul);
+  SPEED_ROUTINE_MPN_KARA_SQR_N (mpn_kara_sqr_n);
 }
 
 double
-speed_mpn_toom32_mul (struct speed_params *s)
+speed_mpn_toom3_mul_n (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_TOOM32_MUL (mpn_toom32_mul);
+  SPEED_ROUTINE_MPN_TOOM3_MUL_N (mpn_toom3_mul_n);
 }
 double
-speed_mpn_toom42_mul (struct speed_params *s)
+speed_mpn_toom3_sqr_n (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_TOOM42_MUL (mpn_toom42_mul);
+  SPEED_ROUTINE_MPN_TOOM3_SQR_N (mpn_toom3_sqr_n);
 }
 
 double
@@ -1105,32 +999,14 @@
 }
 
 double
-speed_mpn_mullo_n (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_MULLO_N (mpn_mullo_n);
-}
-double
-speed_mpn_mullo_basecase (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_MULLO_BASECASE (mpn_mullo_basecase);
-}
-
-double
-speed_mpn_mulmod_bnm1 (struct speed_params *s)
-{
-  SPEED_ROUTINE_MPN_MULMOD_BNM1_CALL (mpn_mulmod_bnm1 (wp, s->size, s->xp, s->size, s->yp, s->size, tp));
-}
-
-double
-speed_mpn_bc_mulmod_bnm1 (struct speed_params *s)
+speed_mpn_mullow_n (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_MULMOD_BNM1_CALL (mpn_bc_mulmod_bnm1 (wp, s->xp, s->yp, s->size, tp));
+  SPEED_ROUTINE_MPN_MULLOW_N (mpn_mullow_n);
 }
-
 double
-speed_mpn_mulmod_bnm1_rounded (struct speed_params *s)
+speed_mpn_mullow_basecase (struct speed_params *s)
 {
-  SPEED_ROUTINE_MPN_MULMOD_BNM1_ROUNDED (mpn_mulmod_bnm1);
+  SPEED_ROUTINE_MPN_MULLOW_BASECASE (mpn_mullow_basecase);
 }
 
 double
@@ -1196,7 +1072,7 @@
   mp_size_t hgcd_scratch = mpn_hgcd_itch (s->size);
   mp_ptr ap;
   mp_ptr bp;
-  mp_ptr tmp1;
+  mp_ptr tmp1, tmp2;
 
   struct hgcd_matrix hgcd;
   int res;
@@ -1241,7 +1117,7 @@
   mp_size_t hgcd_scratch = MPN_HGCD_LEHMER_ITCH (s->size);
   mp_ptr ap;
   mp_ptr bp;
-  mp_ptr tmp1;
+  mp_ptr tmp1, tmp2;
 
   struct hgcd_matrix hgcd;
   int res;
@@ -1283,6 +1159,19 @@
 {
   SPEED_ROUTINE_MPN_GCD (mpn_gcd);
 }
+#if 0
+double
+speed_mpn_gcd_binary (struct speed_params *s)
+{
+  SPEED_ROUTINE_MPN_GCD (mpn_gcd_binary);
+}
+double
+speed_mpn_gcd_accel (struct speed_params *s)
+{
+  SPEED_ROUTINE_MPN_GCD (mpn_gcd_accel);
+}
+#endif
+
 
 double
 speed_mpn_gcdext (struct speed_params *s)
@@ -1584,7 +1473,7 @@
 
 
 /* Compare this to mpn_add_n to see how much overhead mpz_add adds.  Note
-   that repeatedly calling mpz_add with the same data gives branch prediction
+   that repeatedly calling mpz_add with the same data gives branch predition
    in it an advantage.  */
 
 double
@@ -1851,7 +1740,7 @@
     /* divisor from "r" parameter, or a default */      \
     d = s->r;                                           \
     if (d == 0)                                         \
-      d = mp_bases[10].big_base;                        \
+      d = __mp_bases[10].big_base;                      \
                                                         \
     if (normalize)                                      \
       {                                                 \
@@ -2021,7 +1910,7 @@
   /* divisor from "r" parameter, or a default */
   d = s->r;
   if (d == 0)
-    d = mp_bases[10].big_base;
+    d = __mp_bases[10].big_base;
 
   x = s->xp[0];
   q = 0;
@@ -2062,7 +1951,7 @@
   /* divisor from "r" parameter, or a default */
   d = s->r;
   if (d == 0)
-    d = mp_bases[10].big_base;
+    d = __mp_bases[10].big_base;
 
   x = s->xp[0];
   r = 0;
@@ -2096,7 +1985,7 @@
    be typical for count_trailing_zeros in a GCD etc.
 
    r==1 measures on data with the resultant count uniformly distributed
-   between 0 and GMP_LIMB_BITS-1.  This is probably sensible for
+   between 0 and BITS_PER_MP_LIMB-1.  This is probably sensible for
    count_leading_zeros on the high limbs of divisors.  */
 
 int
@@ -2123,7 +2012,7 @@
          rest below.  */
       for (i = 0; i < SPEED_BLOCK_SIZE; i++)
         {
-          mp_limb_t  set = CNST_LIMB(1) << (s->yp_block[i] % GMP_LIMB_BITS);
+          mp_limb_t  set = CNST_LIMB(1) << (s->yp_block[i] % BITS_PER_MP_LIMB);
           mp_limb_t  keep_below = set-1;
           mp_limb_t  keep_above = MP_LIMB_T_MAX ^ keep_below;
           mp_limb_t  keep = (leading ? keep_below : keep_above);
--- 1/tune/Makefile.am
+++ 2/tune/Makefile.am
@@ -41,12 +41,13 @@
 
 libspeed_la_SOURCES =							\
   common.c divrem1div.c divrem1inv.c divrem2div.c divrem2inv.c		\
-  freq.c								\
+  freq.c					\
   gcdext_single.c gcdext_double.c gcdextod.c gcdextos.c			\
   jacbase1.c jacbase2.c jacbase3.c					\
   mod_1_div.c mod_1_inv.c modlinv.c					\
   noop.c powm_mod.c powm_redc.c pre_divrem_1.c				\
-  set_strb.c set_strs.c set_strp.c time.c
+  set_strb.c set_strs.c set_strp.c time.c						\
+  sb_div.c sb_inv.c
 
 libspeed_la_DEPENDENCIES = $(SPEED_CYCLECOUNTER_OBJ) \
   $(top_builddir)/tests/libtests.la $(top_builddir)/libgmp.la
@@ -122,12 +123,9 @@
 # recompiled object will be rebuilt if that file changes.
 
 TUNE_MPN_SRCS = $(TUNE_MPN_SRCS_BASIC) divrem_1.c mod_1.c
-TUNE_MPN_SRCS_BASIC =							\
-  dcpi1_div_qr.c dcpi1_divappr_q.c dcpi1_bdiv_qr.c dcpi1_bdiv_q.c	\
-  invertappr.c invert.c binvert.c divrem_2.c gcd.c gcdext.c		\
-  get_str.c set_str.c matrix22_mul.c hgcd.c mul_n.c sqr_n.c		\
-  mullo_n.c mul_fft.c mul.c tdiv_qr.c mulmod_bnm1.c			\
-  toom22_mul.c toom2_sqr.c toom33_mul.c toom3_sqr.c toom44_mul.c toom4_sqr.c
+TUNE_MPN_SRCS_BASIC = dc_divrem_n.c divrem_2.c gcd.c gcdext.c get_str.c \
+  set_str.c matrix22_mul.c hgcd.c mul_n.c toom44_mul.c toom4_sqr.c	\
+  mullow_n.c mul_fft.c mul.c sb_divrem_mn.c tdiv_qr.c
 
 $(TUNE_MPN_SRCS_BASIC):
 	for i in $(TUNE_MPN_SRCS_BASIC); do \
@@ -146,7 +144,7 @@
 	echo "#include \"mpn/generic/mod_1.c\""     >>mod_1.c
 
 sqr_asm.asm: $(top_builddir)/mpn/sqr_basecase.asm
-	echo 'define(SQR_TOOM2_THRESHOLD_OVERRIDE,SQR_TOOM2_THRESHOLD_MAX)' >sqr_asm.asm
+	echo 'define(SQR_KARATSUBA_THRESHOLD_OVERRIDE,SQR_KARATSUBA_THRESHOLD_MAX)' >sqr_asm.asm
 	echo 'include(../mpn/sqr_basecase.asm)' >>sqr_asm.asm
 
 
@@ -175,6 +173,9 @@
 stg:
 	./speed $(STS) -P stg mpn_kara_sqr_n mpn_toom3_sqr_n
 
+dc:
+	./speed -s 5-40 -c mpn_dc_divrem_sb mpn_dc_divrem_n mpn_dc_tdiv_qr
+
 fib:
 	./speed -s 40-60 -c mpz_fib_ui
 fibg:
--- 1/tune/many.pl
+++ 2/tune/many.pl
@@ -369,7 +369,7 @@
      },
 
      {
-       'regexp'=> 'add_n_sub_n',
+       'regexp'=> 'addsub_n',
        'ret'   => 'mp_limb_t',
        'args'  => 'mp_ptr sum, mp_ptr diff, mp_srcptr xp, mp_srcptr yp, mp_size_t size',
        'speed_flags'=> 'FLAG_R_OPTIONAL',
--- 1/tune/modlinv.c
+++ 2/tune/modlinv.c
@@ -30,7 +30,7 @@
    dependent chain, whereas the "2*" in the standard version isn't.
    Depending on the CPU this should be the same or a touch slower.  */
 
-#if GMP_LIMB_BITS <= 32
+#if BITS_PER_MP_LIMB <= 32
 #define binvert_limb_mul1(inv,n)                                \
   do {                                                          \
     mp_limb_t  __n = (n);                                       \
@@ -44,7 +44,7 @@
   } while (0)
 #endif
 
-#if GMP_LIMB_BITS > 32 && GMP_LIMB_BITS <= 64
+#if BITS_PER_MP_LIMB > 32 && BITS_PER_MP_LIMB <= 64
 #define binvert_limb_mul1(inv,n)                                \
   do {                                                          \
     mp_limb_t  __n = (n);                                       \
@@ -100,7 +100,7 @@
                                                 \
     ASSERT ((__n & 1) == 1);                    \
                                                 \
-    __count = GMP_LIMB_BITS-1;               \
+    __count = BITS_PER_MP_LIMB-1;               \
     do                                          \
       {                                         \
         __inv >>= 1;                            \
@@ -131,11 +131,11 @@
                                                                         \
     ASSERT ((__n & 1) == 1);                                            \
                                                                         \
-    __count = GMP_LIMB_BITS-1;                                       \
+    __count = BITS_PER_MP_LIMB-1;                                       \
     do                                                                  \
       {                                                                 \
         __lowbit = __rem & 1;                                           \
-        __inv = (__inv >> 1) | (__lowbit << (GMP_LIMB_BITS-1));      \
+        __inv = (__inv >> 1) | (__lowbit << (BITS_PER_MP_LIMB-1));      \
         __rem = (__rem - (__n & -__lowbit)) >> 1;                       \
       }                                                                 \
     while (-- __count);                                                 \
--- 1/tune/README
+++ 2/tune/README
@@ -85,7 +85,7 @@
 PARAMETER TUNING
 
 The "tuneup" program runs some tests designed to find the best settings for
-various thresholds, like MUL_TOOM22_THRESHOLD.  Its output can be put
+various thresholds, like MUL_KARATSUBA_THRESHOLD.  Its output can be put
 into gmp-mparam.h.  The program is built and run with
 
         make tune
@@ -281,7 +281,7 @@
 products on the diagonal mean it falls short of this.  Here's an example
 running the two and showing by what factor an NxN mul_basecase is slower
 than an NxN sqr_basecase.  (Some versions of sqr_basecase only allow sizes
-below SQR_TOOM2_THRESHOLD, so if it crashes at that point don't worry.)
+below SQR_KARATSUBA_THRESHOLD, so if it crashes at that point don't worry.)
 
         ./speed -s 1-20 -r mpn_sqr_basecase mpn_mul_basecase
 
@@ -427,12 +427,12 @@
 Note further that the various routines may recurse into themselves on sizes
 far enough above applicable thresholds.  For example, mpn_kara_mul_n will
 recurse into itself on sizes greater than twice the compiled-in
-MUL_TOOM22_THRESHOLD.
+MUL_KARATSUBA_THRESHOLD.
 
 When doing the above comparison between mul_basecase and kara_mul_n what's
 probably of interest is mul_basecase versus a kara_mul_n that does one level
 of Karatsuba then calls to mul_basecase, but this only happens on sizes less
-than twice the compiled MUL_TOOM22_THRESHOLD.  A larger value for that
+than twice the compiled MUL_KARATSUBA_THRESHOLD.  A larger value for that
 setting can be compiled-in to avoid the problem if necessary.  The same
 applies to toom3 and DC, though in a trickier fashion.
 
--- 1/tune/speed.c
+++ 2/tune/speed.c
@@ -1,7 +1,6 @@
 /* Speed measuring program.
 
-Copyright 1999, 2000, 2001, 2002, 2003, 2005, 2006, 2008, 2009 Free Software
-Foundation, Inc.
+Copyright 1999, 2000, 2001, 2002, 2003, 2005, 2006 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -90,10 +89,10 @@
   } while (0)
 
 
-#if GMP_LIMB_BITS == 32
+#if BITS_PER_MP_LIMB == 32
 #define GMP_NUMB_0xAA  (CNST_LIMB(0xAAAAAAAA) & GMP_NUMB_MASK)
 #endif
-#if GMP_LIMB_BITS == 64
+#if BITS_PER_MP_LIMB == 64
 #define GMP_NUMB_0xAA  (CNST_LIMB(0xAAAAAAAAAAAAAAAA) & GMP_NUMB_MASK)
 #endif
 
@@ -153,8 +152,8 @@
   { "mpn_add_n",         speed_mpn_add_n,     FLAG_R_OPTIONAL },
   { "mpn_sub_n",         speed_mpn_sub_n,     FLAG_R_OPTIONAL },
 
-#if HAVE_NATIVE_mpn_add_n_sub_n
-  { "mpn_add_n_sub_n",      speed_mpn_add_n_sub_n,     FLAG_R_OPTIONAL },
+#if HAVE_NATIVE_mpn_addsub_n
+  { "mpn_addsub_n",      speed_mpn_addsub_n,     FLAG_R_OPTIONAL },
 #endif
 
   { "mpn_addmul_1",      speed_mpn_addmul_1,  FLAG_R },
@@ -220,8 +219,6 @@
   { "mpn_divexact_1",    speed_mpn_divexact_1,    FLAG_R },
   { "mpn_divexact_by3",  speed_mpn_divexact_by3          },
 
-  { "mpn_bdiv_q_1",      speed_mpn_bdiv_q_1,      FLAG_R_OPTIONAL },
-  { "mpn_bdiv_q_1_pi1",  speed_mpn_bdiv_q_1_pi1,  FLAG_R_OPTIONAL },
   { "mpn_bdiv_dbm1c",    speed_mpn_bdiv_dbm1c,    FLAG_R_OPTIONAL },
 
 #if HAVE_NATIVE_mpn_modexact_1_odd
@@ -234,7 +231,14 @@
 #endif
 
   { "mpn_dc_tdiv_qr",       speed_mpn_dc_tdiv_qr       },
-  { "mpn_dc_div_qr_n",      speed_mpn_dcpi1_div_qr_n   },
+  { "mpn_dc_divrem_n",      speed_mpn_dc_divrem_n      },
+  { "mpn_dc_divrem_sb",     speed_mpn_dc_divrem_sb     },
+  { "mpn_dc_divrem_sb_div", speed_mpn_dc_divrem_sb_div },
+  { "mpn_dc_divrem_sb_inv", speed_mpn_dc_divrem_sb_inv },
+
+  { "mpn_sb_divrem_m3",     speed_mpn_sb_divrem_m3     },
+  { "mpn_sb_divrem_m3_div", speed_mpn_sb_divrem_m3_div },
+  { "mpn_sb_divrem_m3_inv", speed_mpn_sb_divrem_m3_inv },
 
   { "mpn_lshift",        speed_mpn_lshift, FLAG_R   },
   { "mpn_rshift",        speed_mpn_rshift, FLAG_R   },
@@ -289,47 +293,25 @@
 #endif
 
   { "mpn_mul_n",         speed_mpn_mul_n            },
-  { "mpn_sqr_n",         speed_mpn_sqr_n            },
+  { "mpn_sqr",           speed_mpn_sqr_n            },
 
-  { "mpn_toom2_sqr",     speed_mpn_toom2_sqr        },
-  { "mpn_toom3_sqr",     speed_mpn_toom3_sqr        },
-  { "mpn_toom4_sqr",     speed_mpn_toom4_sqr        },
-  { "mpn_toom22_mul",    speed_mpn_toom22_mul       },
-  { "mpn_toom33_mul",    speed_mpn_toom33_mul       },
-  { "mpn_toom44_mul",    speed_mpn_toom44_mul       },
-  { "mpn_toom32_mul",    speed_mpn_toom32_mul       },
-  { "mpn_toom42_mul",    speed_mpn_toom42_mul       },
+  { "mpn_kara_mul_n",    speed_mpn_kara_mul_n       },
+  { "mpn_kara_sqr_n",    speed_mpn_kara_sqr_n       },
+  { "mpn_toom3_mul_n",   speed_mpn_toom3_mul_n      },
+  { "mpn_toom3_sqr_n",   speed_mpn_toom3_sqr_n      },
   { "mpn_mul_fft_full",      speed_mpn_mul_fft_full      },
   { "mpn_mul_fft_full_sqr",  speed_mpn_mul_fft_full_sqr  },
 
   { "mpn_mul_fft",       speed_mpn_mul_fft,     FLAG_R_OPTIONAL },
   { "mpn_mul_fft_sqr",   speed_mpn_mul_fft_sqr, FLAG_R_OPTIONAL },
 
-  { "mpn_mullo_n",        speed_mpn_mullo_n         },
-  { "mpn_mullo_basecase", speed_mpn_mullo_basecase  },
+  { "mpn_mullow_n",      speed_mpn_mullow_n         },
+  { "mpn_mullow_basecase", speed_mpn_mullow_basecase},
 
-  { "mpn_bc_mulmod_bnm1",      speed_mpn_bc_mulmod_bnm1      },
-  { "mpn_mulmod_bnm1",         speed_mpn_mulmod_bnm1         },
-  { "mpn_mulmod_bnm1_rounded", speed_mpn_mulmod_bnm1_rounded },
-
-  { "mpn_invert",              speed_mpn_invert              },
-  { "mpn_invertappr",          speed_mpn_invertappr          },
-  { "mpn_ni_invertappr",       speed_mpn_ni_invertappr       },
-  { "mpn_binvert",             speed_mpn_binvert             },
-
-  { "mpn_sbpi1_div_qr",        speed_mpn_sbpi1_div_qr        },
-  { "mpn_dcpi1_div_qr",        speed_mpn_dcpi1_div_qr        },
-  { "mpn_sbpi1_divappr_q",     speed_mpn_sbpi1_divappr_q     },
-  { "mpn_dcpi1_divappr_q",     speed_mpn_dcpi1_divappr_q     },
-
-  { "mpn_sbpi1_bdiv_qr",       speed_mpn_sbpi1_bdiv_qr       },
-  { "mpn_dcpi1_bdiv_qr",       speed_mpn_dcpi1_bdiv_qr       },
-  { "mpn_sbpi1_bdiv_q",        speed_mpn_sbpi1_bdiv_q        },
-  { "mpn_dcpi1_bdiv_q",        speed_mpn_dcpi1_bdiv_q        },
-
-  { "mpn_get_str",          speed_mpn_get_str,     FLAG_R_OPTIONAL },
-  { "mpn_set_str",          speed_mpn_set_str,     FLAG_R_OPTIONAL },
-  { "mpn_set_str_basecase", speed_mpn_bc_set_str,  FLAG_R_OPTIONAL },
+  { "mpn_get_str",       speed_mpn_get_str,  FLAG_R_OPTIONAL },
+
+  { "mpn_set_str",         speed_mpn_set_str,     FLAG_R_OPTIONAL },
+  { "mpn_set_str_basecase",speed_mpn_bc_set_str,  FLAG_R_OPTIONAL },
 
   { "mpn_sqrtrem",       speed_mpn_sqrtrem          },
   { "mpn_rootrem",       speed_mpn_rootrem, FLAG_R  },
@@ -350,8 +332,6 @@
 
   { "mpz_mod",           speed_mpz_mod              },
   { "mpn_redc_1",        speed_mpn_redc_1           },
-  { "mpn_redc_2",        speed_mpn_redc_2           },
-  { "mpn_redc_n",        speed_mpn_redc_n           },
 
   { "MPN_COPY",          speed_MPN_COPY             },
   { "MPN_COPY_INCR",     speed_MPN_COPY_INCR        },
@@ -369,18 +349,6 @@
 #if HAVE_NATIVE_mpn_sublsh1_n
   { "mpn_sublsh1_n",     speed_mpn_sublsh1_n        },
 #endif
-#if HAVE_NATIVE_mpn_rsblsh1_n
-  { "mpn_rsblsh1_n",     speed_mpn_rsblsh1_n        },
-#endif
-#if HAVE_NATIVE_mpn_addlsh2_n
-  { "mpn_addlsh2_n",     speed_mpn_addlsh2_n        },
-#endif
-#if HAVE_NATIVE_mpn_sublsh2_n
-  { "mpn_sublsh2_n",     speed_mpn_sublsh2_n        },
-#endif
-#if HAVE_NATIVE_mpn_rsblsh2_n
-  { "mpn_rsblsh2_n",     speed_mpn_rsblsh2_n        },
-#endif
 #if HAVE_NATIVE_mpn_rsh1add_n
   { "mpn_rsh1add_n",     speed_mpn_rsh1add_n        },
 #endif
@@ -811,7 +779,7 @@
 /* Return a limb with n many one bits (starting from the least significant) */
 
 #define LIMB_ONES(n) \
-  ((n) == GMP_LIMB_BITS ? MP_LIMB_T_MAX      \
+  ((n) == BITS_PER_MP_LIMB ? MP_LIMB_T_MAX      \
     : (n) == 0 ? CNST_LIMB(0)                   \
     : (CNST_LIMB(1) << (n)) - 1)
 
@@ -838,7 +806,7 @@
       {
         if (siz > 1 || siz < -1)
           printf ("Warning, r parameter %s truncated to %d bits\n",
-                  s_orig, GMP_LIMB_BITS);
+                  s_orig, BITS_PER_MP_LIMB);
         return l;
       }
   }
@@ -851,10 +819,10 @@
   if (strcmp (s, "bits") == 0)
     {
       mp_limb_t  l;
-      if (n > GMP_LIMB_BITS)
+      if (n > BITS_PER_MP_LIMB)
         {
           fprintf (stderr, "%ld bit parameter invalid (max %d bits)\n",
-                   n, GMP_LIMB_BITS);
+                   n, BITS_PER_MP_LIMB);
           exit (1);
         }
       mpn_random (&l, 1);
@@ -862,10 +830,10 @@
     }
   else  if (strcmp (s, "ones") == 0)
     {
-      if (n > GMP_LIMB_BITS)
+      if (n > BITS_PER_MP_LIMB)
         {
           fprintf (stderr, "%ld bit parameter invalid (max %d bits)\n",
-                   n, GMP_LIMB_BITS);
+                   n, BITS_PER_MP_LIMB);
           exit (1);
         }
       return LIMB_ONES (n);
--- 1/tune/speed-ext.c
+++ 2/tune/speed-ext.c
@@ -82,8 +82,8 @@
   ASSERT (size >= 1);
 
   c = mpn_add_n (wp, xp, yp, size);
-  ret = mpn_rshift (wp, wp, size, 1) >> (GMP_LIMB_BITS-1);
-  wp[size-1] |= (c << (GMP_LIMB_BITS-1));
+  ret = mpn_rshift (wp, wp, size, 1) >> (BITS_PER_MP_LIMB-1);
+  wp[size-1] |= (c << (BITS_PER_MP_LIMB-1));
   return ret;
 }
 
@@ -107,7 +107,7 @@
   c = (wprev < x);
   ret = (wprev & 1);
 
-#define RSHIFT(hi,lo)   (((lo) >> 1) | ((hi) << (GMP_LIMB_BITS-1)))
+#define RSHIFT(hi,lo)   (((lo) >> 1) | ((hi) << (BITS_PER_MP_LIMB-1)))
 
   for (i = 1; i < size; i++)
     {
--- 1/tune/speed.h
+++ 2/tune/speed.h
@@ -1,7 +1,7 @@
 /* Header for speed and threshold things.
 
-Copyright 1999, 2000, 2001, 2002, 2003, 2005, 2006, 2008, 2009 Free Software
-Foundation, Inc.
+Copyright 1999, 2000, 2001, 2002, 2003, 2005, 2006 Free Software Foundation,
+Inc.
 
 This file is part of the GNU MP Library.
 
@@ -31,9 +31,9 @@
   } while (0)
 
 /* A mask of the least significant n bits.  Note 1<<32 doesn't give zero on
-   x86 family CPUs, hence the separate case for GMP_LIMB_BITS. */
+   x86 family CPUs, hence the separate case for BITS_PER_MP_LIMB. */
 #define MP_LIMB_T_LOWBITMASK(n)	\
-  ((n) == GMP_LIMB_BITS ? MP_LIMB_T_MAX : ((mp_limb_t) 1 << (n)) - 1)
+  ((n) == BITS_PER_MP_LIMB ? MP_LIMB_T_MAX : ((mp_limb_t) 1 << (n)) - 1)
 
 
 /* align must be a power of 2 here, usually CACHE_LINE_SIZE is a good choice */
@@ -146,8 +146,7 @@
 
 double speed_mpn_add_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_addlsh1_n __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_addlsh2_n __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_add_n_sub_n __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_addsub_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_and_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_andn_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_addmul_1 __GMP_PROTO ((struct speed_params *s));
@@ -161,15 +160,16 @@
 double speed_mpn_com_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_copyd __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_copyi __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_dcpi1_div_qr_n __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_dc_divrem_n __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_dc_divrem_sb __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_dc_divrem_sb_div __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_dc_divrem_sb_inv __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_dc_tdiv_qr __GMP_PROTO ((struct speed_params *s));
 double speed_MPN_COPY __GMP_PROTO ((struct speed_params *s));
 double speed_MPN_COPY_DECR __GMP_PROTO ((struct speed_params *s));
 double speed_MPN_COPY_INCR __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_divexact_1 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_divexact_by3 __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_bdiv_q_1 __GMP_PROTO ((struct speed_params *));
-double speed_mpn_bdiv_q_1_pi1 __GMP_PROTO ((struct speed_params *));
 double speed_mpn_bdiv_dbm1c __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_divrem_1 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_divrem_1f __GMP_PROTO ((struct speed_params *s));
@@ -189,6 +189,8 @@
 double speed_mpn_gcd __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_gcd_1 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_gcd_1N __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_gcd_binary __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_gcd_accel __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_gcdext __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_gcdext_double __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_gcdext_one_double __GMP_PROTO ((struct speed_params *s));
@@ -202,13 +204,13 @@
 double speed_mpn_jacobi_base_1 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_jacobi_base_2 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_jacobi_base_3 __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_kara_mul_n __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_kara_sqr_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_lshift __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_mod_1 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_mod_1c __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_mod_1_div __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_mod_1_inv __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_mod_1_unnorm __GMP_PROTO ((struct speed_params *));
-double speed_mpn_mod_1_norm __GMP_PROTO ((struct speed_params *));
 double speed_mpn_mod_34lsub1 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_modexact_1_odd __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_modexact_1c_odd __GMP_PROTO ((struct speed_params *s));
@@ -225,31 +227,15 @@
 double speed_mpn_mul_fft_full_sqr __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_mul_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_mul_n_sqr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_mullo_n __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_mullo_basecase __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_mullow_n __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_mullow_basecase __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_nand_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_nior_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_popcount __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_preinv_divrem_1 __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_preinv_divrem_1f __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_preinv_mod_1 __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_sbpi1_div_qr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_dcpi1_div_qr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_sbpi1_divappr_q __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_dcpi1_divappr_q __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_sbpi1_bdiv_qr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_dcpi1_bdiv_qr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_sbpi1_bdiv_q __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_dcpi1_bdiv_q __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_invert __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_invertappr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_ni_invertappr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_binvert __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_redc_1 __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_redc_2 __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_redc_n __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_rsblsh1_n __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_rsblsh2_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_rsh1add_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_rsh1sub_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_rshift __GMP_PROTO ((struct speed_params *s));
@@ -267,19 +253,9 @@
 double speed_mpn_rootrem __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_sub_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_sublsh1_n __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_sublsh2_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_submul_1 __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom2_sqr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom3_sqr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom4_sqr __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom22_mul __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom33_mul __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom44_mul __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom32_mul __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_toom42_mul __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_mulmod_bnm1 __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_bc_mulmod_bnm1 __GMP_PROTO ((struct speed_params *s));
-double speed_mpn_mulmod_bnm1_rounded __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_toom3_mul_n __GMP_PROTO ((struct speed_params *s));
+double speed_mpn_toom3_sqr_n __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_udiv_qrnnd __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_udiv_qrnnd_r __GMP_PROTO ((struct speed_params *s));
 double speed_mpn_umul_ppmm __GMP_PROTO ((struct speed_params *s));
@@ -420,7 +396,6 @@
 mp_limb_t mpn_sb_divrem_mn_inv __GMP_PROTO ((mp_ptr, mp_ptr, mp_size_t, mp_srcptr, mp_size_t));
 
 mp_size_t mpn_set_str_basecase __GMP_PROTO ((mp_ptr, const unsigned char *, size_t, int));
-void mpn_pre_set_str __GMP_PROTO ((mp_ptr, unsigned char *, size_t, powers_t *, mp_ptr));
 
 void mpn_toom3_mul_n_open __GMP_PROTO ((mp_ptr, mp_srcptr, mp_srcptr, mp_size_t, mp_ptr));
 void mpn_toom3_sqr_n_open __GMP_PROTO ((mp_ptr, mp_srcptr, mp_size_t, mp_ptr));
@@ -759,26 +734,6 @@
 #define SPEED_ROUTINE_MPN_DIVEXACT_1(function)				\
   SPEED_ROUTINE_MPN_UNARY_1_CALL ((*function) (wp, s->xp, s->size, s->r))
 
-#define SPEED_ROUTINE_MPN_BDIV_Q_1(function)				\
-    SPEED_ROUTINE_MPN_UNARY_1_CALL ((*function) (wp, s->xp, s->size, s->r))
-
-#define SPEED_ROUTINE_MPN_BDIV_Q_1_PI1_CALL(call)			\
-  {									\
-    unsigned   shift;							\
-    mp_limb_t  dinv;							\
-									\
-    SPEED_RESTRICT_COND (s->size > 0);					\
-    SPEED_RESTRICT_COND (s->r != 0);					\
-									\
-    count_trailing_zeros (shift, s->r);					\
-    binvert_limb (dinv, s->r >> shift);					\
-									\
-    SPEED_ROUTINE_MPN_UNARY_1_CALL (call);				\
-  }
-#define SPEED_ROUTINE_MPN_BDIV_Q_1_PI1(function)			\
-  SPEED_ROUTINE_MPN_BDIV_Q_1_PI1_CALL					\
-  ((*function) (wp, s->xp, s->size, s->r, dinv, shift))
-
 #define SPEED_ROUTINE_MPN_BDIV_DBM1C(function)				\
   SPEED_ROUTINE_MPN_UNARY_1_CALL ((*function) (wp, s->xp, s->size, s->r, 0))
 
@@ -937,7 +892,7 @@
 #define SPEED_ROUTINE_MPN_MUL_N(function)				\
   SPEED_ROUTINE_MPN_MUL_N_CALL (function (wp, s->xp, s->yp, s->size));
 
-#define SPEED_ROUTINE_MPN_MULLO_N_CALL(call)				\
+#define SPEED_ROUTINE_MPN_MULLOW_N_CALL(call)				\
   {									\
     mp_ptr    wp;							\
     unsigned  i;							\
@@ -965,11 +920,11 @@
     return t;								\
   }
 
-#define SPEED_ROUTINE_MPN_MULLO_N(function)				\
-  SPEED_ROUTINE_MPN_MULLO_N_CALL (function (wp, s->xp, s->yp, s->size));
+#define SPEED_ROUTINE_MPN_MULLOW_N(function)				\
+  SPEED_ROUTINE_MPN_MULLOW_N_CALL (function (wp, s->xp, s->yp, s->size));
 
 /* For mpn_mul_basecase, xsize=r, ysize=s->size. */
-#define SPEED_ROUTINE_MPN_MULLO_BASECASE(function)			\
+#define SPEED_ROUTINE_MPN_MULLOW_BASECASE(function)			\
   {									\
     mp_ptr    wp;							\
     unsigned  i;							\
@@ -997,71 +952,6 @@
     return t;								\
   }
 
-#define SPEED_ROUTINE_MPN_MULMOD_BNM1_CALL(call)			\
-  {									\
-    mp_ptr    wp, tp;							\
-    unsigned  i;							\
-    double    t;							\
-    mp_size_t itch;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    itch = mpn_mulmod_bnm1_itch (s->size);				\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (wp, 2 * s->size, s->align_wp);		\
-    SPEED_TMP_ALLOC_LIMBS (tp, itch, s->align_wp2);			\
-									\
-    speed_operand_src (s, s->xp, s->size);				\
-    speed_operand_src (s, s->yp, s->size);				\
-    speed_operand_dst (s, wp, 2 * s->size);				\
-    speed_operand_dst (s, tp, itch);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do									\
-      call;								\
-    while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-#define SPEED_ROUTINE_MPN_MULMOD_BNM1_ROUNDED(function)			\
-  {									\
-    mp_ptr    wp, tp;							\
-    unsigned  i;							\
-    double    t;							\
-    mp_size_t size;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    size = mpn_mulmod_bnm1_next_size (s->size);				\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (wp, 2*size, s->align_wp); /* FIXME 2* */	\
-    SPEED_TMP_ALLOC_LIMBS (tp, 3 * size + 100, s->align_wp2);		\
-									\
-    speed_operand_src (s, s->xp, s->size);				\
-    speed_operand_src (s, s->yp, s->size);				\
-    speed_operand_dst (s, wp, 2 * size);				\
-    speed_operand_dst (s, tp, 3 * size + 100); /* FIXME: Use itch function */ \
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do									\
-      function (wp, size, s->xp, s->size, s->yp, s->size, tp);	\
-    while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-
 #define SPEED_ROUTINE_MPN_MUL_N_TSPACE(call, tsize, minsize)		\
   {									\
     mp_ptr    wp, tspace;						\
@@ -1092,35 +982,18 @@
     return t;								\
   }
 
-#define SPEED_ROUTINE_MPN_TOOM22_MUL_N(function)			\
-  SPEED_ROUTINE_MPN_MUL_N_TSPACE					\
-    (function (wp, s->xp, s->size, s->yp, s->size, tspace),		\
-     mpn_toom22_mul_itch (s->size, s->size),				\
-     MPN_TOOM22_MUL_MINSIZE)
-
-#define SPEED_ROUTINE_MPN_TOOM33_MUL_N(function)			\
+#define SPEED_ROUTINE_MPN_KARA_MUL_N(function)				\
   SPEED_ROUTINE_MPN_MUL_N_TSPACE					\
-    (function (wp, s->xp, s->size, s->yp, s->size, tspace),		\
-     mpn_toom33_mul_itch (s->size, s->size),				\
-     MPN_TOOM33_MUL_MINSIZE)
+    (function (wp, s->xp, s->xp, s->size, tspace),			\
+     MPN_KARA_MUL_N_TSIZE (s->size),					\
+     MPN_KARA_MUL_N_MINSIZE)
 
-#define SPEED_ROUTINE_MPN_TOOM44_MUL_N(function)			\
+#define SPEED_ROUTINE_MPN_TOOM3_MUL_N(function)				\
   SPEED_ROUTINE_MPN_MUL_N_TSPACE					\
-    (function (wp, s->xp, s->size, s->yp, s->size, tspace),		\
-     mpn_toom44_mul_itch (s->size, s->size),				\
-     MPN_TOOM44_MUL_MINSIZE)
+    (function (wp, s->xp, s->yp, s->size, tspace),			\
+     MPN_TOOM3_MUL_N_TSIZE (s->size),					\
+     MPN_TOOM3_MUL_N_MINSIZE)
 
-#define SPEED_ROUTINE_MPN_TOOM32_MUL(function)				\
-  SPEED_ROUTINE_MPN_MUL_N_TSPACE					\
-    (function (wp, s->xp, s->size, s->yp, 2*s->size/3, tspace),		\
-     mpn_toom32_mul_itch (s->size, 2*s->size/3),			\
-     MPN_TOOM32_MUL_MINSIZE)
-
-#define SPEED_ROUTINE_MPN_TOOM42_MUL(function)				\
-  SPEED_ROUTINE_MPN_MUL_N_TSPACE					\
-    (function (wp, s->xp, s->size, s->yp, s->size/2, tspace),		\
-     mpn_toom42_mul_itch (s->size, s->size/2),				\
-     MPN_TOOM42_MUL_MINSIZE)
 
 #define SPEED_ROUTINE_MPN_SQR_CALL(call)				\
   {									\
@@ -1185,21 +1058,16 @@
     return t;								\
   }
 
-#define SPEED_ROUTINE_MPN_TOOM2_SQR(function)				\
+#define SPEED_ROUTINE_MPN_KARA_SQR_N(function)				\
   SPEED_ROUTINE_MPN_SQR_TSPACE (function (wp, s->xp, s->size, tspace),	\
-				mpn_toom2_sqr_itch (s->size),		\
-				MPN_TOOM2_SQR_MINSIZE)
+				MPN_KARA_SQR_N_TSIZE (s->size),		\
+				MPN_KARA_SQR_N_MINSIZE)
 
-#define SPEED_ROUTINE_MPN_TOOM3_SQR(function)				\
+#define SPEED_ROUTINE_MPN_TOOM3_SQR_N(function)				\
   SPEED_ROUTINE_MPN_SQR_TSPACE (function (wp, s->xp, s->size, tspace),	\
-				mpn_toom3_sqr_itch (s->size),		\
-				MPN_TOOM3_SQR_MINSIZE)
-
+				MPN_TOOM3_SQR_N_TSIZE (s->size),	\
+				MPN_TOOM3_SQR_N_MINSIZE)
 
-#define SPEED_ROUTINE_MPN_TOOM4_SQR(function)				\
-  SPEED_ROUTINE_MPN_SQR_TSPACE (function (wp, s->xp, s->size, tspace),	\
-				mpn_toom4_sqr_itch (s->size),		\
-				MPN_TOOM4_SQR_MINSIZE)
 
 #define SPEED_ROUTINE_MPN_MOD_CALL(call)				\
   {									\
@@ -1263,7 +1131,6 @@
     unsigned  i;							\
     mp_ptr    a, d, q, r;						\
     double    t;							\
-    gmp_pi1_t dinv;							\
     TMP_DECL;								\
 									\
     SPEED_RESTRICT_COND (s->size >= 1);					\
@@ -1283,8 +1150,6 @@
     d[s->size-1] |= GMP_NUMB_HIGHBIT;					\
     a[2*s->size-1] = d[s->size-1] - 1;					\
 									\
-    invert_pi1 (dinv, d[s->size-1], d[s->size-2]);			\
-									\
     speed_operand_src (s, a, 2*s->size);				\
     speed_operand_src (s, d, s->size);					\
     speed_operand_dst (s, q, s->size+1);				\
@@ -1303,7 +1168,7 @@
   }
 
 #define SPEED_ROUTINE_MPN_DC_DIVREM_N(function)				\
-  SPEED_ROUTINE_MPN_DC_DIVREM_CALL((*function) (q, a, d, s->size, &dinv, r))
+  SPEED_ROUTINE_MPN_DC_DIVREM_CALL((*function) (q, a, d, s->size))
 
 #define SPEED_ROUTINE_MPN_DC_DIVREM_SB(function)			\
   SPEED_ROUTINE_MPN_DC_DIVREM_CALL					\
@@ -1314,259 +1179,40 @@
     ((*function) (q, r, 0, a, 2*s->size, d, s->size))
 
 
-/* A remainder 2*s->size by s->size limbs */
-
-#define SPEED_ROUTINE_MPZ_MOD(function)					\
-  {									\
-    unsigned   i;							\
-    mpz_t      a, d, r;							\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    mpz_init_set_n (d, s->yp, s->size);					\
-									\
-    /* high part less than d, low part a duplicate copied in */		\
-    mpz_init_set_n (a, s->xp, s->size);					\
-    mpz_mod (a, a, d);							\
-    mpz_mul_2exp (a, a, GMP_LIMB_BITS * s->size);			\
-    MPN_COPY (PTR(a), s->xp, s->size);					\
-									\
-    mpz_init (r);							\
-									\
-    speed_operand_src (s, PTR(a), SIZ(a));				\
-    speed_operand_src (s, PTR(d), SIZ(d));				\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do									\
-      function (r, a, d);						\
-    while (--i != 0);							\
-    return speed_endtime ();						\
-  }
-
-#define SPEED_ROUTINE_MPN_PI1_DIV(function, INV)			\
-  {									\
-    unsigned   i;							\
-    mp_ptr     dp, tp, ap, qp;						\
-    gmp_pi1_t  inv;							\
-    double     t;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ap, 2*s->size, s->align_xp);			\
-    SPEED_TMP_ALLOC_LIMBS (dp, s->size, s->align_yp);			\
-    SPEED_TMP_ALLOC_LIMBS (qp, s->size, s->align_wp);			\
-    SPEED_TMP_ALLOC_LIMBS (tp, 2*s->size, s->align_wp2);		\
-									\
-    MPN_COPY (ap,         s->xp, s->size);				\
-    MPN_COPY (ap+s->size, s->xp, s->size);				\
-									\
-    /* normalize the data */						\
-    dp[s->size-1] |= GMP_NUMB_HIGHBIT;					\
-    ap[2*s->size-1] = dp[s->size-1] - 1;				\
-									\
-    invert_pi1 (inv, dp[s->size-1], dp[s->size-2]);			\
-									\
-    speed_operand_src (s, ap, 2*s->size);				\
-    speed_operand_dst (s, tp, 2*s->size);				\
-    speed_operand_src (s, dp, s->size);					\
-    speed_operand_dst (s, qp, s->size);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do {								\
-      MPN_COPY (tp, ap, 2*s->size);					\
-      function (qp, tp, 2*s->size, dp, s->size, INV);			\
-    } while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
+/* A division of s->size by 3 limbs */
 
-#define SPEED_ROUTINE_MPN_PI1_BDIV_QR(function)				\
-  {									\
-    unsigned   i;							\
-    mp_ptr     dp, tp, ap, qp;						\
-    mp_limb_t  inv;							\
-    double     t;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ap, 2*s->size, s->align_xp);			\
-    SPEED_TMP_ALLOC_LIMBS (dp, s->size, s->align_yp);			\
-    SPEED_TMP_ALLOC_LIMBS (qp, s->size, s->align_wp);			\
-    SPEED_TMP_ALLOC_LIMBS (tp, 2*s->size, s->align_wp2);		\
-									\
-    MPN_COPY (ap,         s->xp, s->size);				\
-    MPN_COPY (ap+s->size, s->xp, s->size);				\
-									\
-    /* divisor must be odd */						\
-    MPN_COPY (dp, s->yp, s->size);					\
-    dp[0] |= 1;								\
-    binvert_limb (inv, dp[0]);						\
-    inv = -inv;								\
-									\
-    speed_operand_src (s, ap, 2*s->size);				\
-    speed_operand_dst (s, tp, 2*s->size);				\
-    speed_operand_src (s, dp, s->size);					\
-    speed_operand_dst (s, qp, s->size);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do {								\
-      MPN_COPY (tp, ap, 2*s->size);					\
-      function (qp, tp, 2*s->size, dp, s->size, inv);			\
-    } while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-#define SPEED_ROUTINE_MPN_PI1_BDIV_Q(function)				\
+#define SPEED_ROUTINE_MPN_SB_DIVREM_M3(function)			\
   {									\
     unsigned   i;							\
-    mp_ptr     dp, tp, qp;						\
-    mp_limb_t  inv;							\
+    mp_ptr     a, d, q;							\
+    mp_size_t  qsize;							\
     double     t;							\
     TMP_DECL;								\
 									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (dp, s->size, s->align_yp);			\
-    SPEED_TMP_ALLOC_LIMBS (qp, s->size, s->align_wp);			\
-    SPEED_TMP_ALLOC_LIMBS (tp, s->size, s->align_wp2);			\
-									\
-    /* divisor must be odd */						\
-    MPN_COPY (dp, s->yp, s->size);					\
-    dp[0] |= 1;								\
-    binvert_limb (inv, dp[0]);						\
-    inv = -inv;								\
-									\
-    speed_operand_src (s, s->xp, s->size);				\
-    speed_operand_dst (s, tp, s->size);					\
-    speed_operand_src (s, dp, s->size);					\
-    speed_operand_dst (s, qp, s->size);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do {								\
-      MPN_COPY (tp, s->xp, s->size);					\
-      function (qp, tp, s->size, dp, s->size, inv);			\
-    } while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-
-#define SPEED_ROUTINE_MPN_INVERT(function,itchfn)			\
-  {									\
-    long  i;								\
-    mp_ptr    up, tp, ip;						\
-    double    t;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ip, s->size, s->align_xp);			\
-    SPEED_TMP_ALLOC_LIMBS (up, s->size,   s->align_yp);			\
-    SPEED_TMP_ALLOC_LIMBS (tp, itchfn (s->size), s->align_wp);		\
-									\
-    MPN_COPY (up, s->xp, s->size);					\
-									\
-    /* normalize the data */						\
-    up[s->size-1] |= GMP_NUMB_HIGHBIT;					\
-									\
-    speed_operand_src (s, up, s->size);					\
-    speed_operand_dst (s, tp, s->size);					\
-    speed_operand_dst (s, ip, s->size);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do									\
-      function (ip, up, s->size, tp);					\
-    while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-
-#define SPEED_ROUTINE_MPN_INVERTAPPR(function,itchfn)			\
-  {									\
-    long  i;								\
-    mp_ptr    up, tp, ip;						\
-    double    t;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ip, s->size, s->align_xp);			\
-    SPEED_TMP_ALLOC_LIMBS (up, s->size, s->align_yp);			\
-    SPEED_TMP_ALLOC_LIMBS (tp, itchfn (s->size), s->align_wp);		\
-									\
-    MPN_COPY (up, s->xp, s->size);					\
-									\
-    /* normalize the data */						\
-    up[s->size-1] |= GMP_NUMB_HIGHBIT;					\
-									\
-    speed_operand_src (s, up, s->size);					\
-    speed_operand_dst (s, tp, s->size);					\
-    speed_operand_dst (s, ip, s->size);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do									\
-      function (ip, up, s->size, tp);					\
-    while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-
-#define SPEED_ROUTINE_MPN_NI_INVERTAPPR(function,itchfn)		\
-  {									\
-    long  i;								\
-    mp_ptr    up, tp, ip;						\
-    double    t;							\
-    TMP_DECL;								\
-									\
     SPEED_RESTRICT_COND (s->size >= 3);					\
 									\
     TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ip, s->size, s->align_xp);			\
-    SPEED_TMP_ALLOC_LIMBS (up, s->size, s->align_yp);			\
-    SPEED_TMP_ALLOC_LIMBS (tp, itchfn (s->size), s->align_wp);		\
-									\
-    MPN_COPY (up, s->xp, s->size);					\
-									\
-    /* normalize the data */						\
-    up[s->size-1] |= GMP_NUMB_HIGHBIT;					\
+    SPEED_TMP_ALLOC_LIMBS (a, s->size, s->align_xp);			\
 									\
-    speed_operand_src (s, up, s->size);					\
-    speed_operand_dst (s, tp, s->size);					\
-    speed_operand_dst (s, ip, s->size);					\
+    SPEED_TMP_ALLOC_LIMBS (d, 3, s->align_yp);				\
+    MPN_COPY (d, s->yp, 3);						\
+    d[2] |= GMP_NUMB_HIGHBIT;						\
+									\
+    qsize = s->size - 3;						\
+    SPEED_TMP_ALLOC_LIMBS (q, qsize, s->align_wp);			\
+									\
+    speed_operand_dst (s, a, s->size);					\
+    speed_operand_src (s, d, 3);					\
+    speed_operand_dst (s, q, qsize);					\
     speed_cache_fill (s);						\
 									\
     speed_starttime ();							\
     i = s->reps;							\
     do									\
-      function (ip, up, s->size, tp);					\
+      {									\
+	MPN_COPY (a, s->xp, s->size);					\
+	function (q, a, s->size, d, 3);					\
+      }									\
     while (--i != 0);							\
     t = speed_endtime ();						\
 									\
@@ -1574,46 +1220,43 @@
     return t;								\
   }
 
-#define SPEED_ROUTINE_MPN_BINVERT(function,itchfn)			\
+/* A remainder 2*s->size by s->size limbs */
+
+#define SPEED_ROUTINE_MPZ_MOD(function)					\
   {									\
-    long  i;								\
-    mp_ptr    up, tp, ip;						\
-    double    t;							\
-    TMP_DECL;								\
+    unsigned   i;							\
+    mpz_t      a, d, r;							\
 									\
     SPEED_RESTRICT_COND (s->size >= 1);					\
 									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ip, s->size, s->align_xp);			\
-    SPEED_TMP_ALLOC_LIMBS (up, s->size,   s->align_yp);			\
-    SPEED_TMP_ALLOC_LIMBS (tp, itchfn (s->size), s->align_wp);		\
+    mpz_init_set_n (d, s->yp, s->size);					\
 									\
-    MPN_COPY (up, s->xp, s->size);					\
+    /* high part less than d, low part a duplicate copied in */		\
+    mpz_init_set_n (a, s->xp, s->size);					\
+    mpz_mod (a, a, d);							\
+    mpz_mul_2exp (a, a, BITS_PER_MP_LIMB * s->size);			\
+    MPN_COPY (PTR(a), s->xp, s->size);					\
 									\
-    /* normalize the data */						\
-    up[0] |= 1;								\
+    mpz_init (r);							\
 									\
-    speed_operand_src (s, up, s->size);					\
-    speed_operand_dst (s, tp, s->size);					\
-    speed_operand_dst (s, ip, s->size);					\
+    speed_operand_src (s, PTR(a), SIZ(a));				\
+    speed_operand_src (s, PTR(d), SIZ(d));				\
     speed_cache_fill (s);						\
 									\
     speed_starttime ();							\
     i = s->reps;							\
     do									\
-      function (ip, up, s->size, tp);					\
+      function (r, a, d);						\
     while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
+    return speed_endtime ();						\
   }
 
+
 #define SPEED_ROUTINE_REDC_1(function)					\
   {									\
     unsigned   i;							\
     mp_ptr     cp, mp, tp, ap;						\
-    mp_limb_t  inv;							\
+    mp_limb_t  Nprim;							\
     double     t;							\
     TMP_DECL;								\
 									\
@@ -1631,91 +1274,7 @@
     /* modulus must be odd */						\
     MPN_COPY (mp, s->yp, s->size);					\
     mp[0] |= 1;								\
-    binvert_limb (inv, mp[0]);						\
-    inv = -inv;								\
-									\
-    speed_operand_src (s, ap, 2*s->size+1);				\
-    speed_operand_dst (s, tp, 2*s->size+1);				\
-    speed_operand_src (s, mp, s->size);					\
-    speed_operand_dst (s, cp, s->size);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do {								\
-      MPN_COPY (tp, ap, 2*s->size);					\
-      function (cp, tp, mp, s->size, inv);				\
-    } while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-#define SPEED_ROUTINE_REDC_2(function)					\
-  {									\
-    unsigned   i;							\
-    mp_ptr     cp, mp, tp, ap;						\
-    mp_limb_t  invp[2];							\
-    double     t;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size >= 1);					\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ap, 2*s->size+1, s->align_xp);		\
-    SPEED_TMP_ALLOC_LIMBS (mp, s->size,     s->align_yp);		\
-    SPEED_TMP_ALLOC_LIMBS (cp, s->size,     s->align_wp);		\
-    SPEED_TMP_ALLOC_LIMBS (tp, 2*s->size+1, s->align_wp2);		\
-									\
-    MPN_COPY (ap,         s->xp, s->size);				\
-    MPN_COPY (ap+s->size, s->xp, s->size);				\
-									\
-    /* modulus must be odd */						\
-    MPN_COPY (mp, s->yp, s->size);					\
-    mp[0] |= 1;								\
-    mpn_binvert (invp, mp, 2, tp);					\
-    invp[0] = -invp[0]; invp[1] = ~invp[1];				\
-									\
-    speed_operand_src (s, ap, 2*s->size+1);				\
-    speed_operand_dst (s, tp, 2*s->size+1);				\
-    speed_operand_src (s, mp, s->size);					\
-    speed_operand_dst (s, cp, s->size);					\
-    speed_cache_fill (s);						\
-									\
-    speed_starttime ();							\
-    i = s->reps;							\
-    do {								\
-      MPN_COPY (tp, ap, 2*s->size);					\
-      function (cp, tp, mp, s->size, invp);				\
-    } while (--i != 0);							\
-    t = speed_endtime ();						\
-									\
-    TMP_FREE;								\
-    return t;								\
-  }
-#define SPEED_ROUTINE_REDC_N(function)					\
-  {									\
-    unsigned   i;							\
-    mp_ptr     cp, mp, tp, ap, invp;					\
-    double     t;							\
-    TMP_DECL;								\
-									\
-    SPEED_RESTRICT_COND (s->size > 8);					\
-									\
-    TMP_MARK;								\
-    SPEED_TMP_ALLOC_LIMBS (ap, 2*s->size+1, s->align_xp);		\
-    SPEED_TMP_ALLOC_LIMBS (mp, s->size,     s->align_yp);		\
-    SPEED_TMP_ALLOC_LIMBS (cp, s->size,     s->align_wp);		\
-    SPEED_TMP_ALLOC_LIMBS (tp, 2*s->size+1, s->align_wp2);		\
-    SPEED_TMP_ALLOC_LIMBS (invp, s->size,   s->align_wp2); /* align? */	\
-									\
-    MPN_COPY (ap,         s->xp, s->size);				\
-    MPN_COPY (ap+s->size, s->xp, s->size);				\
-									\
-    /* modulus must be odd */						\
-    MPN_COPY (mp, s->yp, s->size);					\
-    mp[0] |= 1;								\
-    mpn_binvert (invp, mp, s->size, tp);				\
+    binvert_limb (Nprim, mp[0]);					\
 									\
     speed_operand_src (s, ap, 2*s->size+1);				\
     speed_operand_dst (s, tp, 2*s->size+1);				\
@@ -1727,7 +1286,7 @@
     i = s->reps;							\
     do {								\
       MPN_COPY (tp, ap, 2*s->size);					\
-      function (cp, tp, mp, s->size, invp);				\
+      function (cp, tp, mp, s->size, Nprim);				\
     } while (--i != 0);							\
     t = speed_endtime ();						\
 									\
@@ -2506,8 +2065,8 @@
     for (i = 0; i < s->size; i++)					\
       xp[i] = s->xp[i] % base;						\
 									\
-    wn = ((mp_size_t) (s->size / mp_bases[base].chars_per_bit_exactly)) \
-      / GMP_LIMB_BITS + 2;						\
+    wn = ((mp_size_t) (s->size / __mp_bases[base].chars_per_bit_exactly)) \
+      / BITS_PER_MP_LIMB + 2;						\
     SPEED_TMP_ALLOC_LIMBS (wp, wn, s->align_wp);			\
 									\
     /* use this during development to check wn is big enough */		\
--- 1/tune/time.c
+++ 2/tune/time.c
@@ -1289,7 +1289,7 @@
    psecs might overflow.  2^32 microseconds is only a bit over an hour, or
    2^32 nanoseconds only about 4 seconds.
 
-   The casts to "long" are for the benefit of timebasestruct_t, where the
+   The casts to "long" are for the beneifit of timebasestruct_t, where the
    fields are only "unsigned int", but we want a signed difference.  */
 
 #define DIFF_SECS_ROUTINE(sec, psec, punit)                     \
--- 1/tune/tuneup.c
+++ 2/tune/tuneup.c
@@ -1,7 +1,6 @@
 /* Create tuned thresholds for various algorithms.
 
-Copyright 1999, 2000, 2001, 2002, 2003, 2005, 2006, 2008, 2009 Free Software
-Foundation, Inc.
+Copyright 1999, 2000, 2001, 2002, 2003, 2005 Free Software Foundation, Inc.
 
 This file is part of the GNU MP Library.
 
@@ -65,11 +64,11 @@
    instead.  #define TUNE_PROGRAM_BUILD does this, with help from code at
    the end of gmp-impl.h, and rules in tune/Makefile.am.
 
-   MUL_TOOM22_THRESHOLD for example uses a recompiled mpn_mul_n.  The
+   MUL_KARATSUBA_THRESHOLD for example uses a recompiled mpn_mul_n.  The
    threshold is set to "size+1" to avoid karatsuba, or to "size" to use one
    level, but recurse into the basecase.
 
-   MUL_TOOM33_THRESHOLD makes use of the tuned MUL_TOOM22_THRESHOLD value.
+   MUL_TOOM3_THRESHOLD makes use of the tuned MUL_KARATSUBA_THRESHOLD value.
    Other routines in turn will make use of both of those.  Naturally the
    dependants must be tuned first.
 
@@ -88,7 +87,7 @@
    DIVREM_1_NORM_THRESHOLD.  An assembler mpn_divrem_1 is expected to be
    written and tuned all by hand.  Assembler routines that might have hard
    limits are recompiled though, to make them accept a bigger range of sizes
-   than normal, eg. mpn_sqr_basecase to compare against mpn_toom2_sqr.
+   than normal, eg. mpn_sqr_basecase to compare against mpn_kara_sqr_n.
 
    Limitations:
 
@@ -143,38 +142,27 @@
 
 /* This is not defined if mpn_sqr_basecase doesn't declare a limit.  In that
    case use zero here, which for params.max_size means no limit.  */
-#ifndef TUNE_SQR_TOOM2_MAX
-#define TUNE_SQR_TOOM2_MAX  0
+#ifndef TUNE_SQR_KARATSUBA_MAX
+#define TUNE_SQR_KARATSUBA_MAX  0
 #endif
 
-mp_size_t  mul_toom22_threshold         = MP_SIZE_T_MAX;
-mp_size_t  mul_toom33_threshold         = MUL_TOOM33_THRESHOLD_LIMIT;
+mp_size_t  mul_karatsuba_threshold      = MP_SIZE_T_MAX;
+mp_size_t  mul_toom3_threshold          = MUL_TOOM3_THRESHOLD_LIMIT;
 mp_size_t  mul_toom44_threshold         = MUL_TOOM44_THRESHOLD_LIMIT;
 mp_size_t  mul_fft_threshold            = MP_SIZE_T_MAX;
 mp_size_t  mul_fft_modf_threshold       = MP_SIZE_T_MAX;
 mp_size_t  sqr_basecase_threshold       = MP_SIZE_T_MAX;
-mp_size_t  sqr_toom2_threshold
-  = (TUNE_SQR_TOOM2_MAX == 0 ? MP_SIZE_T_MAX : TUNE_SQR_TOOM2_MAX);
+mp_size_t  sqr_karatsuba_threshold
+  = (TUNE_SQR_KARATSUBA_MAX == 0 ? MP_SIZE_T_MAX : TUNE_SQR_KARATSUBA_MAX);
 mp_size_t  sqr_toom3_threshold          = SQR_TOOM3_THRESHOLD_LIMIT;
 mp_size_t  sqr_toom4_threshold          = SQR_TOOM4_THRESHOLD_LIMIT;
 mp_size_t  sqr_fft_threshold            = MP_SIZE_T_MAX;
 mp_size_t  sqr_fft_modf_threshold       = MP_SIZE_T_MAX;
-mp_size_t  mullo_basecase_threshold     = MP_SIZE_T_MAX;
-mp_size_t  mullo_dc_threshold           = MP_SIZE_T_MAX;
-mp_size_t  mullo_mul_n_threshold        = MP_SIZE_T_MAX;
-mp_size_t  mulmod_bnm1_threshold        = MP_SIZE_T_MAX;
+mp_size_t  mullow_basecase_threshold    = MP_SIZE_T_MAX;
+mp_size_t  mullow_dc_threshold          = MP_SIZE_T_MAX;
+mp_size_t  mullow_mul_n_threshold       = MP_SIZE_T_MAX;
 mp_size_t  div_sb_preinv_threshold      = MP_SIZE_T_MAX;
-mp_size_t  dc_div_qr_threshold          = MP_SIZE_T_MAX;
-mp_size_t  dc_divappr_q_threshold       = MP_SIZE_T_MAX;
-mp_size_t  dc_bdiv_qr_threshold         = MP_SIZE_T_MAX;
-mp_size_t  dc_bdiv_q_threshold          = MP_SIZE_T_MAX;
-mp_size_t  inv_mulmod_bnm1_threshold    = MP_SIZE_T_MAX;
-mp_size_t  inv_newton_threshold         = MP_SIZE_T_MAX;
-mp_size_t  inv_appr_threshold           = MP_SIZE_T_MAX;
-mp_size_t  binv_newton_threshold        = MP_SIZE_T_MAX;
-mp_size_t  redc_1_to_redc_2_threshold   = MP_SIZE_T_MAX;
-mp_size_t  redc_1_to_redc_n_threshold   = MP_SIZE_T_MAX;
-mp_size_t  redc_2_to_redc_n_threshold   = MP_SIZE_T_MAX;
+mp_size_t  div_dc_threshold             = MP_SIZE_T_MAX;
 mp_size_t  powm_threshold               = MP_SIZE_T_MAX;
 mp_size_t  matrix22_strassen_threshold  = MP_SIZE_T_MAX;
 mp_size_t  hgcd_threshold               = MP_SIZE_T_MAX;
@@ -202,8 +190,7 @@
   const char        *name;
   speed_function_t  function;
   speed_function_t  function2;
-  double            step_factor;    /* how much to step relatively */
-  int               step;           /* how much to step absolutely */
+  double            step_factor;    /* how much to step sizes (rounded down) */
   double            function_fudge; /* multiplier for "function" speeds */
   int               stop_since_change;
   double            stop_factor;
@@ -427,7 +414,6 @@
   if (remark != NULL)
     printf ("  /* %s */", remark);
   printf ("\n");
-  fflush (stdout);
 }
 
 void
@@ -469,7 +455,6 @@
   DEFAULT (param->function_fudge, 1.0);
   DEFAULT (param->function2, param->function);
   DEFAULT (param->step_factor, 0.01);  /* small steps by default */
-  DEFAULT (param->step, 1);            /* small steps by default */
   DEFAULT (param->stop_since_change, 80);
   DEFAULT (param->stop_factor, 1.2);
   DEFAULT (param->min_size, 10);
@@ -525,10 +510,21 @@
 
   for (s.size = param->min_size;
        s.size < param->max_size;
-       s.size += MAX ((mp_size_t) floor (s.size * param->step_factor), param->step))
+       s.size += MAX ((mp_size_t) floor (s.size * param->step_factor), 1))
     {
       double   ti, tiplus1, d;
 
+      /* If there's a size limit and it's reached then it should still
+         be sensible to analyze the data since we want the threshold put
+         either at or near the limit.  */
+      if (s.size >= param->max_size)
+        {
+          if (option_trace)
+            printf ("Reached maximum size (%ld) without otherwise stopping\n",
+                    (long) param->max_size);
+          break;
+        }
+
       /*
         FIXME: check minimum size requirements are met, possibly by just
         checking for the -1 returns from the speed functions.
@@ -588,7 +584,7 @@
         }
 
       /* Stop if the threshold implied hasn't changed in a certain
-         number of measurements.  (It's this condition that usually
+         number of measurements.  (It's this condition that ususally
          stops the loop.) */
       if (thresh_idx != new_thresh_idx)
         since_thresh_change = 0, thresh_idx = new_thresh_idx;
@@ -677,7 +673,7 @@
 {
   mp_size_t  step;
 
-  step = MAX ((mp_size_t) 1 << (k-1), GMP_LIMB_BITS) / GMP_LIMB_BITS;
+  step = MAX ((mp_size_t) 1 << (k-1), BITS_PER_MP_LIMB) / BITS_PER_MP_LIMB;
   step *= (mp_size_t) 1 << k;
 
   if (step <= 0)
@@ -836,18 +832,18 @@
 
   param.function = speed_mpn_mul_n;
 
-  param.name = "MUL_TOOM22_THRESHOLD";
-  param.min_size = MAX (4, MPN_TOOM22_MUL_MINSIZE);
-  param.max_size = MUL_TOOM22_THRESHOLD_LIMIT-1;
-  one (&mul_toom22_threshold, &param);
-
-  param.name = "MUL_TOOM33_THRESHOLD";
-  param.min_size = MAX (mul_toom22_threshold, MPN_TOOM33_MUL_MINSIZE);
-  param.max_size = MUL_TOOM33_THRESHOLD_LIMIT-1;
-  one (&mul_toom33_threshold, &param);
+  param.name = "MUL_KARATSUBA_THRESHOLD";
+  param.min_size = MAX (4, MPN_KARA_MUL_N_MINSIZE);
+  param.max_size = MUL_KARATSUBA_THRESHOLD_LIMIT-1;
+  one (&mul_karatsuba_threshold, &param);
+
+  param.name = "MUL_TOOM3_THRESHOLD";
+  param.min_size = MAX (mul_karatsuba_threshold, MPN_TOOM3_MUL_N_MINSIZE);
+  param.max_size = MUL_TOOM3_THRESHOLD_LIMIT-1;
+  one (&mul_toom3_threshold, &param);
 
   param.name = "MUL_TOOM44_THRESHOLD";
-  param.min_size = MAX (mul_toom33_threshold, MPN_TOOM44_MUL_MINSIZE);
+  param.min_size = MAX (mul_toom3_threshold, MPN_TOOM44_MUL_N_MINSIZE);
   param.max_size = MUL_TOOM44_THRESHOLD_LIMIT-1;
   one (&mul_toom44_threshold, &param);
 
@@ -856,61 +852,32 @@
 }
 
 
+/* This was written by the tuneup challenged tege.  Kevin, please delete
+   this comment when you've reviewed/rewritten this.  :-) */
 void
-tune_mullo (void)
+tune_mullow (void)
 {
   static struct param_t  param;
 
-  param.function = speed_mpn_mullo_n;
+  param.function = speed_mpn_mullow_n;
 
-  param.name = "MULLO_BASECASE_THRESHOLD";
-  param.min_size = 1;
+  param.name = "MULLOW_BASECASE_THRESHOLD";
+  param.min_size = 3;
   param.min_is_always = 1;
-  param.max_size = MULLO_BASECASE_THRESHOLD_LIMIT-1;
-  param.stop_factor = 1.5;
-  param.noprint = 1;
-  one (&mullo_basecase_threshold, &param);
-
-  param.name = "MULLO_DC_THRESHOLD";
-  param.min_size = 8;
-  param.min_is_always = 0;
-  param.max_size = 1000;
-  one (&mullo_dc_threshold, &param);
+  param.max_size = MULLOW_BASECASE_THRESHOLD_LIMIT-1;
+  one (&mullow_basecase_threshold, &param);
 
-  if (mullo_basecase_threshold >= mullo_dc_threshold)
-    {
-      print_define ("MULLO_BASECASE_THRESHOLD", mullo_dc_threshold);
-      print_define_remark ("MULLO_DC_THRESHOLD", 0, "never mpn_mullo_basecase");
-    }
-  else
-    {
-      print_define ("MULLO_BASECASE_THRESHOLD", mullo_basecase_threshold);
-      print_define ("MULLO_DC_THRESHOLD", mullo_dc_threshold);
-    }
+  param.min_is_always = 0;	/* ??? */
 
-#if WANT_FFT
-  param.name = "MULLO_MUL_N_THRESHOLD";
-  param.min_size = mullo_dc_threshold;
-  param.max_size = 2 * mul_fft_threshold;
-  param.noprint = 0;
-  param.step_factor = 0.02;
-  one (&mullo_mul_n_threshold, &param);
-#else
-  print_define_remark ("MULLO_MUL_N_THRESHOLD", MP_SIZE_T_MAX,
-                           "without FFT use mullo forever");
-#endif
-}
-
-void
-tune_mulmod_bnm1 (void)
-{
-  static struct param_t  param;
+  param.name = "MULLOW_DC_THRESHOLD";
+  param.min_size = mul_karatsuba_threshold;
+  param.max_size = 1000;
+  one (&mullow_dc_threshold, &param);
 
-  param.name = "MULMOD_BNM1_THRESHOLD";
-  param.function = speed_mpn_mulmod_bnm1;
-  param.min_size = 4;
-  param.max_size = 100;
-  one (&mulmod_bnm1_threshold, &param);
+  param.name = "MULLOW_MUL_N_THRESHOLD";
+  param.min_size = mullow_dc_threshold;
+  param.max_size = 2000;
+  one (&mullow_mul_n_threshold, &param);
 }
 
 
@@ -936,59 +903,59 @@
       param.function = speed_mpn_sqr_n;
       param.min_size = 3;
       param.min_is_always = 1;
-      param.max_size = TUNE_SQR_TOOM2_MAX;
+      param.max_size = TUNE_SQR_KARATSUBA_MAX;
       param.noprint = 1;
       one (&sqr_basecase_threshold, &param);
     }
 
   {
     static struct param_t  param;
-    param.name = "SQR_TOOM2_THRESHOLD";
+    param.name = "SQR_KARATSUBA_THRESHOLD";
     param.function = speed_mpn_sqr_n;
-    param.min_size = MAX (4, MPN_TOOM2_SQR_MINSIZE);
-    param.max_size = TUNE_SQR_TOOM2_MAX;
+    param.min_size = MAX (4, MPN_KARA_SQR_N_MINSIZE);
+    param.max_size = TUNE_SQR_KARATSUBA_MAX;
     param.noprint = 1;
-    one (&sqr_toom2_threshold, &param);
+    one (&sqr_karatsuba_threshold, &param);
 
     if (! HAVE_NATIVE_mpn_sqr_basecase
-        && sqr_toom2_threshold < sqr_basecase_threshold)
+        && sqr_karatsuba_threshold < sqr_basecase_threshold)
       {
         /* Karatsuba becomes faster than mul_basecase before
            sqr_basecase does.  Arrange for the expression
-           "BELOW_THRESHOLD (un, SQR_TOOM2_THRESHOLD))" which
+           "BELOW_THRESHOLD (un, SQR_KARATSUBA_THRESHOLD))" which
            selects mpn_sqr_basecase in mpn_sqr_n to be false, by setting
-           SQR_TOOM2_THRESHOLD to zero, making
-           SQR_BASECASE_THRESHOLD the toom2 threshold.  */
+           SQR_KARATSUBA_THRESHOLD to zero, making
+           SQR_BASECASE_THRESHOLD the karatsuba threshold.  */
 
-        sqr_basecase_threshold = SQR_TOOM2_THRESHOLD;
-        SQR_TOOM2_THRESHOLD = 0;
+        sqr_basecase_threshold = SQR_KARATSUBA_THRESHOLD;
+        SQR_KARATSUBA_THRESHOLD = 0;
 
         print_define_remark ("SQR_BASECASE_THRESHOLD", sqr_basecase_threshold,
-                             "toom2");
-        print_define_remark ("SQR_TOOM2_THRESHOLD",SQR_TOOM2_THRESHOLD,
+                             "karatsuba");
+        print_define_remark ("SQR_KARATSUBA_THRESHOLD",SQR_KARATSUBA_THRESHOLD,
                              "never sqr_basecase");
       }
     else
       {
         if (! HAVE_NATIVE_mpn_sqr_basecase)
           print_define ("SQR_BASECASE_THRESHOLD", sqr_basecase_threshold);
-        print_define ("SQR_TOOM2_THRESHOLD", SQR_TOOM2_THRESHOLD);
+        print_define ("SQR_KARATSUBA_THRESHOLD", SQR_KARATSUBA_THRESHOLD);
       }
   }
 
   {
     static struct param_t  param;
-    mp_size_t toom3_start = MAX (sqr_toom2_threshold, sqr_basecase_threshold);
+    mp_size_t toom3_start = MAX (sqr_karatsuba_threshold, sqr_basecase_threshold);
 
     param.function = speed_mpn_sqr_n;
 
     param.name = "SQR_TOOM3_THRESHOLD";
-    param.min_size = MAX (toom3_start, MPN_TOOM3_SQR_MINSIZE);
+    param.min_size = MAX (toom3_start, MPN_TOOM3_SQR_N_MINSIZE);
     param.max_size = SQR_TOOM3_THRESHOLD_LIMIT-1;
     one (&sqr_toom3_threshold, &param);
 
     param.name = "SQR_TOOM4_THRESHOLD";
-    param.min_size = MAX (sqr_toom3_threshold, MPN_TOOM4_SQR_MINSIZE);
+    param.min_size = MAX (sqr_toom3_threshold, MPN_TOOM4_SQR_N_MINSIZE);
     param.max_size = SQR_TOOM4_THRESHOLD_LIMIT-1;
     one (&sqr_toom4_threshold, &param);
   }
@@ -996,139 +963,68 @@
 
 
 void
-tune_dc_div (void)
-{
-  {
-    static struct param_t  param;
-    param.name = "DC_DIV_QR_THRESHOLD";
-    param.function = speed_mpn_sbpi1_div_qr;
-    param.function2 = speed_mpn_dcpi1_div_qr;
-    param.min_size = 6;
-    one (&dc_div_qr_threshold, &param);
-  }
-  {
-    static struct param_t  param;
-    param.name = "DC_DIVAPPR_Q_THRESHOLD";
-    param.function = speed_mpn_sbpi1_divappr_q;
-    param.function2 = speed_mpn_dcpi1_divappr_q;
-    param.min_size = 6;
-    one (&dc_divappr_q_threshold, &param);
-  }
-}
-
-void
-tune_dc_bdiv (void)
-{
-  {
-    static struct param_t  param;
-    param.name = "DC_BDIV_QR_THRESHOLD";
-    param.function = speed_mpn_sbpi1_bdiv_qr;
-    param.function2 = speed_mpn_dcpi1_bdiv_qr;
-    param.min_size = 4;
-    one (&dc_bdiv_qr_threshold, &param);
-  }
-  {
-    static struct param_t  param;
-    param.name = "DC_BDIV_Q_THRESHOLD";
-    param.function = speed_mpn_sbpi1_bdiv_q;
-    param.function2 = speed_mpn_dcpi1_bdiv_q;
-    param.min_size = 4;
-    one (&dc_bdiv_q_threshold, &param);
-  }
-}
-
-void
-tune_invertappr (void)
+tune_sb_preinv (void)
 {
   static struct param_t  param;
 
-  param.function = speed_mpn_ni_invertappr;
-  param.name = "INV_MULMOD_BNM1_THRESHOLD";
-  param.min_size = 4;
-  param.max_size = 5000;
-  one (&inv_mulmod_bnm1_threshold, &param);
+  if (GMP_NAIL_BITS != 0)
+    {
+      DIV_SB_PREINV_THRESHOLD = MP_SIZE_T_MAX;
+      print_define_remark ("DIV_SB_PREINV_THRESHOLD", MP_SIZE_T_MAX,
+                           "no preinv with nails");
+      return;
+    }
 
-  param.function = speed_mpn_invertappr;
-  param.name = "INV_NEWTON_THRESHOLD";
+  if (UDIV_PREINV_ALWAYS)
+    {
+      print_define_remark ("DIV_SB_PREINV_THRESHOLD", 0L, "preinv always");
+      return;
+    }
+
+  param.check_size = 256;
   param.min_size = 3;
-  param.max_size = 5000;
-  one (&inv_newton_threshold, &param);
+  param.min_is_always = 1;
+  param.size_extra = 3;
+  param.stop_factor = 2.0;
+  param.name = "DIV_SB_PREINV_THRESHOLD";
+  param.function = speed_mpn_sb_divrem_m3;
+  one (&div_sb_preinv_threshold, &param);
 }
 
+
 void
-tune_invert (void)
+tune_dc (void)
 {
   static struct param_t  param;
-
-  param.function = speed_mpn_invert;
-  param.name = "INV_APPR_THRESHOLD";
-  param.min_size = 3;
-  param.max_size = 5000;
-  one (&inv_appr_threshold, &param);
+  param.name = "DIV_DC_THRESHOLD";
+  param.function = speed_mpn_dc_tdiv_qr;
+  param.step_factor = 0.02;
+  one (&div_dc_threshold, &param);
 }
 
-void
-tune_binvert (void)
-{
-  static struct param_t  param;
 
-  param.function = speed_mpn_binvert;
-  param.name = "BINV_NEWTON_THRESHOLD";
-  param.min_size = 8;		/* pointless with smaller operands */
-  param.max_size = 5000;
-  one (&binv_newton_threshold, &param);
-}
+/* This is an indirect determination, based on a comparison between redc and
+   mpz_mod.  A fudge factor of 1.04 is applied to redc, to represent
+   additional overheads it gets in mpz_powm.
+
+   stop_factor is 1.1 to hopefully help cray vector systems, where otherwise
+   currently it hits the 1000 limb limit with only a factor of about 1.18
+   (threshold should be around 650).  */
 
 void
-tune_redc (void)
+tune_powm (void)
 {
-#define TUNE_REDC_2_MAX 100
-#if HAVE_NATIVE_mpn_addmul_2 || HAVE_NATIVE_mpn_redc_2
-#define WANT_REDC_2 1
-#endif
-
-#if WANT_REDC_2
-  {
-    static struct param_t  param;
-    param.name = "REDC_1_TO_REDC_2_THRESHOLD";
-    param.function = speed_mpn_redc_1;
-    param.function2 = speed_mpn_redc_2;
-    param.max_size = TUNE_REDC_2_MAX;
-    param.noprint = 1;
-    one (&redc_1_to_redc_2_threshold, &param);
-  }
-  {
-    static struct param_t  param;
-    param.name = "REDC_2_TO_REDC_N_THRESHOLD";
-    param.function = speed_mpn_redc_2;
-    param.function2 = speed_mpn_redc_n;
-    param.min_size = 16;
-    param.noprint = 1;
-    one (&redc_2_to_redc_n_threshold, &param);
-  }
-  if (redc_1_to_redc_2_threshold >= TUNE_REDC_2_MAX - 1)
-    {
-      /* Disable REDC_2.  This is not supposed to happen.  */
-      print_define ("REDC_1_TO_REDC_2_THRESHOLD", REDC_2_TO_REDC_N_THRESHOLD);
-      print_define_remark ("REDC_2_TO_REDC_N_THRESHOLD", 0, "anomaly: never REDC_2");
-    }
-  else
-    {
-      print_define ("REDC_1_TO_REDC_2_THRESHOLD", REDC_1_TO_REDC_2_THRESHOLD);
-      print_define ("REDC_2_TO_REDC_N_THRESHOLD", REDC_2_TO_REDC_N_THRESHOLD);
-    }
-#else
-  {
-    static struct param_t  param;
-    param.name = "REDC_1_TO_REDC_N_THRESHOLD";
-    param.function = speed_mpn_redc_1;
-    param.function2 = speed_mpn_redc_n;
-    param.min_size = 16;
-    one (&redc_1_to_redc_n_threshold, &param);
-  }
-#endif
+  static struct param_t  param;
+  param.name = "POWM_THRESHOLD";
+  param.function = speed_mpn_redc_1;
+  param.function2 = speed_mpz_mod;
+  param.step_factor = 0.03;
+  param.stop_factor = 1.1;
+  param.function_fudge = 1.04;
+  one (&powm_threshold, &param);
 }
 
+
 void
 tune_matrix22_mul (void)
 {
@@ -1150,6 +1046,17 @@
   one (&hgcd_threshold, &param);
 }
 
+#if 0
+void
+tune_gcd_accel (void)
+{
+  static struct param_t  param;
+  param.name = "GCD_ACCEL_THRESHOLD";
+  param.function = speed_mpn_gcd;
+  param.min_size = 1;
+  one (&gcd_accel_threshold, &param);
+}
+#endif
 void
 tune_gcd_dc (void)
 {
@@ -1299,13 +1206,11 @@
   {
     static struct param_t  param;
 
-    param.check_size = 256;
-
     s.r = GMP_NUMB_MASK / 5;
     param.function = speed_mpn_mod_1_tune;
+    param.min_size = 1;
 
     param.name = "MOD_1_1_THRESHOLD";
-    param.min_size = 2;
     one (&mod_1_1_threshold, &param);
 
     param.name = "MOD_1_2_THRESHOLD";
@@ -1652,7 +1557,7 @@
   double   t1, t2, t3;
   int      method;
 
-  s.size = GMP_LIMB_BITS * 3 / 4;
+  s.size = BITS_PER_MP_LIMB * 3 / 4;
 
   t1 = tuneup_measure (speed_mpn_jacobi_base_1, &param, &s);
   if (option_trace >= 1)
@@ -1736,8 +1641,8 @@
   for (i = 0; i < s->size; i++)
     str[i] = s->xp[i] % base;
 
-  wn = ((mp_size_t) (s->size / mp_bases[base].chars_per_bit_exactly))
-    / GMP_LIMB_BITS + 2;
+  wn = ((mp_size_t) (s->size / __mp_bases[base].chars_per_bit_exactly))
+    / BITS_PER_MP_LIMB + 2;
   SPEED_TMP_ALLOC_LIMBS (wp, wn, s->align_wp);
 
   /* use this during development to check wn is big enough */
@@ -1749,7 +1654,7 @@
   speed_operand_dst (s, wp, wn);
   speed_cache_fill (s);
 
-  chars_per_limb = mp_bases[base].chars_per_limb;
+  chars_per_limb = __mp_bases[base].chars_per_limb;
   un = s->size / chars_per_limb + 1;
   powtab_mem = TMP_BALLOC_LIMBS (mpn_dc_set_str_powtab_alloc (un));
   mpn_set_str_compute_powtab (powtab, powtab_mem, un, base);
@@ -1771,6 +1676,8 @@
 void
 tune_set_str (void)
 {
+  static struct param_t  param;
+
   s.r = 10;  /* decimal */
   {
     static struct param_t  param;
@@ -1807,7 +1714,7 @@
   param.p_threshold         = &mul_fft_threshold;
   param.modf_threshold_name = "MUL_FFT_MODF_THRESHOLD";
   param.p_modf_threshold    = &mul_fft_modf_threshold;
-  param.first_size          = MUL_TOOM33_THRESHOLD / 2;
+  param.first_size          = MUL_TOOM3_THRESHOLD / 2;
   param.max_size            = option_fft_max_size;
   param.function            = speed_mpn_mul_fft;
   param.mul_function        = speed_mpn_mul_n;
@@ -1910,30 +1817,21 @@
   tune_sqr ();
   printf("\n");
 
-  tune_fft_mul ();
-  printf("\n");
-
-  tune_fft_sqr ();
-  printf ("\n");
-
-  tune_mullo ();
-  printf("\n");
-
-  tune_mulmod_bnm1 ();
+  tune_mullow ();
   printf("\n");
 
-  tune_dc_div ();
-  tune_dc_bdiv ();
-  tune_invertappr ();
-  tune_invert ();
-  tune_binvert ();
-  tune_redc ();
+  tune_sb_preinv ();
+  tune_dc ();
+  tune_powm ();
   printf("\n");
 
   tune_matrix22_mul ();
   tune_hgcd ();
   tune_gcd_dc ();
   tune_gcdext_dc ();
+#if 0
+  tune_gcd_accel ();
+#endif
   tune_jacobi_base ();
   printf("\n");
 
@@ -1950,6 +1848,12 @@
   tune_set_str ();
   printf("\n");
 
+  tune_fft_mul ();
+  printf("\n");
+
+  tune_fft_sqr ();
+  printf ("\n");
+
   time (&end_time);
   printf ("/* Tuneup completed successfully, took %ld seconds */\n",
           end_time - start_time);
